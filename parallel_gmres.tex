\chapter{Multi-GPU Preconditioned GMRES}


\section{Introduction}
This handout follows the implementation of my Parallel (Multi-GPU) GMRES algorithm. 

\section{TOREAD} 

http://www.irisa.fr/sage/recherche/sparsesolver.html

(consider Deflated GMRES for gpu. Doesnt appear to have been tested. Li and Saad point out that Deflated PCG was tested with success.)

\section{Related Work: Li and Saad 2011} 

Li and Saad demonstrated that for unstructured matrices SpMV kernels can be up to 10x faster than the CPU (Tesla C1060 vs Intel Xeon E5504). 
Their Incomplete Cholesky (special case of ILU) and Conjugate Gradient method on the GPU is up to 3x faster. 
An ILU preconditioned GMRES method can be up to 4x faster. 

\authnote{ THIS IS MY GOAL: at least 4x speedup for my ILU0 and GMRES for RBF-FD unstructured matrices} 

\section{Related Work: Bahi et al. 2011}
A multi-GPU implementation of GMRES was introduced in 2011 by Bahi, et al. \cite{Bahi2011}. In Algorithm~\ref{alg:gmres}, I have included the original pseudocode presented in \cite{Bahi2011} and {\color{blue}comments} for a discussion of what the algorithm does. 

\begin{algorithm}                      % enter the algorithm environment
\caption{Left-preconditioned GMRES with restarts}          % give the algorithm a caption
\label{alg:gmres}                           % and a label for \ref{} commands later in the document
\begin{algorithmic}[1]                    % enter the algorithmic environment
    \State $\varepsilon$ (tolerance for the residual norm $r$), $x_0$ (initial guess), and set $convergence = false$
    \While{ $convergence == false$}
    \State $r_0 = M^{-1} (b-Ax_0)$ \Comment{{\color{blue} They left off details of restarts, is the rest of their algorithm accurate?}}
    \State $\beta = ||r_0||_2$
    \State $v_1 = r_0 / \beta$ \Comment{ {\color{blue}This should really read: $v_1 = \frac{1}{\beta} r_0$)}}
	\For {$j=1$ to $m$} \label{alg:gmres_rotation_loop_start} \Comment{ {\color{blue}$m$ is the $restart$ input parameter ($\#$ of iters between restarts) } }
			\State $w_j = M^{-1} A v_j$
			\For {$i = 1$ to $j$}\label{alg:gmres_inner_loop_start}\Comment{{\color{blue}Key: we do $j$ iterations so the amount of work always grows }}  
				\State $h_{i,j} = (w_j, v_i)$ \Comment{{\color{blue}$(\cdot,\cdot)$ is an inner/dot product (usually $< \cdot, \cdot >$) }}
				\State $w_j = w_j - h_{i,j} v_i$
			\EndFor \label{alg:gmres_inner_loop_stop}
			\State $h_{j+1, j}  = ||w_j||_2$			\Comment{ {\color{blue}This $(m+1)\times m$ upper Hessenberg matrix must be stored }}
			\State $v_{j+1} = w_j / h_{j+1,j}$		\Comment{{\color{blue}This forms an orthonormal basis $V_m$ and must be stored ($N \times m$ matrix)}}
	\EndFor\label{alg:gmres_rotation_loop_stop} 
	\State Set $V_m = [v_1, \cdots, v_m]$ and $\bar{H}_m = (h_{i,j})$ an upper Hesssenberg matrix of order $(m+1)\times m$
	\State \label{alg:gmres_least_squares} Solve a least-square problem of size $m$: $\min_{y \in \R^m} ||\beta e_1 - \bar{H}_m y||_2$	\Comment{{\color{blue}Ambiguous. Will clarify.}}
	\State $x_m = x_0 + V_m y_m$ \label{alg:gmres_residual_norm}
	\If { $||M^{-1}(b-Ax_m)||_2  < \varepsilon$ }
		\State $convergence = true$
	\EndIf
	\State $x_0 = x_m$
    \EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Communication Points} 

In \cite{Bahi2011} the authors point out that communication happens at two types of points in the algorithm: \begin{enumerate} \item Before SpMV operations, and \item After vector operations \end{enumerate}. I have added markup to their pseudo-code in Algorithm~\ref{alg:gmres_comm} to illustrate where they perform communication. 

\begin{algorithm}                      % enter the algorithm environment
\caption{Left-preconditioned GMRES with restarts and {\color{red} communication poins} }          % give the algorithm a caption
\label{alg:gmres_comm}                           % and a label for \ref{} commands later in the document
\begin{algorithmic}[1]                   % enter the algorithmic environment
    \State $\varepsilon$ (tolerance for the residual norm $r$), $x_0$ (initial guess), and set $convergence = false$
    \While{ $convergence == false$}
    \State {{\color{red} MPI\_AlltoAllv() }}
    \State $r_0 = M^{-1} (b-Ax_0)$ 	
    \State $\beta = ||r_0||_2$		
	\State{{\color{red} MPI\_Allreduce() }}	 	
    \State $v_1 = r_0 / \beta$
	\For {$j=1$ to $m$} 
			\State {{\color{red} MPI\_AlltoAllv() }}
			\State $w_j = M^{-1} A v_j$			 	
			\For {$i = 1$ to $j$}
				\State $h_{i,j} = (w_j, v_i)$ 				
				\State{{\color{red} MPI\_Allreduce() }}
				\State $w_j = w_j - h_{i,j} v_i$
			\EndFor 
			\State $h_{j+1, j}  = ||w_j||_2$			
			\State{{\color{red} MPI\_Allreduce() }}
			\State $v_{j+1} = w_j / h_{j+1,j}$	
	\EndFor
	\State Set $V_m = [v_1, \cdots, v_m]$ and $\bar{H}_m = (h_{i,j})$ an upper Hesssenberg matrix of order $(m+1)\times m$
	\State Solve a least-square problem of size $m$: $\min_{y \in \R^m} ||\beta e_1 - \bar{H}_m y||_2$
	\State $x_m = x_0 + V_m y_m$
	\If { $||M^{-1}(b-Ax_m)||_2 < \varepsilon$ }
		\State $convergence = true$
	\EndIf
	\State{{\color{red} MPI\_Allreduce() }}
	\State $x_0 = x_m$
    \EndWhile
\end{algorithmic}
\end{algorithm}

\subsection{Practical Implementations}

In Algorithm~\ref{alg:gmres}, line~\ref{alg:gmres_least_squares} ambiguously states we need to solve a least-squares problem. 

%This ambiguity is also present in Saad's book (see e.g., page 165, \cite{Saad2003}) although the authors return on page 169 to discuss Givens rotations and other useful details for ``Practical Implementation". This section summarizes a few of those details. 

We could take the algorithm as is and express the least-squares problem as 
\begin{equation} 
\bar{H}_m^T \beta e_1 - \bar{H}_m^T \bar{H}_m y = 0 \label{eq:least_squares}
\end{equation}
for a cost of O($m^3$) operations on line~\ref{alg:gmres_least_squares}. If $m$ is small this is not a concern. However, it is common practice instead to find the implicit QR decomposition of $\bar{H}_m$ to reduce the complexity to O($m^2$). 

Saad and Schultz \cite{Saad1986} detail an efficient method of computing plane rotations at the end of each inner-loop iteration (i.e., just before line~\ref{alg:gmres_rotation_loop_stop} of Algorithm~\ref{alg:gmres}). Their method computes one Givens rotation at a cost of O($m$) per iteration, such that at the end of the loop, the QR factorization is implicitly formed with an overhead of O($m^2$). With the QR factor, the minimization problem transforms to 
$$\min_{y \in \R^m} ||Q^T (\beta e_1 - \bar{H}_m y) ||_2 = \min_{y \in \R^m} ||g_m - R_m y||_2$$
where $R_m$ is the upper triangular $m\times m$ factor of $\bar{H}_m$ from the QR process, and $g_m = Q^T \beta e_1$ has been updated at each iteration. Now, using the knowledge that the 2-norm of a vector is invariant under orthogonal transformations (i.e., the matrix $Q^T$), our minimization problem reduces to solving $$ R_m y = g_m $$ which can be done with a simple upper triangular back solve. 

Saad and Schultz \cite{Saad1986} also demonstrate that by using the implicit QR factorization, the residual norm of the approximate solution $x_m$ is available at every iteration without explicitly forming $x_m$ on Line~\ref{alg:gmres_residual_norm}. Instead, due to the Givens rotations of system, the residual norm is always available in the $(k+1)$-st component of $g_k$. The explicit formation of $x_m$ involves the product of $N \times m$ matrix $V_m$ and $N$ length vector $x_m$. Therefore, using the implicit residual norm and postponing the formation of our solution saves a significant O($Nm$) operations per iteration. 

As the first multi-GPU GMRES paper, Bahi et al. \cite{Bahi2011} do not specify if they use Givens rotations for efficiency. The description of their algorithm states one thread on the GPU takes responsibility in Algorithm~\ref{alg:gmres} at line~\ref{alg:gmres_least_squares} and solves the size-$m$ least squares problem. This leads me to believe they solve Equation~\ref{eq:least_squares}. They also mention a kernel that forms a local view of $x_m$ on line~\ref{alg:gmres_residual_norm}, which further argues against soliciting the help of the implicit QR. \authnote{THIS COULD BE A NEW CONTRIBUTION. Do the plane rotations require additional synchronization? Do the plane rotations impact convergence? Is there a reason to avoid plane rotations stated in other parallel GMRES algorithms publications?} 


\section{Related Work: Dekker 2000}
In 2000, Dekker \cite{Dekker2000} published a report describing the Parallel GMRES method. His approach was to apply an additive schwarz decomposition to the standard GMRES (with Givens rotations) and then add a few tweaks to increase scalability. The punch line of his method is that it increases computational overhead per processor in order to reduce the overall communication time. This is ideal for GPUs where the cost of communication includes both CPU-CPU and two way CPU-GPU transfers. I suspect this is the approach we want to take. However, in Dekker's subsequent papers (see \cite{Dekker2001}, \cite{Dekker2005}) he mentions a requirement for his convergence studies to have red-black ordering on the nodes. This is not possible with RBF-FD as far as I know. Red-black ordering would require us to partition every other node into red and the rest in black. This can be extended to more than two colors, but I'm not sure our connectivity would allow for the separation in matrix blocks that he leverages in the latter papers. \cite{Dekker2000} does not appear to have this requirement but I need more exposure with the method to be certain.  


\section{CUSP Implementation} 

To implement my own multi-GPU Parallel GMRES, I chose to leverage the existing code-base of libraries like ViennaCL and CUSP. Starting with our existing physical domain partitioning, we can drop in our calls to ``sendrecvUpdates(...)" for the MPI\_AlltoAll calls. Then we just need to add an MPI\_Allreduce replacement, and we should be nearly finished. 

The one aspect of this parallel implementation that I'm concerned with is the least-squares solution. In CUSP and ViennaCL, the implicit rotations are used to accelerate the least squares step. If this cannot span multiple GPUs---so far the discussion is that its high in communication cost, but nothing saying its impossible---then I will have to re-work more of their algorithms. 

I'll start by 

