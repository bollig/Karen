SpMV
	Caching on vector not on weights
	ELL format for fewer loadts








- Need to explain: 
	ELL
	CSR
	COO
	GPU hardware
	OpenCL
	GMRES algorithm (high level; refer to other papers)
	GPU SpMV vs CPU SpMV
	CPU SpMV in UBLAS, MKL(?)

(DONE) - Need benchmarks: 

(done)    How does SpMV strongly scale as n varies and N is 4096000
(done) 		-> tells us how stencil size impacts overlap and how MPI time grows
(done) 		How does SpMV weakly scale as N varies for multiple n? 
(done)		 -> Tells us how efficient the 
(DONE)	 Write algorithms for Distributed


Gather related work on SpMV and Distributed
Write algorithms for SpMV
	Many problems have simple stencils that can be coded directly. e.g., 5-point
	Simply state that initial attempts did what we did
	Then say considering the fact that SpMV has low computational complexity we anticipate it will be memory bound. 
	Then say that if this is true we need to improve memory layout and reduce memory load overhead
	Then say formats are best optimization. 
	Expectations: what are teh GFLOPs achieved by each implementation? What is the peak possible for 2:1 complexity? 
	How does ViennaCL allow control of blocks, tuning? 
		By default all kernels run with 128 or 256 threads. Tuning
		ViennaCL allows the kernels to load the appropriate number of
		threads per kernel 





What is really important: 
	- GPU was targeted
		why 
			peak performance of GPU
			limitations on expectations
			significance of SpMV
		how
			kernels
			viennacl
			performance benchmarks

		DONE:   benchmark viennacl with top GFLOPs (used tuned
			parameters for SpMV tests on one GPU 
		TODO:   write-up


	- Distributed GPU
		why
			target multi-GPU cluster
			overlap comm/comp to hide comm+copy latency
		how
			overlap comm/comp
			performance benchmarks with overlapping comm/comp
		
		DONE: 	Scaling benchmarks on Fermi with and without overlap
		TODO:   write-up

	- Multi-Domain per GPU
		why
			concurrent kernel and dynamic parallelism allow k20
			to saturate with multiple domains
			each domain is independent, processed by sub-GPU
		how
			multiple MPI procs per GPU
			performance benchmarks		

		TODO: 	writeup K20 data, add test cases for larger resolution
			per proc (N=4000,6000,8000,10000)

	- Stokes
		why
			implicit solver, GMRES, needed for unsymmetric matrix
			core of many solvers (poisson solve)
			completes implicit+explicit needs
			Iterative solver based on SpMV and SAXPY, so tuned SpMV
			goes a long way 
		how
			GMRES algorithm
			Stokes form
			solve convergence
			performance benchmarks (GMRES iter/second)

		TODO:	Get convergence plot for Stokes without preconditioning
			Get GMRES iter/s (single GPU vs single CPU)

	- Stokes preconditioning
		why
			high number of iterations needed
			each iteration is a vector of N elements
			reduce iterations and reduce memory. 
		how
			ILU0 initial test
			algorithm implemented CPU only for prototyping
			convergence before and after
			still high number of iterations required
		Future
			find other preconditioner to solve problem
			reformulate problem as SIMPL/SIMPLER method or some 
			other preconditioning or split method. 

		TODO: 	Get convergence plot for Stokes with preconditioning
			Get GMRES iter/s with Preconditioning added

	- Appendices

		DONE: Fix GFLOPs to be 24*N*n + 23*N or 16*N*n + 15*N
