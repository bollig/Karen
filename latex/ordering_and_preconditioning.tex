\makeatletter
\@ifundefined{standalonetrue}{\newif\ifstandalone}{}
\@ifundefined{section}{\standalonetrue}{\standalonefalse}
\makeatother
\ifstandalone
\documentclass{report}

\input{all_usepackages} 

\begin{document}
\fi




\chapter{An Alternative Stencil Generation Algorithm for RBF-FD}
\label{chap:stencils}

Like all RBF methods, RBF-FD is designed to handle irregular node distributions, and the emphasis in literature focuses on how the method manages point clouds. While nothing prevents implementations of RBF-FD from utilizing existing meshes/lattices, most work in the field concentrates on simple geometries like the sphere \cite{FlyerWright07,FlyerLehto10,FlyerLehto11,FlyerWright09} and quadrilateral/prism \cite{FiNDTHESE} to better understand properties of the method and develop extensions. 
%TODO: find refs


Without mesh/lattice connectivity available, stencils are generated by choosing the $n$-nearest neighbors to each node, including the query node. This is known more formally as a \emph{$k$-nearest neighbor (k-NN)} problem \cite{TagliasacchiMFE} (a.k.a. $\ell$-nearest neighbor search \cite{WendlandBook}). Here ``nearest" is defined with the Euclidean distance metric, although it is possible to generalize to other metrics (see e.g., \cite{MatlabKDTreeSearcher}). 

In comparison to the RBF-FD method, global RBF methods with infinite support connect all nodes to all other nodes, so there is no need for neighbor queries. On the other hand, compact RBF methods require all nodes---with no limit on the count---that lie within the support/radius of the RBF centered at each node. This type of neighbor query is referred to as a \emph{ball query} (a.k.a. range query \cite{WendlandBook}) due to the closed ball created by the radius of support in Equation~\ref{eqn:csrbf}. 

The $k$-NN and ball query share many similarities, but the former is harder to solve. Consider, for example, the scenario in Figure~\ref{fig:nearest_neighbor_example}. Two ball queries around a green stencil center are represented as dashed and dash-dot circles. The inner query returns four neighbors, and the outer returns six. If a stencil of size $n=6$ is desired, then the outer query can be truncated to give the five required neighbors shown in blue. In this example the red node and the farthest blue node are equidistant from the center, and ties are broken arbitrarily. Although $k$-NN is simply a truncated ball query, the real challenge lies in finding the proper search radius to enclose at least the $k$ desired neighbors. To find the radius in practice depends on the choice of data structure used to access node locations. 

\begin{figure}
\centering
\includegraphics[width=5.5cm]{rbffd_methods_content/neighbors/ball_query_vs_kNN.png}
\caption{A stencil center in green finds neighboring stencil nodes in blue. Two ball queries are shown as dashed and dash-dot circles to demonstrate the added difficulty of finding the right query radius to obtain the $k$-nearest neighbors.}
\label{fig:nearest_neighbor_example}
\end{figure}


A na\"{i}ve approach to neighbor queries would be a brute-force search that checks distances from all nodes to every other node. Obviously the cost of such a method is high: $O(N^2)$ for all stencils. Multi-dimensional data structures can limit the scope of searching and reduce the cost of stencil generation to $O(N \log{N})$. 

For the most part, investigations in RBF communities that delve into efficient neighbor queries are limited methods that depend on ball queries. For example, the Partition of Unity method for approximation (e.g., \cite{Wendland2002,WendlandBook}), and particle methods like the Fast Multipole Method (e.g., \cite{Ying2006, Gumerov2003}) or Smoothed Particle Hydrodynamics (e.g., \cite{Krog2010}). Examples of fast algorithms employed in these fields include the fixed-grid method \cite{WendlandBook,Krog2010}, $k$-D Trees \cite{WendlandBook}, Range Trees \cite{Wendland2002,WendlandBook}, and $2^d$-Trees (i.e., Quad- and Octrees) \cite{Gumerov2003, Ying2006}. Surprisingly, while other communities continue the quest for fast neighbor queries, RBF collocation and RBF-FD communities have been slow on the uptake. For many years, the standard in the community has been to use $k$-D Trees (see e.g., \cite{Fasshauer2007, FlyerLehto11,FornbergLehto11}). 


This chapter considers the use of an alternative approach to generate stencils for RBF-FD. It is based loosely on the fixed-grid method from \cite{Krog2010}. \cite{Samet2005} would classify the algorithm as a \emph{fixed grid ``bucket" method with one-dimensional spatial ordering}. However, due to the use of spatial hashing to order nodes in memory it is referred to here as the \emph{Hash Algorithm}. 

The Hash Algorithm loosens the requirements for $k$-nearest neighbors ($k$-NN) stencils to allow $k$-``approximately nearest" neighbors ($k$-ANN). It also reorders nodes according to space-filling curves. In what follows, the Hash Algorithm is compared to an efficient implementation of $k$-D Tree (\cite{TagliasacchiMFE}). Benchmarks demonstrate that with the proper choice of parameters the Hash Algorithm is up to 2x faster than $k$-D Tree, and it comes with a free bonus: up to 5x faster SpMV due to the impact of spatial reordering that occurs during stencil generation. 


\section{$k$-D Tree}

A $k$-D Tree is a spatial data structure that generally decomposes a space/volume into a small number of cells. All $k$-D Trees are binary and iteratively subdivide volumes and sub-volumes at each level into two parts. The ``$k$" in $k$-D Tree refers to the dimensionality of the data/volume  partitioned---that is $k \equiv d$. 


Given a set of points bounded by a $d$-dimensional volume, a $k$-D Tree applies a hierarchy of $(d-1)$-dimensional axis aligned \emph{splitting planes} to cut the space. At each level of the hierarchy the splitting planes result in two new \emph{half-planes} \cite{Skiena2008}. Consecutive splits intercept one another at a \emph{splitting value}. $k$-D Trees do not require that half-planes equally subdivide a volume; more often it is the data contained within the volume that is equally partitioned. The choice of dimension for the splitting plane, in conjunction with a variety of methods for choosing the splitting values allows for many flavors of $k$-D Trees (see e.g., \cite{Samet2005, Skiena2008, Berg2008} for comprehensive lists). \emph{Point $k$-D Trees}, \emph{$2^d$ Trees} (i.e., quad-/octrees), \emph{BSP-Trees}, and \emph{R-trees} are all members of the general $k$-D Tree class \cite{Skiena2008,Ying2006}.

This work considers \emph{Point $k$-D Trees} \cite{Samet2005}, which partition a set of discrete points/nodes as outlined by the recursive procedure in Algorithm~\ref{alg:kdtree_build}. Point $k$-D Trees assume that splitting planes intercept nodes rather than occur arbitrarily along the half-plane. The splitting value at each level of the tree is set to the \emph{median coordinate} of the points in the half-plane, which ensures the tree is well balanced on initial construction. All nodes with coordinate (in the current dimension) less than or equal to the splitting value are contained by the left half-plane, and all nodes with coordinate greater than the splitting value are contained by the right. Half-planes containing only one element correspond to leaves of the tree. The median coordinate of a half-plane is found by sorting the $n$ node coordinates contained by the partition and selecting the $\left\lceil \frac{n}{2} \right\rceil$-th element \cite{Berg2008}. 
\begin{algorithm} 
\caption{BuildKDTree($P$, $depth$)}         \label{alg:kdtree_build}  
\begin{algorithmic}[1]    
    \State \textbf{Input:} A set of $d$-dimensional points $P$ and the current $depth$.
    \State \textbf{Output:} The root of the $k$-D Tree for $P$.
    \State
    \If{$size(P) = 1$}
    \State \Return a new leaf storing $P$
    \EndIf
    \State $L_i := median(coord(P, depth))$ 
    \State $v_{l} := $ BuildKDTree($coord(P, depth) \leq L_i$, $(depth+1)$ modulo $d$)
    \State $v_{r} := $ BuildKDTree($coord(P, depth) > L_i$, $(depth+1)$ modulo $d$) 
    \State \Return A new node $v := \begin{pmatrix} \text{value} := L_i \\ \text{left} := v_l \\ \text{right} := v_r \end{pmatrix}$ 
\end{algorithmic}
\end{algorithm}

The $k$-D Tree in Figure~\ref{fig:kdtree_example} is an example of a Point $k$-D Tree. Given a set of eight nodes in two dimensions, the tree is constructed by applying one-dimensional cuts along the $x$-dimension, then the $y$-dimension, then back to the $x$-dimension. This approach is referred to as \emph{cyclic splitting}, as consecutive cuts are applied by iterating dimensions in a round-robin fashion \cite{Samet2005}. The first cut, $L1$, shown in green, splits the nodes into two sets on either side of $A$. The corresponding tree in the center of Figure~\ref{fig:kdtree_example} shows $L1$ as the tree root with all nodes having $x$-coordinates less than or equal to $A$ to the left, and all nodes having $x$-coordinates greater than $A$ to the right. The second level of the tree, $L2$ and $L3$ (in blue), splits the half-planes on either side of $A$ at nodes $B$ and $C$. The axis parallel splits for each half-plane intercept $L1$ independently to partition half-planes along the $y$-dimension; once again, nodes with coordinates less than or equal (i.e., below the cut) to the splitting value branch left in the tree, and $y$-coordinates greater than  (i.e., above) the value branch right. The third level (red) returns to splitting half-planes in the $x$-dimension. Nodes $D$ and $H$ are not intersected by a splitting plane; their half-planes contain only one node so they immediately become leaves of the tree. This process to build a Point $k$-D Tree has a complexity of $O(N \log N)$ with $O(N)$ storage required \cite{Berg2008,Samet2005}.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{rbffd_methods_content/neighbors/kdTree_example.png}
\caption{An example $k$-D Tree in 2-Dimensions. Nodes are partitioned with a cyclic dimension splitting rule (i.e., splits occur first in $X$, then $Y$, then $X$, etc.); all splits occur at the median node in each dimension. }
\label{fig:kdtree_example}
\end{figure}

Frequently, the terms \emph{$k$-D Tree} and \emph{Point $k$-D Tree} are used synonymously by the RBF community (see e.g., \cite{Fasshauer2007,FlyerLehto11,FornbergLehto11}); the same is convention adopted here. 

Generating an RBF-FD stencil with a $k$-D Tree can be efficiently accomplished in $O(n \log{N})$ time---where $n$ is the stencil size---following an approach introduced in \cite{Friedman1977}, and presented in Algorithm~\ref{alg:kdtree_knn}. The $k$-NN search starts a depth-first recursive search of the $k$-D Tree to find the nearest neighbor to a query point, $X_q$. Traversal of the the tree occurs by following branches left or right based on comparison of $X_q$ coordinates to the splitting value stored at each node of the tree, with the objective to find the smallest half-plane containing $X_q$. The search traverses the height of the tree in $O(\log N)$ steps to find the leaf that stores the nearest neighbor to $X_q$. The neighbor point and its distance from $X_q$ are inserted into a global priority queue, $pq$. Points in the priority queue are sorted in descending order according to distance. 


\begin{algorithm} 
\caption{KNNSearchKDTree($X_q$, $n$, $root$, $depth$)}         \label{alg:kdtree_knn}  
\begin{algorithmic}[1]    
    \State \textbf{Input:} A query node $X_q$, number of desired neighbors ($n$), the current \emph{root} of the $k$-D Tree, and the current $depth$ of traversal.
    \State \textbf{Output:} A global priority queue, $pq$, containing the $n$-nearest neighbors to $X_q$ sorted by distance from $X_q$ in descending order.
    \State \textbf{Assume:} A routine named ``BoundsOverlapBall" exists to determine if the boundaries of the current half-plane are intersected by the ball centered at $X_q$ with radius equal to the maximum distance in $pq$. As long as $pq.size < n$, ``BoundsOverlapBall" defaults to true. %$B_{+}$ and $B_{-}$ are $d$-dimensional arrays initialized to $\pm \infty$ on first call and used within BoundsOverlapBall.
    \State
    \If{$root$ is leaf}
        \State Insert $\{root,\text{dist}(X_q,root)\}$ into $pq$
        \If{$pq.\text{size} > n$} 
            \State $pq$.pop \Comment{Keep only $n$-nearest neighbors}
        \EndIf
        \State \Return
    \EndIf
    \State
    \If{$\text{coord}(X_q, depth) <= root.\text{value}$} 
       % \State $temp := B_{+}[depth]$, and $B_{+}[depth] := root.\text{value}$
        \State KNNSearchKDTree($X_q$, $n$, $root.\text{left}$, $(depth+1) \text{ \% } d$)
       % \State $B_{+}[depth] := temp$
    \Else
        %\State $temp := B_{-}[depth]$, and $B_{-}[depth] := root.\text{value}$
        \State KNNSearchKDTree($X_q$, $n$, $root.\text{right}$, $(depth+1) \text{ \% } d$)
       % \State $B_{-}[depth] := temp$
    \EndIf
    \State 
    \If{$\text{coord}(X_q, depth) <= root.\text{value}$} 
        %\State $temp := B_{-}[depth]$, and $B_{-}[depth] := root.\text{value}$
        \If{BoundsOverlapBall($X_q$)}
            \State KNNSearchKDTree($X_q$, $n$, $root.\text{right}$, $(depth+1) \text{ \% } d$)
        \EndIf
        %\State $B_{-}[depth] := temp$
    \Else
        %\State $temp := B_{-}[depth]$, and $B_{-}[depth] := root.\text{value}$
        \If{BoundsOverlapBall($X_q$)}
            \State KNNSearchKDTree($X_q$, $n$, $root.\text{left}$, $(depth+1) \text{ \% } d$)
        \EndIf
        %\State $B_{+}[depth] := temp$
    \EndIf
    \State \Return
    \end{algorithmic}
\end{algorithm}

After finding the nearest neighbor the algorithm returns to the previous node in the tree and onto the opposing half-plane (i.e., down the far branch) to look for other leaves. So long as the size of $pq$ is at less than capacity ($n$) the search automatically adds points to the priority queue. If $pq$ reaches capacity the algorithm starts to pop off excess points with the understanding that the action removes those points farthest from $X_q$. 

In order to prune branches from the search and reduce complexity, Algorithm~\ref{alg:kdtree_knn} makes use of a routine called ``BoundsOverlapBall", which checks if any boundaries of the current level half-plane intersect/overlap with a closed ball centered at $X_q$. The ball is given a radius equal to the maximum distance in $pq$. Then, if the ball and a boundary intersect, the search will continue onto the half-plane on the opposite side of that boundary. This step handles the possibility that nearer nodes occur within the overlapped region in the other half-plane. If the ball and boundary do not intersect, the opposing half-plane and its related subtree are pruned from the search. Additional details on the implementation of ``BoundsOverlapBall" can be found in \cite{Friedman1977,TagliasacchiGC}. 

The authors of \cite{Friedman1977} find Algorithm~\ref{alg:kdtree_knn} capable of efficiently querying the $n$-nearest neighbors with a complexity proportional to $O(\log{N})$ (dominated by the cost of tree traversal). The relationship between stencil size $n$, and grid size, $N$, is better expressed as $O(n \log{N})$ for one stencil. 

RBF-FD only needs to generate stencils once, so the overall time for the step subsumes the cost of tree construction and $N$ queries. The resulting overall complexity is proportional to $O(N\log{N})$. %The implementation of $k$-D Tree tested in Section~\ref{sec:stencils_benchmarks} reflects this complexity. 


% TODO: other related work. What implementations are available in MATLAB and C++?



\section{A Fixed-Grid Hashing Algorithm}

While a $k$-D Tree functions well for queries, the cost to build the tree structure is unnecessary overhead. Among the many data-structures that exist for nearest neighbor queries, alternatives like fixed-grid methods \cite{Samet2005,Wendland2002,WendlandBook} (a.k.a. uniform grid \cite{Krog2010,Green2010}) bypass much of the cost in construction with an assumption that only lower spatial dimensions (e.g., 2-D or 3-D) are significant. This discards the need to build a tree and shifts focus onto querying neighbors. 

Fixed-grid methods get their name from a coarse 2-D or 3-D regular grid that is overlaid on the domain. The $d$-dimensional grid divides the domain's axis aligned bounding box (AABB)---that is, the minimum bounding box containing the entire domain with edges parallel to axes---into $(h_n)^d$ cells. Subdivisions are uniform, so one can easily identify the cell containing any sample point, $p$, given the coordinates of the AABB and $(h_n)^d$. For example, let $(c_x, c_y, c_z)$ be the desired containing cell in 3-D, and $(x_{min}, y_{min}, z_{min})$ and $(x_{max}, y_{max}, z_{max})$ be the minimum and maximum coordinates of the AABB (resp.). Then the cell coordinates are found by:  
\begin{align}
(dx, dy, dz) & = \left(\frac{(x_{max} - x_{min})}{h_n}, \frac{(y_{max} - y_{min})}{h_n}, \frac{(z_{max} - z_{min})}{h_n}\right) \nonumber \\
(c_x, c_y, c_z) & = \left(\left\lfloor\frac{(p_x - x_{min})}{dx}\right\rfloor , \left\lfloor\frac{(p_y - y_{min})}{dy}\right\rfloor , \left\lfloor\frac{(p_z - z_{min})}{dz}\right\rfloor \right).
\label{eq:cell_hash}
\end{align}
Cells neighboring $(c_x, c_y, c_z)$ are trivial to find by adding positive and negative offsets to each coordinate. Note that the fixed-grid in this method is virtual. Nodes are matched with cell coordinates, but the full grid is never explicitly constructed.

Fixed grid methods also make use of \emph{space filling curves}. Space filling curves pass through every point in $d$-dimensional space, and through each point only once. Equivalently, space filling curves map $d$-dimensional space down to 1-D, where every point is converted to a unique index or traversal order based on its spatial coordinates. These mapping properties make space filling curves ideal for use as hash functions. Traversing the $d$-dimensional points (i.e., playing ``connect the dots") draws the space filling curve. Figure~\ref{fig:space_filling_curves} presents two common orderings of a 2-D fixed-grid. Note that one-dimensional orderings are not unique. On the left is a \emph{Raster}-ordering (a.k.a. Scanline- or $ijk$-ordering): $f(c_x,c_y,c_z) = ((c_x * h_n) + c_y) * h_n + c_z$. The right half of Figure~\ref{fig:space_filling_curves} shows an ordering known as Morton- or $Z$-ordering; construction of this ordering is discussed later in this chapter. The lower left corner of each cell in Figure~\ref{fig:space_filling_curves} indicates the mapped index. Traversing the cells in order produces the curves superimposed in red. 

\begin{figure}
\centering
\includegraphics[width=0.65\textwidth]{rbffd_methods_content/neighbors/space_filling_curves.png}
\caption{Two example space filling curves to linearize the same fixed-grid. Left: Raster-ordering ($ijk$); Right: Morton-/Z-ordering.}
\label{fig:space_filling_curves}
\end{figure} 


At a high level, fixed-grid methods have the following preprocessing steps \cite{Krog2010}:
\begin{enumerate}
\item Subdivide the domain with the overlay grid.
\item For each node, identify the containing cell coordinates.
\item For each node, use the cell coordinates as input to a spatial hash function (i.e., a space-filling curve).
\item Sort the nodes according to their spatial hash.
\end{enumerate}
Particular details of how nodes are sorted, the choice of hashing function, etc. determine the specific class of fixed-grid method and corresponding complexity. A comprehensive list of options and classifications can be found in \cite{Samet2005}. 

The implementation in this work is inspired by fixed-grid approaches for GPU particle simulations \cite{Krog2010,Green2010,Johnson2011}. Particle methods require a ball query at each time-step. With time-steps often dominated by the cost of querying neighbors, the community understandably devotes significant effort to finding the most efficient algorithms possible. The fixed-grid method is competitive for at least two reasons: a) by bypassing the need to build a tree, half the cost in querying neighbors is avoided; and b) nodes sorted according to a space filling curve are typically closer in memory to their neighbors, resulting in a higher likelihood that data will be cached when required. 

The choice of spatial hash function certainly impacts memory access patterns. 

\begin{algorithm} 
\caption{BuildFixedGrid($P$, $h_n$)}         
\label{alg:fixed_grid}  
\begin{algorithmic}[1]    
    \State \textbf{Input:} A set of $d$-dimensional points $P$, and the desired resolution of the overlay grid, $hnx$.
    \State \textbf{Output:} The sorted set of points $\hat{P}$.
    \State
    \State Create $Q$: an $(h_n)^d$ array of empty vectors. 
    \For{point $p_i$ in $P$}
       \State $c := \text{CellCoords}(p_i)$ 
       \State $ind := \text{SpatialHash}(c)$
       \State Append $i$ to $Q[ind]$
    \EndFor
    \For{each $Q[j]$, $j=0,1,...,(h_n)^d$}
    \If{$Q[j]$ is not empty}
        \State Append $P[Q[j]]$ to $\hat{P}$
        \State Overwrite $Q[j]$ with new indices in $\hat{P}$ 
    \EndIf
    \EndFor
    \State \Return $\hat{P}$, $Q$
    \end{algorithmic}
\end{algorithm}



\subsection{$k$-NN with Fixed-Grid}

\begin{algorithm} 
\caption{QueryFixedGrid($\hat{P}$, $Q$, $h_n$)}         
\label{alg:fixed_grid}  
\begin{algorithmic}[1]    
    \State \textbf{Input:} A set of $d$-dimensional points $P$, and the desired resolution of the overlay grid, $hnx$.
    \State \textbf{Output:} The sorted set of points $\hat{P}$.
    \State Get $min$ and $max$ coordinates from $P$
    \State Create a vector of $(h_n)^d$ empty vectors, $Q$. 
    \For{point $p_i$ in $P$}
       \State $c := \text{CellCoords}(p_i)$ 
       \State $ind := \text{SpatialHash}(c)$
       \State Append $i$ to $Q[ind]$
    \EndFor
    \For{each $Q[j]$, $j=0,1,...,(h_n)^d$}
    \If{$Q[j]$ is not empty}
        \State Append all (unsorted) nodes in $P[Q[j]]$ to $\hat{P}$
        \State Overwrite $Q[j]$ with new indices from $\hat{P}$ 
    \EndIf
    \EndFor
    \State \Return $\hat{P}$, $Q$
    \end{algorithmic}
\end{algorithm}



\subsection{Performance} 

%TODO: move down
Although RBF-FD only requires neighbor queries once, the results that follow reveal a long lasting positive impact on memory with a fixed-grid method, which is sufficient to justify its use. Investigations into moving node coordinates and/or local refinements for RBF methods (e.g., \cite{FlyerLehto10}) would find the fixed-grid method significantly more beneficial. As of this writing no known applications of RBF-FD consider moving nodes

Due to the limited significance of stencil generation under RBF-FD, the overhead in implementing and debugging the fixed-grid method on the GPU is difficult to justify. The implementation tested here was developed as a pure CPU prototype with minimal attention to optimization. The added complexity in reproducing the efficient fixed-grid method on the GPU could be the subject of future work for moving nodes. 

    
\cite{Krog2010} assumes that at each time-step a new virtual fixed grid is generated 


Implementations of fixed-grid often assume that no more than two nodes reside in any one cell.  

This implementation is CPU only. Porting the one time stencil generation to the GPU was seen as 

%TODO: low cost of build and easy to query ball
use of fixed-grid methods originated due to low overhead before performing queries.
Although all three related works focus on offloading simulations to the GPU, the method is also popular for CPU implementations 

In \cite{Wendland2002} and later \cite{WendlandBook}, Wendland presents the fixed-grid method as a low-cost alternative for RBF methods based on Partition of Unity. %TODO: state what wendland did or did not do

\cite{Purcell2003} adopt the a similar approach that incrementally grows the 

Following \cite{Samet2005}, the method used in this work is classified as a fixed-grid \emph{bucket} method with one dimensional ordering. The emphasis on bucket arises from the application of a coarse grid that allows 

 implementations of the method leverage radix sort to directly sort nodes based on their spatial hash. The radix sort completely reorders nodes based on their 
%TODO: related work here on how fixed-grid is implemented
%TODO: radix sort on GPU
%TODO: alternatives for space filling curves
Space filling curves are . Examples include $Z$- or \emph{Morton} ordering, $U$- or \emph{Greycode}-ordering, $X$-ordering, etc. Hilbert curves have been demonstrated to be the most efficient \cite{INREFS}. 

The algorithm chosen for this work is provided in Algorithm~\ref{alg:hash_build}. The algorithm starts by creating a list of lists representing the $(h_n)^d$ cells. As nodes are matched with cell indices, the node index is appended to the 



These tests assume $dx = dy = dz$ to ensure spherical stencils. 

With regularly spacing it is possible to directly access the $(i,j,k)$-th cell in the domain with the formula $cell = ((h_n * i) + j) * h_n + k$.

 Each cell of the overlay  queries are popular in particle-based methods like Smoothed Particle Hydrodynamics \cite{Krog2010,Krog2010b}, where neighbors are determined based on spatial proximity in lower dimensions (i.e., 2-D or 3-D). Since RBF-FD stencils are also based on spatial proximity it is possible to 

 The low cost of both construction and query phases allow methods with moving nodes to perform updated nearest neighbor queries at each iteration without significant overhead. 

Since stencil are rarely based on high dimensional data. It suffices to use a 



Having witnessed the success of the fixed-grid method in particle-based simulations at the onset of this work (\cite{Krog2010,Krog2010b,Purcell2003}), a prototype 


The algorithm used in this work is considered a particular

The fixed-grid algorithm 

Figure~\ref{fig:hash_highlevel} clarifies this algorithm presents the variant of the fixed-grid method used in this work from a high-level perspective. Starting with an input set of points, steps 1 through 3 are 


\cite{Brown2010} introduce a minimal $k$-D Tree for the GPU which 

Fixed grid method from a high level. 
Works great for quasi-regular nodes. 
Avoids construction with overlaid grid of known cell indices. 

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{../figures/chapter2/hashing_example/LSH_Concept.png}
\caption{High level overview of the \emph{Fixed-Grid Bucket} (a.k.a. Hash) Algorithm. A coarse regular grid is overlaid on the domain. Nodes coordinates are hashed to containing cell indices and pushed onto a list for the appropriate cell. The cells are reordered in memory according to a space filling curve (i.e., Raster (IJK), Morton (Z), Graycode (U), etc.). Stencil queries start search with the cell containing the stencil center and expand to neighboring cells until at least $n$ candidate nodes are found. The candidate list is truncated to the $n$ closest neighbors. }
\label{fig:hash_highlevel}
\end{figure} 


The fixed-grid method 


An implementation of the method used in \cite{Wendland2002}, discussed in \cite{WendlandBook} and 


For moving nodes, such as in Lagrangian schemes, this cost is prohibitively high. In an attempt to reduce the cost, lagrangian schemes introduced approximate nearest neighbor queries based on 



The algorithm introduced here is the same as the Fixed-Grid Method in \cite{WendlandBook}.     

\subsection{ANN}
Our benchmarks and testing focus on the more challenging problem of $k$-NN

As part of the preprocessing stage for RBF-FD, the scattered point cloud must
be analyzed to generate stencils. To generate a stencil, any collection of
nodes can be selected. However, by choosing nodes close to the stencil center
and well balanced around it, we stand to get the best possible approximations
to derivatives. 

Why? well, the approximations are based on differences. Similar to classic FD,
draw a secant connecting two nodes of a stencil. The slope of the secant
determines the gradient at either point or a point in between. In the limit as
the points are moved closer to the same spot, the approximation to the
derivative at that point becomes exact. 

So ideally every RBF-FD stencil will operate on nearest neighbors. 



 can be described as a \emph{fixed grid ``bucket" method with one-dimensional spatial ordering} as classified by \cite{Samet2005}. 

The implementation runs entirely on the CPU (\cite{Krog2010} runs on the GPU). 

The $k$-D tree implementation compared in this work is the \emph{$kd$-Tree Matlab} library from Tagliasacchi \cite{TagliasacchiMFE,TagliasacchiGC}. Originally posted to the Matlab FileExchange and now maintained as a Google Code project. Until the addition of $KNNsearcher$ in Matlab 

Tagliasacchi \cite{TagliasacchiGC} uses the standard split by median in each dimension, with alternating chosen by incrementing the dimension by one, modulo the number of dimensions (e.g., in 2D the order is $X, Y, X, Y, ...$). Wendland \cite{WendlandBook} suggests a number of more advanced median cuts such as 

\cite{TagliasacchiMFE}. 
\cite{MatlabKDTreeSearcher}

Hashing, shown in Figure~\ref{fig:hashing_example} overlays a regular grid. This is equivalent to an axis aligned bounding box AABB, with refinement. In other words, we form a quad-tree in 2D, an octree in 3D. The neighbor query starts with the cell in which $x_c$ resides. Since we use an axis aligned bounding box, this cell index is easily calculated given the coordinate and number of subdivisions in each dimension. Once the cell index is resolved, the stencil is populated by taking the $n$ nearest neighbors from within the current cell. If the cell does not contain sufficient number of nodes to fill the stencil, the search for neighbors expands to include the cells immediately adjoining the center cell, taking only the nearest nodes in the provided cells. The search continues to expand outward in a rasterized circle/sphere until $n$ is satisifed. This search is considered approximate because it can happen that a true nearest neighbor would lie in a cell that is is not included in the rasterized circle, and other nodes are substituted from the far reaches of the discretized grid.



%TODO: cite lagrangian sph knn
The complexity of the method is still higher than the more efficient implementations used by Lagrangian methods, but as demonstrated in Figure~\ref{sten_methods_compare} the savings are significant. Generating stencils for RBF-FD is a preprocessing cost, so we do not dedicate an excessive amount of attention to this algorithm. However, a few ideas that would improve: hilbert ordering, choose AABB resolution based on $N$ not user parameters, faster sorting, GPU implementation



In the benchmarks below, a $k$-D Tree implementation by Tagliasacchi \cite{TagliasacchiGC,TagliasacchiMFE}. The implementation follows the construction process and 

it was a popular package offered on the MATLAB FileExchange and garnered attention due to its MEX compiled interface that allowed use as both a C++ and MATLAB library. 

  Most of the RBF community leverages the $k$-D tree, due to its low computational complexity for querying neighbors and its wide availability as standalone software in the public domain (e.g., matlab central has a few implementations for download, and the MATLAB Statistics Toolbox includes an efficient k-D Tree). 

The Matlab central $k$-D Tree is MEX compiled and efficient. We integrated the standalone C++ code into our library.  


\subsection{$k$-NN vs $k$-ANN} 

Start at the center cell and add the 8 cells immediately surrounding it. Add to this the 16 cells in the second halo for a total of 25 cells. 

So while a $k$-NN stencil of size $n=8$ in Figure~\ref{fig:nearest_neighbor_example} would contain the black node in the right-most column of the grid. The $k$-ANN generated a stencil of size $n=8$ would opt for the black node to the top right because it is in the second halo and accepted as ``closer" because the stencil can be satisfied without querying the extra 24 cells on the outermost halo. 


Therefore, it is not essential that stencils contain only nearest neighbors. Instead, one can acquire the \emph{approximate nearest neighbors}. Figure~\ref{fig:ann_example} demonstrates a case where a does not contain all nearest neighbors. As illustrated in the Figure, the ANN stencil and true nearest neighbor stencil differ by one node. THis is not dire


The query is approximate because nodes such as the white node in Figure~\ref{fig:nearest_neighbor_example} could be included in the stencil before 


RBF-FD operates on general node distributions. Historically, stencils are uniform in size ($n$) and generated by selecting the $(n-1)$ true nearest neighbors to a node $x_c$. This is a $k$-NN query. 

Alternative queries are possible: ball query and approximate nearest neighbor. The approximate is of particular interest because nodes closest to the stencil will always be selected, whereas the nodes further away have minimal influence so swapping out cant hurt. The justification in altering the selection is for reduced complexity in neighbor queries.  


\begin{figure}
\centering
\includegraphics[width=8.5cm]{rbffd_methods_content/neighbors/neighbor_incorrect.png}
\caption{A stencil generated with $k$-ANN satisfies the required stencil size, but is not guaranteed to choose the true nearest neighbors.}
\label{fig:approximate_nearest_neighbors}
\end{figure}




\subsection{Integer Dilation and Node Reordering}

\cite{MellorCrummey2001} found that reodering nodes via RCM and space filling curves offer similar benefits in terms of reduced TLB misses and better cache coherency. 

\begin{figure}
\centering
\begin{subfigure}{0.425\textwidth}
\includegraphics[width=1.0\textwidth]{../figures/chapter2/hashing_example/bruteforce_N6400_n50-eps-converted-to.png}
\caption{k-D Tree} 
\end{subfigure} 
\begin{subfigure}{0.425\textwidth}
\includegraphics[width=1.0\textwidth]{../figures/chapter2/hashing_example/lsh_N6400_n50-eps-converted-to.png}
\caption{LSH}
\end{subfigure}
\caption{Example effects of node reordering within neighbor query algorithm for MD node set $N=6400$ with $n=50$. Matrix is $0.78\%$ full. k-D Tree maintains original ordering of the nodes and deceptively appears nearly dense. LSH algorithm reorders nodes according to raster ordering and reveals sparsity of the problem.  }
\end{figure} 

\cite{Samet2005} labels this type of algorithm as a \emph{fixed grid bucket algorithm} with the twist that it also has \emph{one dimensional ordering}





\section{Performance Comparison}


At the start of this dissertation, the most widely used $k$-D Tree implementation in the RBF community was \cite{TagliasacchiMFE}. As a MEX-compiled 



\begin{itemize}
\item Performance improvement
\item SpMV impact
\item impact from space filling curves
\end{itemize}





Many algorithms exist to query the $k$-nearest neighbors (equivalently all nodes in the minimum/smallest enclosing circle). Some algorithms overlay a grid similar to Locality Sensitive Hashing and query such as... \cite{HarPeledMazumdar2003}.


For the purpose of generating RBF-FD stencils the $k$D-Tree is built and all queries are made once. There is no reuse of the tree so the build and query times are combined into one. 

impact from ordering on matrix sparsity. Bandwidth impact. Bandwidth impact on condition considered in future chapter. 

what is best overlay resolution? based on time to generate. choose resolution as n/2, n/9? etc? 

perform neighbor queries on raster order because its easiest to jump cells. then each cell can be reordered according to z,x,u, etc. for better memory locality. Matlab script to do this (can be ported to C)

\url{http://www.vincentgarcia.org/data/Garcia_2010_ICIP.pdf}

GPU version of Locality Sensitive Hashing could reduce complexity further \cite{Pan2011}

This can be done efficiently using neighbor query algorithms or spatial partitioning data-structures such as Locality Sensitive Hashing (LSH) and $k$D-Tree. Different query algorithms often have a profound impact on the DM structure and memory access patterns. We choose a Raster ($ijk$) ordering LSH algorithm \cite{Bollig2011} leading to the matrix structure in Figures~\ref{fig:oneThreadPerStencil} and \ref{fig:oneWarpPerStencil}. While querying neighbors for each stencil is an embarrassingly parallel operation, the node sets used here are stationary and require stencil generation only once. Efficiency and parallelism for this task has little impact on the overall run-time of tests, which is dominated by the time-stepping. We preprocess node sets and generate stencils serially, then load stencils and nodes from disk at run-time. In contrast to the RBF-FD view of a static grid, Lagrangian/particle based PDE algorithms promote efficient parallel variants of LSH in order to accelerate querying neighbors at each time-step \cite{Pan2011, Goswami2010}. 


At the onset of our work on RBF-FD, the most commonly used KDTree implmentation used by the RBF community was \cite{Tagliasacchi2008}. Recently, improvements were made to the KDTree algorithm to reduce the cost of building the KDTree to $O(N log^2 N)$. 

Figure~\ref{fig:stencil_query_old_and_new} compares the total time to generate $N$ stencils of size $n=50$ with three methods: \cite{Tagliasacchi2008}, our hash-based neighbor query, and the improved KDTree from \cite{Tagliasacchi2012}. 
Until the new release of KDTree, our algorithm was a major improvement to the performance of stencil generation. The hash-based approach achieved greater than 


Our work in \cite{BolligFlyerErlebacher2012} leveraged an alternative to $k$-D tree, based loosely on space-filling curve orderings common in Lagrangian schemes like Smoothed Particle Hydrodynamics (e.g., \cite{IanThesis}, \cite{Kelager}). %TODO: I have used Locality Sensitive Hashing \cite{Indyk} in the past. According to Samet \cite{Samet2005} LSH uses a set of random hashes to assign indices for each node. Our case uses space-filling curve, so we use . 





\subsection{Hashing}

\cite{Connor2009} provide a fast parallel 

\cite{Henke2012} is working on parallel generation. \cite{IanJohnsonThesis} has OpenCL neighbor queries

We started with a CPU implementation to test appropriateness. 

Approximate nearest neighbors will be nearly balanced. 
We observe that RBF-FD functions as well on stencils of true nearest neighbors as it does on approximate nearest neighbors.

To demonstrate the savings in choice of stencil generation method, we provide Figure~\ref{fig:sten_methods_compare}. 
 

The impact of our neighbor query also extends influence on the structure of the RBF-FD DMs.
has is to To quantify the sparsity of a Differentiation Matrix we consider the ratio of non-zeros ($N*n$) to total elements in the matrix ($N^2$). For example, a problem of size $N = 10,000$ with stencil size $n=31$ has a ratio of $0.0031$ and is $99.69\%$ empty. 

Querying neighbors requires searching at least the immediate cell one layer of neighbors. by including one extra layer we ensure that small stencils near the border of the immediate cell can pick up neighbors in adjacent cells.



A prototype implementation of this method allowed for a variety of space filling curves to reorder the cells. The curves are created through integer dilation \cite{IntegerD} 


\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{rbffd_methods_content/hashing/originalorder_regulargrid-eps-converted-to.pdf}
\caption{In order: a) node ordering test cases; b) original ordering of regular grid (raster); c) coarse grid overlay for hash functions ($hnx = 6$); d) example stencil ($n=31$) spanning multiple Z's; e) spy of DM after orderings. \authnote{REGENERATE FIGURES WITH RANDOM/HALTON NODES}}
\label{fig:orderings}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{rbffd_methods_content/hashing/overlay_regulargrid-eps-converted-to.pdf} \includegraphics[width=0.45\textwidth]{rbffd_methods_content/hashing/stencil_regulargrid-eps-converted-to.pdf}
\caption{In order: a) node ordering test cases; b) original ordering of regular grid (raster); c) coarse grid overlay for hash functions ($hnx = 6$); d) example stencil ($n=31$) spanning multiple Z's; e) spy of DM after orderings. }
\label{fig:orderings}
\end{figure}

The KDTree implementation used in this work is from 

Original data showed our algorithm as wildly successful against a version  

The Cuthill McKee algorithms can be equated to a breadth-first search. The algorithm queues nodes in order of degree at each level of the search and traverses the lowest degree priority. The Reverse variant of Cuthill-McKee inverts the node order so that the lowest degree and top level node are at the end of the matrix rather than the beginning. Aside from ordering, the Reverse and Standard Cuthill McKee algorithms are identical processes. RCM is the more popular of the variants though, due to storage savings and reduced fill-in for some decompositions \cite{LiuSherman1976}.	. 

%TODO: \cite{Indyk "Algorithms for Nearest Neighbor Search"} for reference on quad vs kd vs lsh
%TODO: \cite{ChenChang "Neighbor-Finding Based on Space-Filling Curves"} 
%TODO: \cite{Stocco and Schrack "On Spatial Orders and Location Codes"}

Obviously, the ideal case for bandwidth is when all rows contain the $\frac{n}{2}$ nodes corresponding to solution value to either side of $u_j$. In 1-D this corresponds to every node containing the $\frac{n}{2}$ nodes to the left and right of $x_j$. In 2-D this is only possible if the nodes in the domain are properly indexed such that stencils contain the proper set of neighbors---a stringent requirement that will 


\begin{figure}
\centering
\includegraphics[width=9.5cm]{../figures/stencils/kdtree_old_reg_subsets_4m_stencil_gen_time.png}
%\includegraphics[width=9.5cm]{../figures/stencils/reg_subsets_4m_stencil_gen_time.png}
\includegraphics[width=9.5cm]{../figures/stencils/sphere_subsets_1m_stencil_gen_time.png}
\caption{Querying the $n=50$ nearest neighbors on a regular grid up to $N=160^3$ demonstrates the significant gains achieved by our spatially binned neighbor query. While KDTree queries grow as $O(N log N)$}
\label{fig:hash_results}
\end{figure}


The early implementation of $k$-D tree had an $O(n^2)$ growth in complexity. This algorithm was developed to alleviate that cost. It took a few hours to implement but has had some surprising impacts. Note that the complexity of $k$-D tree was reduced to $O(N \log^2 N)$ in 2012. On a regular grid (generated with raster/IJK ordering), the cost of $k$-D tree grows at the same rate as the hashing method. 

At $N=32000$ the cost of hashing drops below $k$-D Tree due to the decreasing number of empty hash cells. Likewise, at $N=1000000$ and beyond, the gap between hashing and $k$-D Tree begins to close as cells contain more than one 


Q: why does the curve drop for $hnx=100$? 
Q: the complexity of the algorithm? 
Q: the sphere I understand: its localizing the search to small patches on the sphere, and 





\begin{figure}
\centering
\includegraphics[width=9.5cm]{../figures/stencils/kdtree_old_reg_subsets_4m_stencil_gen_speedup.png}
\includegraphics[width=9.5cm]{../figures/stencils/reg_subsets_4m_stencil_gen_speedup.png}
\caption{Querying the $n=50$ nearest neighbors on a regular grid up to $N=160^3$ demonstrates the significant gains achieved by our spatially binned neighbor query. While KDTree queries grow as $O(N log N)$}
\label{fig:hash_results}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=9.5cm]{../figures/stencils/reg_subsets_4m_spmv_speedup.png}
\caption{Querying the $n=50$ nearest neighbors on a regular grid up to $N=160^3$ demonstrates the significant gains achieved by our spatially binned neighbor query. While KDTree queries grow as $O(N log N)$}
\label{fig:hash_results}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=7.5cm]{../figures/stencils/sphere_subsets_1m_stencil_gen_speedup.png}
\includegraphics[width=7.5cm]{../figures/stencils/sphere_subsets_1m_spmv_speedup.png}
\caption{Generating stencils for increasing subsets of the $N=1e6$ CVT nodes mesh.}
\label{fig:hash_results}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=10.5cm]{../figures/stencils/sphere_stencil_gen_speedup.png}
\includegraphics[width=10.5cm]{../figures/stencils/sphere_spmv_speedup.png} 
\caption{Based on the proper choice of overlay resolution, the hash stencil query can accelerate stencil generation, but the sophistication of the algorithm is low enough that negative impact is more likely. On the other hand, the impact on SpMV performance is always positive with the routine accelerated up to 4.9x faster.}
\label{fig:hash_results}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=7.5cm]{../figures/stencils/cvt1m_stencil_gen_speedup.png}
\includegraphics[width=7.5cm]{../figures/stencils/cvt1m_spmv_speedup.png} 
\caption{As the coarse grid resolution increases the hashing algorithm achieves both 2x faster than KDTree in stencil generation, with greater than 4x gain in SpMV performance (for free).}
\label{fig:hash_results}
\end{figure}

For every $N$ there is an optimal $hnx$. This is depicted for $N=500000$ CVT and $hnx=100$. 

\section{On Space Filling Curves and Other Orderings} 


\subsection{Integer Dilation}
One frequently hears that ordering via space filling curves like Morton Ordering and/or gray codes can benefit memory access patterns. 

(Related? \url{http://publish.uwo.ca/~shaque4/presentationSONAD.pdf} \url{http://www.cs.duke.edu/~alvy/papers/sc98/} \url{http://www.cs.indiana.edu/~dswise/Arcee/Papers/medea06.pdf} \url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.7720&rep=rep1&type=pdf} \url{http://stackoverflow.com/questions/4260002/benefits-of-nearest-neighbor-search-with-morton-order})

\cite{Saad2003} mentions the impact of ordering on conditioning.

Algorithms like Reverse Cuthill McKee and Approximate Minimum Degree ordering allow general restructuring of matrices. 

\authnote{NEed to compare conditioning of LSH and other algorithms in Matlab}

Q: what is an ideal ordering?
Q: what is the best conditioning from ordering?
Q: what is the relative cost of ordering?



\begin{figure}
\centering
\includegraphics[width=0.65\textwidth]{rbffd_methods_content/hashing/node_orderings-eps-converted-to.pdf} \\
\includegraphics[width=0.65\textwidth]{rbffd_methods_content/hashing/spy_regulargrid-eps-converted-to.pdf} 
\caption{In order: a) node ordering test cases; b) original ordering of regular grid (raster); c) coarse grid overlay for hash functions ($hnx = 6$); d) example stencil ($n=31$) spanning multiple Z's; e) spy of DM after orderings. }
\label{fig:orderings}
\end{figure}


\section{Conclusions on Stencil Generation}



\ifstandalone
\bibliographystyle{plain}
\bibliography{merged_references}
\end{document}
\else
\expandafter\endinput
\fi
