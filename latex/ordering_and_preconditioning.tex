\makeatletter
\@ifundefined{standalonetrue}{\newif\ifstandalone}{}
\@ifundefined{section}{\standalonetrue}{\standalonefalse}
\makeatother
\ifstandalone
\documentclass{report}

\input{all_usepackages} 

\begin{document}
\fi




\chapter{An Alternative Stencil Generation Algorithm for RBF-FD}
\label{chap:stencils}

Like all RBF methods, RBF-FD is designed to handle irregular node distributions, so the emphasis in literature focuses on how the method manages point clouds. While nothing prevents implementations of RBF-FD from utilizing existing meshes/lattices, most work in the field concentrates on simple geometries to better understand properties of the method and develop extensions. Without mesh/lattice connectivity available, stencils are generated by choosing the $n$-nearest neighbors to a center node, inclusive of the center. This is known more formally as a \emph{$k$-nearest neighbor (k-NN)} problem \cite{TagliasacchiMFE} (a.k.a. $\ell$-nearest neighbor search \cite{WendlandBook}). Here ``nearest" is defined with the Euclidean distance metric, although it is possible to generalize to other metrics (see e.g., \cite{MatlabKDTreeSearcher}). 

In comparison to the RBF-FD method, global RBF methods with infinite support connect all nodes to all other nodes, so there is no need for neighbor queries. On the other hand, compact RBF methods require all nodes---with no limit on the count---that lie within the support/radius of the RBF centered at each node. This type of neighbor query is referred to as a \emph{ball query} (a.k.a. range query \cite{WendlandBook}) due to the closed ball created by the radius of support for a compact RBF (see Equation~\ref{eqn:csrbf}). 

The $k$-NN and ball query share many similarities, but the former can be harder to solve. Consider, for example, the scenario in Figure~\ref{fig:nearest_neighbor_example}. Two ball queries around a blue stencil center are represented as dashed and dash-dot circles. The inner query returns four neighbors, and the outer returns six. If a stencil of size $n=6$ is desired, then the outer query can be truncated to give the five required neighbors shown in blue. In this example the red node and the farthest blue node are equidistant from the center, and ties are broken arbitrarily. Although $k$-NN is simply a truncated ball query, the real challenge lies in finding the proper search radius to enclose at least the $n$ desired neighbors. To find the radius in practice depends on the choice of data structure used to access node locations. 

\begin{figure}
\centering
\includegraphics[width=5.5cm]{rbffd_methods_content/neighbors/ball_query_vs_kNN.png}
\caption{A stencil center in blue finds neighboring stencil nodes in blue. Two ball queries are shown as dashed and dash-dot circles to demonstrate the added difficulty of finding the right query radius to obtain the $k$-nearest neighbors.}
\label{fig:nearest_neighbor_example}
\end{figure}


A na\"{i}ve approach for neighbor queries would be a brute-force search that checks distances from all nodes to every other node. Obviously the cost of such a method is high: $O(N^2)$ for all stencils. Multi-dimensional data structures, such as those discussed here, can limit the scope of searching and reduce the cost of stencil generation to $O(N \log{N})$. 

For the most part, investigations in RBF communities that delve into efficient neighbor queries are limited to ball queries. For example, the Partition of Unity method for approximation (e.g., \cite{Wendland2002,WendlandBook}), and particle methods like the Fast Multipole Method (e.g., \cite{Ying2006, Gumerov2003}) or Smoothed Particle Hydrodynamics (e.g., \cite{Krog2010}). Examples of fast algorithms employed in these fields include the fixed-grid method \cite{WendlandBook,Krog2010}, $k$-D Trees \cite{WendlandBook}, Range Trees \cite{Wendland2002,WendlandBook}, and $2^d$-Trees (i.e., Quad- and Octrees) \cite{Gumerov2003, Ying2006}. Surprisingly, while other communities continue the quest for fast neighbor queries, RBF collocation and RBF-FD communities have been slow on the uptake. For many years, the standard in the community has been to use $k$-D Trees (see e.g., \cite{Fasshauer2007, FlyerLehto11,FornbergLehto11}). 


This chapter considers the use of an alternative neighbor query algorithm to generate RBF-FD stencils. It is based loosely on the fixed-grid method from \cite{Krog2010,Green2010,Johnson2011}. Samet\cite{Samet2005} would classify the algorithm as a \emph{fixed grid ``bucket" method with one-dimensional spatial ordering}. The fixed-grid method loosens the requirements for finding the $k$-nearest neighbors ($k$-NN) stencils to accept $k$-``approximately nearest" neighbors ($k$-ANN). It also reorders nodes according to space-filling curves. In what follows, the fixed-grid method is compared to an efficient implementation of $k$-D Tree available for use in C++ and MATLAB (\cite{TagliasacchiMFE}). Benchmarks demonstrate that, with the proper choice of parameters for the fixed-grid, the method can be up to 2x faster than $k$-D Tree, and it comes with a free bonus: up to 5x faster SpMV performance due to the impact of spatial reordering that occurs during stencil generation. 


\section{$k$-D Tree}

A $k$-D Tree is a spatial data structure that generally decomposes a space/volume into a small number of cells. All $k$-D Trees are binary and iteratively subdivide volumes and sub-volumes at each level into two parts. The ``$k$" in $k$-D Tree refers to the dimensionality of the data/volume  partitioned---that is $k \equiv d$. 


Given a set of points bounded by a $d$-dimensional volume, a $k$-D Tree applies a hierarchy of $(d-1)$-dimensional axis aligned \emph{splitting planes} to cut the space. At each level of the hierarchy the splitting planes result in two new \emph{half-planes} \cite{Skiena2008}. Consecutive splits intercept one another at a \emph{splitting value}. $k$-D Trees do not require that half-planes equally subdivide a volume; more often it is the data contained within the volume that is equally partitioned. The choice of dimension for the splitting plane, in conjunction with a variety of methods for choosing the splitting values allows for many flavors of $k$-D Trees (see e.g., \cite{Samet2005, Skiena2008, Berg2008} for comprehensive lists). \emph{Point $k$-D Trees}, \emph{$2^d$ Trees} (i.e., quad-/octrees), \emph{BSP-Trees}, and \emph{R-trees} are all members of the general $k$-D Tree class \cite{Skiena2008,Ying2006}.

This work considers \emph{Point $k$-D Trees} \cite{Samet2005}, which partition a set of discrete points/nodes as outlined by the recursive procedure in Algorithm~\ref{alg:kdtree_build}. Point $k$-D Trees assume that splitting planes intercept nodes rather than occur arbitrarily along the half-plane. The splitting value at each level of the tree is set to the \emph{median coordinate} of the points in the half-plane, which ensures the tree is well balanced on initial construction. All nodes with coordinate (in the current dimension) less than or equal to the splitting value are contained by the left half-plane, and all nodes with coordinate greater than the splitting value are contained by the right. Half-planes containing only one element correspond to leaves of the tree. The median coordinate of a half-plane is found by sorting the $n$ node coordinates contained by the partition and selecting the $\left\lceil \frac{n}{2} \right\rceil$-th element \cite{Berg2008}. 
\begin{algorithm} 
\caption{BuildKDTree($P$, $depth$)}         \label{alg:kdtree_build}  
\begin{algorithmic}[1]    
    \State \textbf{Input:} A set of $d$-dimensional points $P$ and the current $depth$.
    \State \textbf{Output:} The root of the $k$-D Tree for $P$.
    \State
    \If{$size(P) = 1$}
    \State \Return a new leaf storing $P$
    \EndIf
    \State $L_i := median(coord(P, depth))$ 
    \State $v_{l} := $ BuildKDTree($coord(P, depth) \leq L_i$, $(depth+1)$ modulo $d$)
    \State $v_{r} := $ BuildKDTree($coord(P, depth) > L_i$, $(depth+1)$ modulo $d$) 
    \State \Return A new node $v := \begin{pmatrix} \text{value} := L_i \\ \text{left} := v_l \\ \text{right} := v_r \end{pmatrix}$ 
\end{algorithmic}
\end{algorithm}

The $k$-D Tree in Figure~\ref{fig:kdtree_example} is an example of a Point $k$-D Tree. Given a set of eight nodes in two dimensions, the tree is constructed by applying one-dimensional cuts along the $x$-dimension, then the $y$-dimension, then back to the $x$-dimension. This approach is referred to as \emph{cyclic splitting}, as consecutive cuts are applied by iterating dimensions in a round-robin fashion \cite{Samet2005}. The first cut, $L1$, shown in blue, splits the nodes into two sets on either side of $A$. The corresponding tree in the center of Figure~\ref{fig:kdtree_example} shows $L1$ as the tree root with all nodes having $x$-coordinates less than or equal to $A$ to the left, and all nodes having $x$-coordinates greater than $A$ to the right. The second level of the tree, $L2$ and $L3$ (in blue), splits the half-planes on either side of $A$ at nodes $B$ and $C$. The axis parallel splits for each half-plane intercept $L1$ independently to partition half-planes along the $y$-dimension; once again, nodes with coordinates less than or equal (i.e., below) to the splitting value branch left in the tree, and $y$-coordinates greater than  (i.e., above) the value branch right. The third level (red) returns to splitting half-planes in the $x$-dimension. Nodes $D$ and $H$ are not intersected by a splitting plane; their half-planes contain only one node so they immediately become leaves of the tree. This process to build a Point $k$-D Tree has a complexity of $O(N \log N)$ with $O(N)$ storage required \cite{Berg2008,Samet2005}.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{rbffd_methods_content/neighbors/kdTree_example.png}
\caption{An example $k$-D Tree in 2-Dimensions. Nodes are partitioned with a cyclic dimension splitting rule (i.e., splits occur first in $X$, then $Y$, then $X$, etc.); all splits occur at the median node in each dimension. }
\label{fig:kdtree_example}
\end{figure}

Frequently, the terms \emph{$k$-D Tree} and \emph{Point $k$-D Tree} are used synonymously by the RBF community (see e.g., \cite{Fasshauer2007,FlyerLehto11,FornbergLehto11}); the same is convention adopted here. 

Generating an RBF-FD stencil with a $k$-D Tree can be efficiently accomplished in $O(n \log{N})$ time---where $n$ is the stencil size---following an approach introduced in \cite{Friedman1977}, and presented in Algorithm~\ref{alg:kdtree_knn}. The $k$-NN search starts a depth-first recursive search of the $k$-D Tree to find the nearest neighbor to a query point, $X_q$. Traversal of the the tree occurs by following branches left or right based on comparison of $X_q$ coordinates to the splitting value stored at each node of the tree, with the objective to find the smallest half-plane containing $X_q$. The search traverses the height of the tree in $O(\log N)$ steps to find the leaf that stores the nearest neighbor to $X_q$. The neighbor point and its distance from $X_q$ are inserted into a global priority queue, $pq$. Points in the priority queue are sorted in descending order according to distance. 


\begin{algorithm} 
\caption{KNNSearchKDTree($X_q$, $n$, $root$, $depth$)}         \label{alg:kdtree_knn}  
\begin{algorithmic}[1]    
    \State \textbf{Input:} A query node $X_q$, number of desired neighbors ($n$), the current \emph{root} of the $k$-D Tree, and the current $depth$ of traversal.
    \State \textbf{Output:} A global priority queue, $pq$, containing the $n$-nearest neighbors to $X_q$ sorted by distance from $X_q$ in descending order.
    \State \textbf{Assume:} A routine named ``BoundsOverlapBall" exists to determine if the boundaries of the current half-plane are intersected by the ball centered at $X_q$ with radius equal to the maximum distance in $pq$. As long as $pq.size < n$, ``BoundsOverlapBall" defaults to true. %$B_{+}$ and $B_{-}$ are $d$-dimensional arrays initialized to $\pm \infty$ on first call and used within BoundsOverlapBall.
    \State
    \If{$root$ is leaf}
        \State Insert $\{root,\text{dist}(X_q,root)\}$ into $pq$
        \If{$pq.\text{size} > n$} 
            \State $pq$.pop \Comment{Keep only $n$-nearest neighbors}
        \EndIf
        \State \Return
    \EndIf
    \State
    \If{$\text{coord}(X_q, depth) <= root.\text{value}$} 
       % \State $temp := B_{+}[depth]$, and $B_{+}[depth] := root.\text{value}$
        \State KNNSearchKDTree($X_q$, $n$, $root.\text{left}$, $(depth+1) \text{ \% } d$)
       % \State $B_{+}[depth] := temp$
    \Else
        %\State $temp := B_{-}[depth]$, and $B_{-}[depth] := root.\text{value}$
        \State KNNSearchKDTree($X_q$, $n$, $root.\text{right}$, $(depth+1) \text{ \% } d$)
       % \State $B_{-}[depth] := temp$
    \EndIf
    \State 
    \If{$\text{coord}(X_q, depth) <= root.\text{value}$} 
        %\State $temp := B_{-}[depth]$, and $B_{-}[depth] := root.\text{value}$
        \If{BoundsOverlapBall($X_q$)}
            \State KNNSearchKDTree($X_q$, $n$, $root.\text{right}$, $(depth+1) \text{ \% } d$)
        \EndIf
        %\State $B_{-}[depth] := temp$
    \Else
        %\State $temp := B_{-}[depth]$, and $B_{-}[depth] := root.\text{value}$
        \If{BoundsOverlapBall($X_q$)}
            \State KNNSearchKDTree($X_q$, $n$, $root.\text{left}$, $(depth+1) \text{ \% } d$)
        \EndIf
        %\State $B_{+}[depth] := temp$
    \EndIf
    \State \Return
    \end{algorithmic}
\end{algorithm}

After finding the nearest neighbor the algorithm returns to the previous split in the tree and traverses onto the opposing half-plane (i.e., down the far branch) to look for other leaves. So long as the size of $pq$ is at less than capacity ($n$) the search automatically adds points to the priority queue. If $pq$ reaches capacity the algorithm starts to pop off excess points with the understanding that the action removes those points farthest from $X_q$. 

In order to prune branches from the search and reduce complexity, Algorithm~\ref{alg:kdtree_knn} makes use of a routine called ``BoundsOverlapBall", which checks if any boundaries of the current level half-plane intersect/overlap with a closed ball centered at $X_q$. The ball is given a radius equal to the maximum distance in $pq$. Then, if the ball and a boundary intersect, the search will continue onto the half-plane on the opposite side of that boundary. This step handles the possibility that nearer nodes occur within the overlapped region in the other half-plane. If the ball and boundary do not intersect, the opposing half-plane and its related subtree are pruned from the search. Additional details on the implementation of ``BoundsOverlapBall" can be found in \cite{Friedman1977,TagliasacchiGC}. 

The authors of \cite{Friedman1977} find Algorithm~\ref{alg:kdtree_knn} capable of efficiently querying the $n$-nearest neighbors with a complexity proportional to $O(\log{N})$ (dominated by the cost of tree traversal). The relationship between stencil size $n$, and grid size, $N$, is better expressed as $O(n \log{N})$ for one stencil. 

RBF-FD only needs to generate stencils once, so the overall time for the step subsumes the cost of tree construction and $N$ queries. The resulting total complexity of stencil generation for all stencils is then proportional to $O(N\log{N})$. %The implementation of $k$-D Tree tested in Section~\ref{sec:stencils_benchmarks} reflects this complexity. 


% TODO: other related work. What implementations are available in MATLAB and C++?



\section{A Fixed-Grid Algorithm}

While a $k$-D Tree functions well for queries, the cost to build the tree structure is unnecessary overhead. Among the many data-structures that exist for nearest neighbor queries, alternatives like fixed-grid methods \cite{Samet2005,Wendland2002,WendlandBook} (a.k.a. uniform grid \cite{Krog2010,Green2010}) bypass much of the cost in construction with an assumption that only lower spatial dimensions (e.g., 2-D or 3-D) are significant for choosing neighbors. This discards the need to build a tree and shifts focus onto querying neighbors. 

Fixed-grid methods get their name from a coarse 2-D or 3-D regular grid that is overlaid on the domain. The $d$-dimensional grid divides the domain's axis aligned bounding box (AABB)---that is, the minimum bounding box containing the entire domain with edges parallel to axes---into $(h_n)^d$ cells. Subdivisions are uniform, so one can easily identify the cell containing any sample point, $p$, given the coordinates of the AABB and $(h_n)^d$. For example, let $(c_x, c_y, c_z)$ be the desired cell in 3-D, and $(x_{min}, y_{min}, z_{min})$ and $(x_{max}, y_{max}, z_{max})$ be the minimum and maximum coordinates of the AABB (resp.). Then the cell coordinates are found by:  
\begin{align}
(dx, dy, dz) & = \left(\frac{(x_{max} - x_{min})}{h_n}, \frac{(y_{max} - y_{min})}{h_n}, \frac{(z_{max} - z_{min})}{h_n}\right) \nonumber \\
(c_x, c_y, c_z) & = \left(\left\lfloor\frac{(p_x - x_{min})}{dx}\right\rfloor , \left\lfloor\frac{(p_y - y_{min})}{dy}\right\rfloor , \left\lfloor\frac{(p_z - z_{min})}{dz}\right\rfloor \right).
\label{eq:cell_hash}
\end{align}
Cells neighboring $(c_x, c_y, c_z)$ are trivial to find by adding positive and negative offsets to each coordinate. %Note that the fixed-grid in this method is virtual. Nodes are matched with cell coordinates, but the full grid (i.e., vertices/edges) is never explicitly constructed.

Fixed grid methods also make use of \emph{space filling curves}. Space filling curves pass through every point in $d$-dimensional space, and through each point only once. Equivalently, space filling curves map $d$-dimensional space down to 1-D, where every point is converted to a unique index or traversal order based on its spatial coordinates. These mapping properties make space filling curves ideal for use as hash functions. Traversing the $d$-dimensional points (i.e., playing ``connect the dots") draws the space filling curve. Figure~\ref{fig:space_filling_curves} presents two common orderings of a 2-D fixed-grid. Note that one-dimensional orderings are not unique. On the left is a \emph{Raster}-ordering (a.k.a. Scanline- or $ijk$-ordering): $f(c_x,c_y,c_z) = ((c_x * h_n) + c_y) * h_n + c_z$. The right half of Figure~\ref{fig:space_filling_curves} shows an ordering known as Morton- or $Z$-ordering. $Z$-ordering construction is discussed later in this chapter. On both sides of Figure~\ref{fig:space_filling_curves}, the lower left corner of each cell indicates the mapped index. Traversing the cells in order produces the curves superimposed in red. 

\begin{figure}
\centering
\includegraphics[width=0.65\textwidth]{rbffd_methods_content/neighbors/space_filling_curves.png}
\caption{Two example space filling curves to linearize the same fixed-grid. Left: Raster-ordering ($ijk$); Right: Morton-/Z-ordering.}
\label{fig:space_filling_curves}
\end{figure} 


At a high level, fixed-grid methods have the following construction steps \cite{Krog2010}:
\begin{enumerate}
\item Subdivide the domain with the overlay grid.
\item For each node, identify the containing cell coordinates.
\item For each node, use the cell coordinates as input to a spatial hash function (i.e., a space-filling curve).
\item Sort the nodes according to their spatial hash.
\end{enumerate}
Particular details of how nodes are sorted, the choice of hashing function, the number of nodes allowed per cell, etc. determine the specific class of fixed-grid method and corresponding complexity. A comprehensive list of options and classifications can be found in \cite{Samet2005}. 

\subsection{Fixed-grid Construction}
The algorithm in this work is inspired by fixed-grid approaches for GPU particle simulations (\cite{Krog2010,Green2010,Johnson2011}). Particle methods require a ball query at each time-step. With time-steps often dominated by the cost of querying neighbors, the community understandably devotes significant effort to seek out the most efficient solutions possible \cite{Goswami2010}. The fixed-grid method is competitive for at least two reasons: a) by bypassing the need to build a tree, half the cost in querying neighbors is avoided; and b) nodes sorted according to a spatial hash reside closer in memory to nearby neighbors than in the case of unsorted nodes. The spatial locality results in a higher likelihood that data will be cached when required. Note that reordering by cell hash sorts nodes across cells but not within them---that is, nodes contained by the same cell are contiguous in memory, but remain arbitrarily ordered with respect one another. Fortunately, with contiguous groups of nodes, nearest neighbor queries can directly access all nodes per cell. 


The authors of \cite{Krog2010,Johnson2011,Green2010} assume a raster-ordering on cells, and that the uniform grid is sufficiently refined to ensure cells contain at most eight nodes. Particle interactions are limited to ball queries on the containing cell plus one halo of neighboring cells (i.e., 8 surrounding cells in 2-D and 26 cells in 3-D). Since the number of cells to check is fixed, the neighboring nodes can be obtained by direct access in constant time. A similar approach is taken by \cite{Goswami2010}, but the authors opt for a $Z$-ordering of cells. 
 As a point of difference in implementations, the authors of \cite{Green2010, Krog2010, Goswami2010} leverage a fast radix sort algorithm to order nodes based on hash index, while \cite{Johnson2011} utilizes a slower bitonic sort algorithm. The fixed-grid in \cite{Wendland2002,WendlandBook} forgoes logic to refine the grid and enforce a maximum limit on the number of nodes per cell. The author also avoids sorting nodes based on cell hashes. Instead, a list is maintained that stores the indices of all contained nodes per cell. %, which implies random memory access when operating on the nodes of a cell.    


The implementation presented here is a hybrid of the related algorithms. For example, cells are sorted based on raster-ordering, but without the restriction on max number of nodes per cell. Rather than a radix- or bitonic sort to reorder nodes, the list of node indices for each cell (\cite{Wendland2002,WendlandBook}) is constructed as part of a single-pass bucket sort. Finally, in stark contrast to  \cite{Krog2010,Green2010,Johnson2011,Wendland2002,WendlandBook}, querying neighbors is not restricted to a fixed radius, or number of cell halos. To satisfy the $k$-NN query, this implementation iteratively increases the query radius to include a new halo of cells at each iteration. This multi-pass ball-query was demonstrated in Figure~\ref{fig:nearest_neighbor_example}. The iteration terminates when the desired count of neighboring nodes is satisfied or exceeded.
%TODO: Figure~\ref{fig:hash_highlevel} demonstrates the algorithm 
%\begin{figure}
%\centering
%\includegraphics[width=1.0\textwidth]{../figures/chapter2/hashing_example/LSH_Concept.png}
%\caption{High level overview of the \emph{Fixed-Grid Bucket} (a.k.a. Hash) Algorithm. A coarse regular grid is overlaid on the domain. Nodes coordinates are hashed to containing cell indices and pushed onto a list for the appropriate cell. The cells are reordered in memory according to a space filling curve (i.e., Raster (IJK), Morton (Z), Graycode (U), etc.). Stencil queries start search with the cell containing the stencil center and expand to neighboring cells until at least $n$ candidate nodes are found. The candidate list is truncated to the $n$ closest neighbors. }
%\label{fig:hash_highlevel}
%\end{figure} 

\begin{algorithm} 
\caption{BuildFixedGrid($P$, $h_n$)}         
\label{alg:fixed_grid_build}  
\begin{algorithmic}[1]    
    \State \textbf{Input:} A set points $P$, and the fixed-grid resolution, $h_n$.
    \State \textbf{Output:} The reordered points in $P$, and corresponding cell buckets $Q$.
    \State
    \State Create $Q$: an $(h_n)^d$ array of empty buckets. 
    \For{point $p_i$ in $P$}
       \State $c := \text{CellCoords}(p_i)$ 
       \State $ind := \text{SpatialHash}(c)$
       \State Append index $i$ onto $Q[ind]$
    \EndFor
    \For{$j=0,1,...,(h_n)^d$}
    \If{$Q[j]$ is not empty}
        \State Append the set $P[Q[j]]$ onto $\hat{P}$
        \State Overwrite the set $Q[j]$ with new indices of $\hat{P}$ 
    \EndIf
    \EndFor
    \State $P := \hat{P}$
    \State \Return 
    \end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:fixed_grid_build} presents the fixed-grid build process. The routine starts by allocating an array of empty buckets, $Q$, which is populated based on the spatially hashed cell coordinates. The second for-loop in Algorithm~\ref{alg:fixed_grid_build} iterates through $Q$, looking for non-empty buckets. When one is found, nodes referenced by that bucket are transcribed/appended onto the ``sorted" list of nodes $\hat{P}$. This way the nodes in each cell are contiguous, but maintain the original ordering with respect to one another. Additionally, node indices in $\hat{P}$ replace the old indices within $Q$.

The entire build process complexity is proportional to $O(N)$, and requires $O((h_n)^d + N)$ storage. Samet \cite{Samet2005} would classify this approach as a \emph{fixed-grid bucket method with one-dimensional ordering}. The term \emph{bucket} refers to the allowance for each cell to contain an arbitrary number of nodes. \emph{One-dimensional ordering} is indicative of attempts later in the chapter to employ alternative space-filling curves in place of raster-ordering. 

A special note: the final step of Algorithm~\ref{alg:fixed_grid_build} overwrites the original list of nodes with the sorted equivalent. The spatially sorted list is included in the cost of stencil generation, but available for reuse elsewhere. Since the first step in RBF-FD applications is to generate stencils, overwriting the input node set can guarantee that the node values throughout the entire life-cycle of an RBF-FD application will benefit from the same spatial locality as stencil generation. This benefit is (almost) free. 

Consider Figure~\ref{fig:reorder_example}, which shows two differentiation matrices generated based on the same $N=6400$ MD-node set (unit sphere), with each row representing an RBF-FD stencil of $n=50$ non-zeros (blue dots). The left matrix in Figure~\ref{fig:reorder_example} is generated with stencils queried by a $k$-D Tree. The $k$-D Tree maintains the original ordering on $P$. The matrix on the right of Figure~\ref{fig:reorder_example} is a permuted equivalent of the left, but results from a fixed-grid sorted by raster-ordering with $h_n = 10$. 

Looking at Figure~\ref{fig:reorder_example} it should be obvious that reordering the nodes can improve memory access patterns for SpMV. If each row is applied as a sparse dot-product with a dense vector, the more condensed non-zeros are in the row, the more likely values from the dense vector will be resident in cache when needed. Likewise, non-zeros that appear on consecutive rows can benefit from cache reuse. Later in this chapter, the impact of spatial orderings are compared to determine how RBF-FD can benefit the most. 

\begin{figure}
\centering
\begin{subfigure}{0.425\textwidth}
\includegraphics[width=1.0\textwidth]{../figures/chapter2/hashing_example/bruteforce_N6400_n50-eps-converted-to.png}
\caption{$k$-D Tree} 
\end{subfigure} 
\begin{subfigure}{0.425\textwidth}
\includegraphics[width=1.0\textwidth]{../figures/chapter2/hashing_example/lsh_N6400_n50-eps-converted-to.png}
\caption{Fixed-Grid}
\end{subfigure}
\caption{Example effects of node reordering for MD node set $N=6400$ with $n=50$. The differentiation matrices are permuted equivalents and roughly $0.78\%$ full. Stencils generated based on $k$-D Tree maintain the original node ordering, while a fixed-grid with $h_n=10$ condenses non-zeros for improved memory access patterns (i.e., cache reuse).}
\label{fig:reorder_example}
\end{figure} 

\subsection{Fixed-Grid Neighbor Query}

Querying the $k$-nearest neighbors for a node, $X_q$, is the subject of Algorithm~\ref{alg:fixed_grid_query}. The process begins by finding $X_q$'s containing cell, $c$. The hash value of $c$ is used to identify (in $Q$) the list of nodes contained within that cell, all of which are appended to a vector of neighbor candidates, $pq$. 

It is possible for the number nodes in $c$ to exceed $n$; however, the algorithm conservatively assumes that it is necessary to search at least one halo of cells around it.  This ensures that nodes near the cell boundaries will find nearby neighbors outside of $c$, and the stencils will be balanced. Also, for certain fixed-grid resolutions, a single halo may not satisfy the stencil size requirements, so the algorithm iterates outward. As cells are checked, their nodes are appended onto $pq$. 

The final stage of Algorithm~\ref{alg:fixed_grid_query} calculates the distance from all candidate nodes to $X_q$, and uses that metric to sort $pq$ in ascending order. The first $n$ nodes in $pq$ are returned as the stencil. 
\begin{algorithm} 
\caption{QueryFixedGrid($X_q$, $n$, $P$, $Q$ )}         
\label{alg:fixed_grid_query}  
\begin{algorithmic}[1]    
    \State \textbf{Input:} A query point, $X_q$; the desired number of neighbors, $n$; a set of $d$-dimensional points $P$; and the matching cell bucket list, $Q$.
    \State \textbf{Output:} The $n$-nearest neighbors list $pq$.
    \State     
    \State $halo := 1$
    \State $c := \text{CellCoords}(X_q)$ 
    \State $ind := \text{SpatialHash}(cells)$
    \State Append $P[Q[ind]]$ onto $pq$
    \While{$pq$.size $< n$ OR $halo < 2$}
        \State $cells := \text{NeighboringCellCoords}(c, halo)$
        \State $inds := \text{SpatialHash}(cells)$
        \For{each $q$ in $Q[inds]$}
            \If{$q$ is not empty} 
            \State Append node list $P[q]$ onto $pq$
            \EndIf
        \EndFor
        \State increment $halo$
    \EndWhile
    \State $dists := \text{ComputeDistances}(pq)$
    \State Sort $pq$ by $dists$
    \State \Return the first $n$ nodes in $pq$
    \end{algorithmic}
\end{algorithm}


Complexity of Algorithm~\ref{alg:fixed_grid_query} can vary based on the choice of $h_n$. For a sufficiently refined fixed-grid the $k$-ANN is dominated by the cost of the \emph{while-loop} and behaves as $O(\log h_n)$ per stencil. In the worst case, when $h_n$ is small, the cost of sorting $pq$ dominates, and is proportional to $O(N \log N)$ (using a C++ STL Sort) in the worst case. %TODO: verify with Gordon.  
Results below demonstrate that proper choice of $h_n$ can maintain logarithmic complexity similar to the $k$-D Tree query. 


The fixed grid query algorithm is considered an \emph{approximate nearest neighbor} (ANN) search. Consider again the nodes in Figure~\ref{fig:nearest_neighbor_example}. A $k$-NN stencil of size $n=8$ should contain the blue center, all blue nodes, plus the red node and one black node. The true $k$-NN would select the black node in the right-most column of the grid (i.e. the node closer to the dashed ball query). Under the fixed-grid method, however, the alternate black node is selected even though it is more distant. This happens because the more distant black node occupies the second halo of cells around the stencil center, whereas the true near neighbor is in the third. Algorithm~\ref{alg:fixed_grid_query} is able to truncate the search in the second iteration by satisfying the requirement on $n$. 

Similarly, if cells are rectangular in shape, the ball-query under fixed-grid functions as an ellipsoid. In this case, stencils are biased with more nodes in one direction. To combat this, and ensure spherical stencils, this work assumes the AABB bounding the domain is a cube (i.e., $dx = dy = dz$). 

The difference between a true $k$-NN and $k$-ANN is insignificant from the perspective of RBF-FD. The method compensates automatically for differences in node locations when weights are calculated. Additionally, the only differences in stencils generated by $k$-NN versus $k$-ANN occur at nodes on the outermost reaches of the stencil (i.e., nodes with the least impact on the stencil center).


%\begin{figure}
%\centering
%\includegraphics[width=8.5cm]{rbffd_methods_content/neighbors/neighbor_incorrect.png}
%\caption{A stencil generated with $k$-ANN satisfies the required stencil size, but is not guaranteed to choose the true nearest neighbors.}
%\label{fig:approximate_nearest_neighbors}
%\end{figure}


\section{Performance Comparison}

The implementation of $k$-D tree compared in this work, the \emph{$kd$-Tree Matlab} library, was originally posted to the Matlab FileExchange in 2008 \cite{TagliasacchiMFE} and now maintained as an independent Google Code project \cite{TagliasacchiGC}. The implementation is written in C++, but includes a MEX compiled interface, allowing for a consistent and efficient $k$-D Tree API in both languages. 
The original release of $kd$-Tree Matlab (pre-2012) was in use throughout the RBF community at the onset of this work. The dual language API is appealing for rapid-prototyping with MATLAB, and then porting applications to C++. 

The pre-2012 implementation of $kd$-Tree Matlab followed the $O(N \log N)$ expected complexity for neighbor queries, but cost significantly more to build the tree. While build times for small and medium sized grids (i.e., less than $N=50000$ nodes) were small enough to be inconspicuous, the implementation scaled as $O(N^2)$ making it prohibitively expensive for large problems. 

The high cost ultimately led to work on a fixed-grid method to test the concept of neighbor queries with low build costs. The original prototype developed in pure MATLAB (\cite{BolligRBFFixedGrid}), outperformed the ``efficient" MEX-compiled $k$-D Tree for problem sizes $N > 20000$, and lead to the development of a C++ implementation tested here. In the 2012 release of $kd$-Tree Matlab (\cite{TagliasacchiGC}), the author has significantly improved the performance of the build process to achieve the $O(N \log N)$ behavior expected for a Point $k$-D Tree with cyclic splitting as presented above. 

All benchmarks in this section were performed on the Itasca HPC cluster at the Minnesota Supercomputing Institute. Itasca is an HPLinux cluster with 1,134HP ProLiant blade servers, each with two-socket, quad-core 2.8 GHz Intel Xeon processors sharing at least 24GB of RAM \cite{MSIItasca}. Both $k$-D Tree and fixed-grid implementations are compiled with the Intel compiler toolchain (v13), and the ``-O3" optimizations for auto-vectorization, loop unrolling, etc.. 

Figure~\ref{fig:stencil_query_old_and_new} demonstrates the performance of the $k$-D Tree and the fixed-grid method on increasing 3-D regular grid resolutions up to four million nodes. Both the current $k$-D Tree implementation (2012) and the original implementation (pre-2012) are shown in Figure~\ref{fig:sten_query_a} as evidence of the significant improvement in the latest release. For comparison, the C++ fixed-grid method is shown with two resolutions: $h_n = 50$ and $h_n=100$. On the bottom, Figure~\ref{fig:stencil_query_old_and_new} shows the associated speedups---defined as the ratio of time to compute $k$-D Tree stencils, over the time for the same stencil generation with a fixed-grid---achieved over the more efficient release of $kd$-Tree Matlab. These results cover the total time to build the data-structure and query all stencils. The test grid is generated in raster-order, as is the fixed-grid. 


Prior to the 2012 improvements, the C++ implementation was over 130x faster for four million nodes. With the newer and more reasonable $k$-D Tree benchmarks, the fixed-grid method is only 2x faster in the best case shown here. Although 2x is not as impressive, it is a notable improvement. One issue in Figure~\ref{fig:stencil_query_old_and_new} is that the fixed-grid with $h_n=50$ is consistently more than twice as slow as the $k$-D Tree. This is due to under-resolved cells with 32 nodes per cell; $h_n=100$ has only 4 nodes per cell. For small $N$ the $k$-D Tree is faster than both resolutions of fixed grid due to over-resolution. As $N$ increases beyond one million nodes the $h_n=100$ fixed-grid loses ground on the $k$-D Tree due to under-resolution. 


\begin{figure}
\centering
\begin{subfigure}{11cm}
\centering
\includegraphics[width=\textwidth]{../figures/stencils/kdtree_old_reg_subsets_4m_stencil_gen_time.png}
\caption{3-D Regular Grid with stencil size $n=50$}
\label{fig:sten_query_a}
\end{subfigure}
\begin{subfigure}{9.5cm}
\centering
%\includegraphics[width=9.5cm]{../figures/stencils/reg_subsets_4m_stencil_gen_time.png}
\includegraphics[width=\textwidth]{../figures/stencils/reg_subsets_4m_stencil_gen_speedup.png}
\caption{Speedup of fixed-grid method versus $k$-D Tree}
\end{subfigure}
%TODO: regenerate with true RG generated at specified resolutions
\caption{Querying the $n=50$ nearest neighbors on a regular grid up to $N=160^3$ demonstrates the gains achieved by the fixed-grid neighbor query method.}
\label{fig:stencil_query_old_and_new}
\end{figure}


Similar behavior is seen in Figure~\ref{fig:speedup_sphere} where the fixed-grid and $k$-D Tree are compared for various discretizations of the unit sphere. Each of the node sets described in Figure~\ref{fig:speedup_sphere} are discussed in Chapter~\ref{chap:rbffd_method} and available for download (\cite{BolligSphereGrids}). A range of resolutions are covered from a few hundred up to one million with some overlap. Each distribution (MD, Icosahedral, CVT) are generated differently and the nodes are naturally spatially sorted (icosahedral) or random (MD, CVT). Due to sorting in the fixed-grid method, node sets that are originally random exhibit more gain over $k$-D Tree. This is most evident in Figure~\ref{fig:speedup_sphere}, where different slopes appear for the segment corresponding to MD nodes and similar resolutions of Icosahedral nodes. The speedup curves for $h_n=50$ and $h_n=100$ demonstrate the dependence of fixed-grid method's success on the proper choice of $h_n$. When over-resolved (i.e., for $N < 10000$) the $k$-D Tree performs best. Under-resolution degrades performance rapidly starting at $N=100000$ for $h_n=50$, and $N=500000$ for $h_n=100$. 


\begin{figure}
\centering
\includegraphics[width=10.5cm]{../figures/stencils/sphere_stencil_gen_speedup.png}
\caption{Fixed-grid speedup versus $k$-D Tree with stencil size $n=50$.}
\label{fig:speedup_sphere}
\end{figure}


The speedup curves in Figures~\ref{fig:stencil_query_old_and_new} and \ref{fig:speedup_sphere} hint at a maximum value $h_n$ for which the fixed-grid will outperform its competitor, and with that value: a maximum speedup possible. In Figure~\ref{fig:cvt_hn_speedup}, the $N=10^6$ resolution CVT is used to generate stencils of $n=50$, with range of values for $h_n$. The maximum of this curve shows that the fixed-grid can achieve up to 2.4x better than the $k$-D Tree for $h_n=160$. In this case the number of cells, $(h_n)^3$, sufficiently resolves the domain for most cells intersecting the sphere to have one or two nodes. At $h_n=70$ the fixed-grid is on par with $k$-D Tree; most cells have less than 8 nodes, which is consistent with the resolution sought by \cite{Krog2010,Goswami2010,Green2010}.

% NOTE: for 160^3 fixed grid for a CVT with 1-million nodes we have the following distribution: 
%  Count \mid Nodes Per Cell
%========================
% 3560709 0
% 409034 1
%  44542 2
%  13669 3
%  11480 4
%  11780 5
%  11422 6
%  10341 7
%   8518 8
%   6077 9
%   4037 10
%   2271 11
%   1190 12
%    552 13
%    243 14
%     86 15
%     38 16
%      9 17
%      1 19
%      1 20
%%%%%
%% Mean :               160            140        180         120            70
% exclude (0, 1, 2)	6.14186            % exclude (0,1)	    4.68066          % exclude (0)	        1.86814        2.02195     1.74984     2.24683        4.24401% All	0.244141


\begin{figure}
\centering
\includegraphics[width=9.5cm]{../figures/stencils/cvt1m_stencil_gen_speedup.png}
\caption{Fixed-grid speedup versus $k$-D Tree on a one-million node CVT (unit sphere), with stencil size $n=50$.}
\label{fig:cvt_hn_speedup}
\end{figure}

%TODO: repeat cvt_hn_speedup but get the number of nodes per cell.
\subsection{Impact on SpMV}

Based on evidence so far, the fixed-grid method is not a big winner against the $k$-D Tree, but it does eke out a small victory with the right choice of $h_n$. The reality is that stencil generation only occurs once, and the difference between $k$-D Tree and fixed-grid is easily amortized by iterations during the bulk of the RBF-FD application phase. However, as Figure~\ref{fig:spmv_impact_rg} illustrates, the fixed-grid method includes another longer lasting benefit. Namely, the Sparse Matrix-Vector Multiply (SpMV)---the major overhead in RBF-FD applications---benefits positively due to the reordering that occurs during stencil generation. Reordering cells improves locality of nearby nodes and associated values, and leads to up to 5x speedup in the SpMV for random node distributions. 

Regular grid and icosahedral sphere test cases in Figure~\ref{fig:spmv_impact_rg} see no more than 5\% improvement, which is unsurprising as their nodes are previously sorted. CVT and MD node sets, however, see a maximum speedup of 5x and 1.4x respectively. Furthermore, none of the test cases result in SpMV slowdown. Figure~\ref{fig:spmv_vs_hn} demonstrates the impact on SpMV as a function of $h_n$ for the $N=10^6$ CVT test case. The results show that even a coarse fixed-grid ($h_n \ge 40$) is capable of achieving the spatial locality of data that results in 5x faster SpMV. The combined results of 2x faster stencils and 5x faster SpMV make a case for the use of a fixed-grid query as a general safety net to precondition RBF-FD when properties of the input grid are unknown. 

\begin{figure}
\centering
\begin{subfigure}{9.5cm}
\includegraphics[width=\textwidth]{../figures/stencils/reg_subsets_4m_spmv_speedup.png}
\caption{3-D Regular Grid}
\end{subfigure}
\begin{subfigure}{10.5cm}
\includegraphics[width=\textwidth]{../figures/stencils/sphere_spmv_speedup.png} 
\caption{Unit Sphere}
\end{subfigure}
\caption{Fixed-grid impact on SpMV for stencil size $n=50$.}
\label{fig:spmv_impact_rg}
\end{figure}


%TODO: add function for N=1e5, 1e4 to see as a function of N and hnx. 
\begin{figure}
\centering
\includegraphics[width=7.5cm]{../figures/stencils/cvt1m_spmv_speedup.png} 
\caption{Impact on SpMV performance for $N=10^6$ CVT of the unit sphere due to choice of $h_n$.}
\label{fig:spmv_vs_hn}
\end{figure}


%
%\begin{figure}
%\centering
%\includegraphics[width=9.5cm]{../figures/stencils/kdtree_old_reg_subsets_4m_stencil_gen_speedup.png}
%\includegraphics[width=\textwidth]{../figures/stencils/sphere_subsets_1m_stencil_gen_time.png}
%\caption{Querying the $n=50$ nearest neighbors on a regular grid up to $N=160^3$ demonstrates the significant gains achieved by our spatially binned neighbor query. While KDTree queries grow as $O(N log N)$}
%\label{fig:hash_results}
%\end{figure}
%
%\begin{figure}
%\centering
%\includegraphics[width=7.5cm]{../figures/stencils/sphere_subsets_1m_stencil_gen_speedup.png}
%\includegraphics[width=7.5cm]{../figures/stencils/sphere_subsets_1m_spmv_speedup.png}
%\caption{Generating stencils for increasing subsets of the $N=10^6$ CVT nodes mesh.}
%\label{fig:hash_results}
%\end{figure}







\section{Alternative Orderings}

As previously mentioned, the choice of space filling curve influences the sparsity pattern of differentiation matrices, which in turn impacts cache effects. The use of $Z$-ordering (Figure~\ref{fig:space_filling_curves}) for potential benefits on memory access is a common recommendation in fixed-grid methods (see e.g., the suggestions by \cite{Johnson2011,Green2010,Krog2010} and implementations in \cite{Goswami2010, MellorCrummey2001}). Under raster-ordering, cells are traversed with priority on one dimension. The resulting layout only has adjacent cells along that one dimension consecutive in memory. Other approaches, like $Z$-ordering, traverse cells by alternating dimensions/directions and increase locality of data in multiple dimensions. 

This section investigates the potential benefits of $Z$-ordering and various other space filling curves implemented as part of the original MATLAB fixed-grid prototype (\cite{BolligRBFFixedGrid}). Examples of the tested curves are presented in Figure~\ref{fig:orderings}. Here each node correlates to a fixed-grid cell, and curves start at coordinates $(0,0)$. 

Of the six cases shown in Figure~\ref{fig:orderings}, five of them (Raster, X, Z, U, 4-node Z) are directly produced via \emph{bit-interleaving} hash functions. Those same tiles are named based on the hierarchical pattern of connected nodes and groups of nodes. 

The sixth tile in Figure~\ref{fig:orderings} shows a Reverse Cuthill-McKee (RCM) ordering. The RCM method is distinct from the other orderings in that it is a \emph{bandwidth reduction} algorithm that focuses on condensing the sparsity pattern of differentiation matrices around the diagonal rather than spatially sorting nodes. Figure~\ref{fig:orderings} shows a special case that results when all nodes have stencils of size $n=3$. It nicely illustrates the RCM as a breadth-first traversal through the grid. More discussion on RCM occurs later in this section. 
\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{rbffd_methods_content/hashing/node_orderings.png} 
\caption{Example space filling curves used to reorder cells/nodes. The Raster, $Z$-, $X$-, $U$- and 4-nodes per Edge $Z$-orderings are space filling curves applied to reorder cells of the fixed-grid stencil queries. Reverse Cuthill-McKee (RCM) operates on output stencils and their associated adjacency graph. The RCM shown here is a special case with only 3 neighbors per node.}
\label{fig:orderings}
\end{figure}

%TODO Investigations such as \cite{MellorCrummey2001} consider the impact of orderings on multiple levels of memory hierarchy

%TODO: ordering occurs when nodes are hashed to cell. Hash node with IJK, convert ijk to ordering, hash neighbors with IJK, convert ijk to ordering. Its a prototype, so performance of the conversion is not paramount. 
%TODO: why x, u, z, 4-node z? Originally it was Z, but the 


\subsection{Bit-Interleaved Orderings}

Bit-interleaving zips bits from two or more integers into one. For example, given two integers, $X$, and $Y$ the bits are interleaved as:
\begin{align}
X &= \{x_3x_2x_1x_0\} \nonumber \\
Y &= \{y_3y_2y_1y_0\} ,\nonumber \\
interleave(X,Y) & = \{x_3y_3x_2y_2x_1y_1x_0y_0\}.
\end{align}
Including a third value, $Z$, results in the following: 
\begin{align*}
interleave(X,Y,Z) & = \{x_3y_3z_3x_2y_2z_3x_1y_1z_3x_0y_0z_3\}.
\end{align*}

Interleaving depends on \emph{integer dilation} to modify an input integer value, so the output value has the original input bits spaced by new zeros. On a $d$-dimensional domain, bit-interleaving requires $d-1$ zeros between each bit. For example, dilating $1111_2$ expands the four bit integer to ${\color{red}0}1{\color{red}0}1{\color{red}0}1{\color{red}0}1_2$ in 2-D, and ${\color{red}0}{\color{red}0}1{\color{red}0}{\color{red}0}1{\color{red}0}{\color{red}0}1{\color{red}0}{\color{red}0}1_2$ in 3-D. 


The process to dilate integers applies multiple bit-mask operations to spread the digits appropriately. 
For any $b$-bit integer, $X_0$, the dilation is given by the following iteration:
\begin{align}
X_{i+1} = ((X_i \ll S_i) \mid X_i)\ \&\ M_i \ \ \ \ \ \text{for }i=0,...,\log_2(b)
\label{eq:dilate}
\end{align}
where operators $\ll$, $\mid$, and $\&$ indicate left-shift, bit-wise OR, and bit-wise AND operators respectively. In general, a $b$-bit integer requires $\log_2(b)$ iterations through the dilation kernel. The masks ($M_i$) and shifts ($S_i$) necessary to dilate a 32-bit integer are provided in Table~\ref{tbl:bitmasks2D} for 2-D, and Table~\ref{tbl:bitmasks3D} for 3-D. Both binary and hexadecimal representations are provided for $M_i$. Observe that the shifts and bit-masks applied during each iteration depend on the size of the integer and the dimension (refer to \cite{Stocco2009} for more details). 


\begin{table}
\centering
\caption{2-D Integer Dilation Masks for 32-Bit Integers}
\label{tbl:bitmasks2D}
\begin{tabular}{ c | c | c | c }
Level ($i$) & \multicolumn{2}{ c | }{Mask ($M_i$)} & $S_i$ \\ 
\hline
0 & $00000000000000001111111111111111_2$  & $0\text{x}0000ffff$  & 32 \\
1 & $11111111000000000000000011111111_2$  & $0\text{x}ff0000ff$  & 16 \\
2 & $00001111000000001111000000001111_2$  & $0\text{x}0f00f00f$  & 8 \\
3 & $11000011000011000011000011000011_2$  & $0\text{x}c30c30c3$  & 4 \\
4 & $01001001001001001001001001001001_2$  & $0\text{x}49249249$  & 2 \\
\hline
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{3-D Integer Dilation Masks for 32-Bit Integers}
\label{tbl:bitmasks3D}
\begin{tabular}{ c | c | c | c }
Level ($i$) & \multicolumn{2}{ c | }{Mask ($M_i$)} & Shift ($S_i$) \\ 
\hline
0 & $00000000000000001111111111111111_2$  & $0\text{x}0000ffff$  & 48 \\
1 & $00000000000000000000000011111111_2$  & $0\text{x}000000ff$  & 24 \\
2 & $00000000000011110000000000001111_2$  & $0\text{x}000f000f$  & 12 \\
3 & $00000011000000110000001100000011_2$  & $0\text{x}03030303$  & 6 \\
4 & $00010001000100010001000100010001_2$  & $0\text{x}11111111$  & 3 \\
\hline
\end{tabular}
\end{table}


%At the end of Equation~\ref{eq:dilate}, the completely dilated integer is stored in $X_5$ with only one caveat: d
It is possible to lose bits in translation when dilating values that require greater than $\left\lfloor ^b/_d \right\rfloor$ bits. That means, in the case of 32-bits, any input to Equation~\ref{eq:dilate} must be representable in 16-bits or less (i.e., $X_0 \leq 65535 $) for 2-D, and 10-bits or less (i.e., $X_0 \leq 1023$) for 3-D. Note that these limits then become the maximum resolution possible for $h_n$ in the fixed-grid method. 

With dilation available, the process for 2-D bit-interleaving reduces to:
\begin{align}
(d_x, d_y) = (dilate(X), dilate(Y)) \nonumber \\
interleave(X,Y) = (d_x \ll 1) \mid d_y. 
\label{eq:interleave2d}
\end{align}
And in 3-D: 
\begin{align}
(d_x, d_y, d_z) = (dilate(X), dilate(Y), dilate(Z)) \nonumber \\
interleave(X,Y,Z) = ((d_x \ll 2) \mid (d_y \ll 1)) \mid d_z.
\label{eq:interleave3d}
\end{align}
Equations~\ref{eq:interleave2d} and \ref{eq:interleave3d} can be used as hash functions to produce a vast number of space filling curves (see \cite{Stocco2009}).

By default, Equation~\ref{eq:interleave2d} results in the $Z$-ordering in Figures~\ref{fig:space_filling_curves} and \ref{fig:orderings}. Consider for example:
\begin{align}
(c_x,c_y) & = (5, 3) = (0101_2, 0011_2) \nonumber \\
(d_x,d_y) & =  ({\color{red}0}0{\color{red}0}1{\color{red}0}0{\color{red}0}1_2, {\color{red}0}0{\color{red}0}0{\color{red}0}1{\color{red}0}1_2) \nonumber \\
interleave(c_x, c_y) & = ( 00100111_2 ) = 39. \nonumber
\end{align}
% 0000000100010
%  000000000101
% 0000000100111 = 32 + 7
The mapped $Z$-index of 39 can be verified by counting the number of steps from  $(0,0)$ to $(5,3)$ on the $Z$-order curve in Figure~\ref{fig:orderings}. 

The $X$- and $U$- orders shown in Figure~\ref{fig:orderings} also result from Equation~\ref{eq:interleave2d} with only slight modifications to the input. Table~\ref{tbl:orderings} provides a comparison of orderings and the operators/inputs in 2-D and 3-D that produce them. The $U$ and $X$ orderings depend on a bit-wise XOR operator, $\oplus$, to combine coordinates before interleaving. 

\begin{table}
\centering
\caption{Integer Dilation Interleaving Operators}
\label{tbl:orderings}
\begin{tabular}{ c | c | c }
  Ordering & 2-D & 3-D \\
  \hline                        
  IJK & $(X * h_n) + Y$ & $((X * h_n) + Y)*h_n + Z$ \\
  Z & $interleave(X, Y)$ & $interleave(X, Z, Y)$ \\
  U & $interleave(X, X \oplus Y)$ & $interleave(X, Z, Z \oplus Y)$ \\
  X & $interleave(X \oplus Y, X)$ & $interleave(X \oplus Y, Z, Y)$ \\
  4-Node Z * & $(d_x \ll 2) \mid d_y$ & $((d_x \ll 4) \mid (d_z \ll 2)) \mid d_y$ \\
  \hline  
\end{tabular}
\end{table}


The last row of Table~\ref{tbl:orderings}, the 4-node $Z$-curve, is a special case of bit interleaving. The 4-node Z is similar in theory to a standard $Z$-order, except when coordinates are dilated an extra extra zero per bit is requested, as though the dilation were operating in the next higher dimension. Then, using the operators provided in Table~\ref{tbl:orderings} to interleave bits results in the following pattern:
\begin{align}
interleave_{4node}(X,Y) = \{x_5x_4y_5y_4x_3x_2y_3y_2x_1x_0y_1y_0\}.
\end{align}
By reserving two bits for each dimension, the 4-node $Z$-curve is able to traverse up to four nodes in the $Y$ direction before taking its first step in the $X$ direction. 

%TODO: why was each curve chosen? 
% Z because of others like Goswami; U because it also widely used (gray-code)


Often, interleaved cell coordinates near the boundaries of the domain do not produce contiguous hash indices and may balloon to values greater than the number of nodes/cells in the domain. In these situations, missing indices reference coordinates outside the domain that would complete the hierarchical power-of-2 curves before entering the domain again. When sorted and traversed least to greatest, the hashed indices give a proper ordering of the domain, and missing indices are skipped. 

%TODO: how are reorderings applied


\subsection{Bandwidth Reduction and Reverse Cuthill-McKee} 

The purpose of reordering nodes/cells is to improve memory access patterns. If condensing non-zeros of a sparse matrix achieves this, then the ideal case arises when $n$ non-zeros occupy the first $n$ diagonals of the matrix (including the main diagonal) for all rows. In terms of stencils on a grid, this ideal case corresponds to all nodes on a 1-D line connected to their left and right nearest neighbors. In higher dimensions, reproducing the ideal structure requires one to neglect spatially nearest neighbors, except in cases of very low resolution or small stencil sizes, and select neighbors by following a space filling curve left and right. Stencils in 2-D or 3-D constructed through $k$-NN (or $k$-ANN) queries result in a sparsity pattern with gaps between non-zeros. In the worst case, matrices would have non-zeros in each row spaced as far apart as possible while maintaining the appropriate connectedness of the adjacency graph. 


As a measure of how well condensed non-zeros are, the \emph{bandwidth} of a matrix, $A$, is defined as (\cite{Cuthill1969, LiuSherman1976}): 
\begin{align}
bw(A) = \max\{|i - j| + 1 : A_{i,j} \neq 0\}.
\end{align}
The bandwidth simply counts the number of super- and sub-diagonals---i.e., diagonals above and below the main diagonal---that contain at least one non-zero. Incrementing by one accounts for a non-zero main diagonal. The lower the bandwidth, the better. 

For any matrix, $A$, a permutation, $P$, can be constructed such that 
\begin{align}
P A P^T
\end{align}
reorders the matrix and reduces the bandwidth of $A$. A variety of \emph{bandwidth reduction} algorithms exist with unique approaches for the construction of $P$ (see e.g., \cite{Gibbs1976, LiuSherman1976, MellorCrummey2001}).  In contrast to the space filling curves discussed above, bandwidth reduction algorithms are applied \emph{after} stencils are generated in order to operate on the differentiation matrix and corresponding adjacency graph. 

The effect pre- and post-multiplying $A$ by $P$ is equivalent to reindexing nodes and and updating all stencil connections to the new numbering. In practice, rather than construct a full permutation matrix $P$, which has exactly one 1 per row and per column, it is common to represent $P$ as a vector where each entry indicates the column location of the 1. In that case the vector elements reference the original node ordering, and the indices of those elements provide the new ordering. 

The most popular methods for bandwidth reduction are Cuthill-McKee (CM) algorithms \cite{Cuthill1969, LiuSherman1976}. In particular this work is concerned with the \emph{Reverse Cuthill-McKee} (RCM) variant. The authors of \cite{LiuSherman1976} show that, in comparison to regular CM, the RCM algorithm offers better conditioning of $PAP^T$ for reduced fill-in and reduced computation during matrix decompositions. Although fill-in is not a concern in this work, the RCM variant is attained at no extra cost.

At a high level, CM methods start by selecting a node in the adjacency graph with the lowest degree (i.e., fewest non-zeros per row in the matrix). From that node, a breadth-first traversal begins, passing through all connected neighbors in order from least to greatest degree. If nodes have the same degree, as is the situation for RBF-FD with stencil size, $n$, then ties are broken arbitrarily. The original indices for visited nodes are pushed onto a queue, $r$. Another queue is maintained to catalogue previously traversed nodes and prevent the process from doubling back on itself. In the event that the adjacency graph has multiple disconnected sub-graphs, the algorithm exhausts the first sub-graph before beginning the next; as always, beginning with the lowest degree node available. 

CM methods produce $r$ as the vector representation of $P$. The RCM variant differs from CM by reversing the order of $r$ so that rows corresponding to the last traversed nodes are at the top of $PAP^T$. 

Most CM and RCM implementations assume the algorithms operate on square, symmetric adjacency matrices. In the case of RBF-FD the adjacency matrices are symmetrized with $A+A^T$. For rectangular matrices with dimensions $N \times M$ (as are encountered in Chapter~\ref{chap:distributed_rbffd}), RCM is applied only to the $N\times N$ square block with symmetry similarly constructed. 


%Figure~\ref{fig:rcm_example} provides an example of RCM bandwidth reduction for the adjacency graph produced by $N=8^2$ nodes, each with a stencil of size $n=5$ (320 non-zeros). The grid of nodes is originally raster-ordered. In Figure~\ref{fig:rcm_dm_before_after} the matrix before and after RCM is shown with the bandwidth, $bw$, included in the $x$-label. 
%
%\begin{figure}
%\centering
%\begin{subfigure}{12.5cm}
%\centering
%\includegraphics[width=\textwidth]{rbffd_methods_content/hashing/rcm_example_dm.eps} 
%\label{fig:rcm_dm_before_after}
%\caption{DM before and after}
%\end{subfigure}
%\begin{subfigure}{12.5cm}
%\centering
%\includegraphics[width=\textwidth]{rbffd_methods_content/hashing/node_ordering_rcm_n5.eps} 
%\caption{The RCM Ordering Curve}
%\label{fig:rcm_curve}
%\end{subfigure}
%\caption{An RCM reordering of $N=64$ nodes with stencil size $n=5$.}
%\label{fig:rcm_example}
%\end{figure}



%TODO: metric; mean speedup in SpMV for range of orderings based on optimal h_n per grid resolution. 
%TODO: metric; ideal h_n per grid resolution; sphere. 

\subsection{Impact of Orderings}

To compare the impact of orderings in Figure~\ref{fig:orderings}, the MATLAB implementation of fixed-grid (\cite{BolligRBFFixedGrid}) generates stencils and writes the sorted nodes and stencils to disk. The nodes and stencils are accepted as input for the C++ SpMV benchmark used above to compare 
the $k$-D Tree and fixed-grid stencils. Visualization and bandwidth calculation are both managed in MATLAB. 
Bit interleaving occurs in a MATLAB script that leverages the internal \emph{bitshift}, \emph{bitand}, \emph{bitor}, and \emph{bitxor} routines for bitwise operations. Bitwise operators in MATLAB are restricted to 32-bits, but the maximum number of cells per dimension allowed by 32-bits ($\leq 1023$ in $3$-D) is more than sufficient. The MATLAB internal \emph{symrcm} computes the Reverse Cuthill-McKee reordering. % based on the output from the raster-order (IJK) stencil generation. 
Since the same number of non-zeros exist per row, RCM selects a different starting node based on the input ordering of nodes; this allows for some variance in the bandwidth. For consistency, results shown here apply RCM to the output of a raster-ordered fixed-grid query. 

Figure~\ref{fig:ordering_impact_rg} illustrates six sparsity patterns resulting from the curves in Figure~\ref{fig:orderings}. The test case is a regular grid of $N=18^3$ nodes, each with stencil size $n=31$. From left to right, the top row shows raster-ordering, Reverse Cuthill-McKee, and the 4-node Z order. The second row shows $X$-, $Z$-, and $U$-orders generated with bit-interleaving. The fixed-grid resolution in each dimension is $h_n = 6$. Bandwidth, $bw$, is included in the bottom label for each matrix, as well as the number of non-zeros, $nz$. Of all the orderings in Figure~\ref{fig:ordering_impact_rg}, RCM is by far the smallest bandwidth ($bw=858$). The runner up, with a 30\% wider bandwidth, is raster-ordering.  The four remaining options are aesthetically pleasing, but their non-zero distributions have significantly less structure. 

In similar fashion, Figure~\ref{fig:ordering_impact_cvt} provides the six orderings for $N=4096$ MD-nodes on the unit sphere. Each row has $n=31$ non-zeros, but in this case the fixed-grid resolution in each dimension is $h_n = 10$. Once again the top two orderings by bandwidth are RCM and raster-ordering. 

While bandwidth is significant, the end-goal of reordering is to improve the benchmark for SpMV. Consider Table~\ref{tbl:ordering_impact}, which lists ordering impact on $N=10^6$ CVT nodes on the unit sphere with $n=50$ and $h_n=160$ (i.e., the optimal $h_n$ found in Figure~\ref{fig:cvt_hn_speedup}). 
%Bandwidths on the first row reveal the raster-/IJK-order is again second only to RCM for bandwidth. 
The first row of Table~\ref{tbl:ordering_impact} provides bandwidths by order, and the second shows the speedup gained in the SpMV versus the benchmark for raster-ordering. In this case, the RCM has both the lowest bandwidth and highest yield in the benchmark with a 9\% improvement (5.06x faster than $k$-D Tree). Surprisingly the $Z$- and $X$-orderings are not far behind RCM even with 80--120x wider bandwidths. 
%TODO: more? 

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{rbffd_methods_content/hashing/spy_regulargrid_N18d3_n31_hn6.png} 
\caption{Impact of node orderings on an $N=18^3$ regular grid in 3-D. Stencil size $n=31$, fixed-grid resolution $(h_n)^d=6^3$. }
\label{fig:ordering_impact_rg}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{rbffd_methods_content/hashing/dm_node_orderings_md4096_n31_hn10.png} 
\caption{ The impact of reordering on sparsity patterns for $N=4096$ MD nodes on the sphere. Stencil size $n=31$, fixed-grid resolution per dimension $h_n=10$. }
\label{fig:ordering_impact_cvt}
\end{figure}

\begin{table}[h]\footnotesize
  \centering
  \caption{Ordering Comparison for $N=10^6$ CVT unit sphere. Stencil size $n=50$, fixed-grid resolution, $h_n=160$}
  \label{tbl:ordering_impact}
  \begin{tabular}{ c | c | c | c | c | c | c }
               & IJK & RCM & Z & X & U & 4-node Z \\
               \hline
   Bandwidth   &  7885	& 6566	& 575606 &	819312 & 611513 & 513579   \\
   \hline
   SpMV Speedup & 1 &	1.09 & 1.06 & 1.06 & 1.04 & 0.94 \\
  \end{tabular}
\end{table}


%\begin{table}[h]\footnotesize
%  \centering
%  \caption{Ordering Comparison for $N=10^5$ CVT unit sphere. Stencil size $n=50$, fixed-grid resolution, $h_n=100$}
%  \begin{tabular}{ c | c | c | c | c | c | c }
%               & IJK & RCM & Z & X & U & 4-node Z \\
%               \hline
%   Bandwidth   &  8209	& 12973	 & 70545	& 87983 & 73699 & 70573   \\
%   \hline
%   SpMV Speedup vs IJK & 1 &	1.005 & 1.004 & 0.98 & 1.01 & 1.01 \\
%  \end{tabular}
%\end{table}

%TODO: possibility that majority of non-zeros are clustered near diagonal and outliers are deceiving the bandwidth metric. 


\section{Conclusion and Future Work}

This chapter introduced a fixed-grid method for RBF-FD stencil generation, and compared it to an efficient implementation of $k$-D Tree. The method proves itself capable of generating stencils twice as fast as $k$-D Tree.  
%---previously employed in work on RBF Partition of Unity schemes \cite{WendlandBook,Wendland2002} and 
%particle-based methods like Smoothed Particle Hydrodynamics \cite{Johnson2011,Green2010,Krog2010}---never 
%to RBF collocation method or RBF-FD---
Reordering nodes during stencil generation improves later performance of some RBF-FD solutions by a factor of five, but with the help of bit-interleaving and space filling curves that performance can be boosted slightly more.

In addition to ordering nodes based on space filling curves, a bandwidth reduction algorithm named Reverse Cuthill-McKee was also employed to reorder the differentiation matrix. Reverse Cuthill-McKee results show that not only is the algorithm exceptional at reducing the bandwidth, it also results in the most improvement to SpMV benchmarks. 

The implementation benchmarked above was developed as a pure CPU prototype with minimal attention to optimization. Although RBF-FD only requires neighbor queries once, the fixed-grid method's long lasting positive impact on memory is sufficient to justify its continued use. 

The C++ fixed-grid stencil generator defaults to the Raster-/IJK-order due to its ability to produce consistently small bandwidths. For situations when reordering is necessary for the best performance, the Reverse Cuthill-McKee algorithm is employed. In order to avoid MATLAB, RBF-FD applications in C++ rely on an implementation of the Cuthill-McKee algorithm provided by the BOOST library \cite{BoostSite}. 


%TODO: Similar conclusions are drawn by the authors of \cite{MellorCrummey2001}, who found that reordering nodes via RCM and space filling curves offer similar benefits in terms of reduced TLB misses and better cache coherency. 


Due to the limited significance of stencil generation under RBF-FD, the overhead in implementing and debugging a GPU implementation of the fixed-grid method is hard to justify. No known investigations consider RBF-FD on moving nodes, but such research could prove local refinements  (e.g., \cite{FlyerLehto10}) should find the fixed-grid method significantly relevant. Future work in this direction would easily justify a GPU implementation following \cite{Krog2010,Green2010,Johnson2011, Goswami2010}. 

It also is worth noting that recent work by Connor and Kumar \cite{Connor2009} developed operators to directly transform floating point coordinates into $Z$-orderings. The MATLAB fixed-grid prototype (\cite{BolligRBFFixedGrid}) includes a test of those operators, although the investigation is still underway. 

%
%\begin{figure}
%\centering
%\begin{subfigure}{0.45\textwidth}
%\centering
%\includegraphics[width=\textwidth]{rbffd_methods_content/hashing/originalorder_regulargrid.eps}
%\caption{Original Node Distribution}
%\end{subfigure}
%\begin{subfigure}{0.45\textwidth}
%\centering
%\includegraphics[width=\textwidth]{rbffd_methods_content/hashing/overlay_regulargrid-eps-converted-to.pdf}
%\caption{Fixed-grid cells sorted by $Z$-order}
%\end{subfigure}
%\begin{subfigure}{0.45\textwidth}
%\centering
%\includegraphics[width=\textwidth]{rbffd_methods_content/hashing/stencil_regulargrid-eps-converted-to.pdf}
%\caption{Example Stencil $n=31$}
%\end{subfigure}
%\caption{In order: a) node ordering test cases; b) original ordering of regular grid (raster); c) coarse grid overlay for hash functions ($hnx = 6$); d) example stencil ($n=31$) spanning multiple Z's; e) spy of DM after orderings. }
%\end{figure}

%\begin{figure}
%\centering
%\includegraphics[width=0.45\textwidth]{rbffd_methods_content/hashing/stencil_regulargrid-eps-converted-to.pdf}
%\caption{A simple example illustrates the use of a fixed-grid with $h_n = 6$ for $N=18^3$ regular nodes.  d) example stencil ($n=31$) spanning multiple Z's; e) spy of DM after orderings. }
%\label{fig:orderings}
%\end{figure}



\ifstandalone
\bibliographystyle{plain}
\bibliography{merged_references}
\end{document}
\else
\expandafter\endinput
\fi
