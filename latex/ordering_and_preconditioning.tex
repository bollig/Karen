\makeatletter
\@ifundefined{standalonetrue}{\newif\ifstandalone}{}
\@ifundefined{section}{\standalonetrue}{\standalonefalse}
\makeatother
\ifstandalone
\documentclass{report}

\input{all_usepackages} 

\begin{document}
\fi




\chapter{An Alternative Stencil Generation Algorithm for RBF-FD}
\label{chap:stencils}

Like all RBF methods, RBF-FD is designed to handle irregular node distributions, so the emphasis in literature focuses on how the method manages point clouds. While nothing prevents implementations of RBF-FD from utilizing existing meshes/lattices, most work in the field concentrates on simple geometries like the sphere \cite{FlyerWright07,FlyerLehto10,FlyerLehto11,FlyerWright09} and quadrilateral/prism \cite{FiNDTHESE} to better understand properties of the method and develop extensions. 
%TODO: find refs


Without mesh/lattice connectivity available, stencils are generated by choosing the $n$-nearest neighbors to each node, including the query node. This is known more formally as a \emph{$k$-nearest neighbor (k-NN)} problem \cite{TagliasacchiMFE} (a.k.a. $\ell$-nearest neighbor search \cite{WendlandBook}). Here ``nearest" is defined with the Euclidean distance metric, although it is possible to generalize to other metrics (see e.g., \cite{MatlabKDTreeSearcher}). 

In contrast to RBF-FD, global RBF methods with infinite support connect all nodes to all other nodes, so there is no need to query for nearest neighbors. Methods based on compact RBFs require all nodes---with no limit on the count---that lie within the support/radius of the RBF centered at each node. The neighbor query for compact RBFs is referred to as a \emph{ball query} (a.k.a. range query \cite{WendlandBook}) due to the closed ball created by the radius of support in Equation~\ref{eqn:csrbf}. 

The $k$-NN and ball query problems are similar, but the former is harder to solve. Consider, for example, the scenario in Figure~\ref{fig:nearest_neighbor_example}. Two ball queries around a green stencil center are represented as dashed and dash-dot circles. The inner query returns four neighbors, and the outer returns six. If a stencil of size $n=6$ is desired, then the outer query can be truncated to give the five required neighbors shown in blue. In this example the red node and the farthest blue node are equidistant from the center, but ties are broken arbitrarily. Although $k$-NN is simply a truncated ball query, the real challenge lies in finding the proper search radius to enclose at least the $k$ desired neighbors.  

\begin{figure}
\centering
\includegraphics[width=5.5cm]{rbffd_methods_content/neighbors/ball_query_vs_kNN.png}
\caption{A stencil center in green finds neighboring stencil nodes in blue. Two ball queries are shown as dashed and dash-dot circles to demonstrate the added difficulty of finding the right query radius to obtain the $k$-nearest neighbors.}
\label{fig:nearest_neighbor_example}
\end{figure}


A na\"{i}ve approach to neighbor queries would be a brute-force search that checks distances from all nodes to every other node. Obviously the cost of such a method is high: $O(N^2)$. Multi-dimensional data structures can limit the scope of searching and reduce the cost of queries as low as $O(N)$. %TODO: correct lower bound? 


For the most part, investigations in RBF communities that delve into efficient neighbor queries are limited methods which depend on ball queries, For example, the Partition of Unity method for approximation (e.g., \cite{Wendland2002,WendlandBook}) and particle methods like the Fast Multipole Method (e.g., \cite{Ying2006, Gumerov2003}) or Smoothed Particle Hydrodynamics (e.g., \cite{Krog2010}). Examples of fast algorithms employed in these fields include the fixed-grid method \cite{WendlandBook,Krog2010}, $k$-D Trees \cite{WendlandBook}, Range Trees \cite{Wendland2002,WendlandBook}, and $2^d$-Trees (i.e., Quad- and Octrees) \cite{Gumerov2003, Ying2006}. Surprisingly, while other communities continue the quest for fast neighbor queries, RBF collocation and RBF-FD communities have been slow on the uptake. For many years, the standard in the community has been to use $k$-D Trees (see e.g., \cite{Fasshauer2007, FlyerLehto11,FornbergLehto11}). 


This chapter considers the use of an alternative algorithm to generate stencils for RBF-FD. It is based loosely on the fixed-grid method from \cite{Krog2010}. \cite{Samet2005} would classify the algorithm as a \emph{fixed grid ``bucket" method with one-dimensional spatial ordering}. However, due to the use of spatial hashing to order nodes in memory it is referred to here as the \emph{Hash Algorithm}. 

The Hash Algorithm loosens the requirements for $k$-nearest neighbors ($k$-NN) stencils to allow $k$-``approximately nearest" neighbors ($k$-ANN). It also reorders nodes according to space-filling curves. In what follows, the Hash Algorithm is compared to an efficient implementation of $k$-D Tree (\cite{TagliasacchiMFE}). Benchmarks demonstrate that with the proper choice of parameters the Hash Algorithm is up to 2x faster than $k$-D Tree, and it comes with a free bonus: up to 5x faster SpMV on quasi-random nodes due to the spatial reordering that occurs during stencil generation. 


\section{$k$-D Tree}

A $k$-D Tree is a spatial data structure that generally decomposes a space/volume into a small number of cells. All $k$-D Trees are binary and iteratively subdivide volumes and sub-volumes into two parts. The ``$k$" in $k$-D Tree refers to the dimensionality of the data/volume  partitioned. 

A distinguishing feature for $k$-D Trees is the use of \emph{half-planes} for partitioning. Given a set of points bounded by a $k$-dimensional volume, a half-plane is defined as a $(k-1)$-dimensional line/plane/hyperplane that equally splits the data (or volume) in half. A $k$-D Tree iteratively applies half-plane splits until a number of small cells contain only one value. A binary tree is constructed by storing splitting information (i.e., the splitting dimension and splitting value) at each node of the tree. Branches then represent the two halves of the sub-volume, and leaves of the tree contain the data within cells. 

$k$-D Trees do not require that splitting planes partition the bounding volume evenly. Often, the half-planes split data into equal sized groups and result in unevenly partitioned volumes. The choice of dimension for the splitting plane, in conjunction with a variety of methods for choosing the splitting values allows for many flavors of $k$-D Trees (see e.g., \cite{Samet2005, Skiena2008} for comprehensive lists). \emph{Point $k$-D Trees}, \emph{$2^d$ Trees} (i.e., quad-/octrees), \emph{BSP-Trees}, and \emph{R-trees} are all members of the general $k$-D Tree class \cite{Skiena2008,Ying2006}. 


This work considers \emph{Point $k$-D Trees}, which partition a set of nodes into equal sized groups and results in uneven cuts in the bounding volume. Point $k$-D Trees are distinguished by the assumption that all splitting values occur at nodes rather than allowing arbitrary cuts in the volume. This assumption simplifies the structure of the corresponding binary tree such that each level of the tree corresponds to the node used as the splitting value.


The $k$-D Tree in Figure~\ref{fig:kdtree_example} is an example of a Point $k$-D Tree. Given a set of eight nodes in two dimensions, the tree is constructed by applying one-dimensional cuts along the $x$-dimension, then the $y$-dimension, then back to the $x$-dimension, and a final cut in $y$. This approach is referred to as \emph{cyclic splitting} as consecutive cuts are applied by cycling dimensions in a round-robin fashion \cite{Samet2005}. The first cut, shown in green, splits the nodes into two sets at node $A$. In the corresponding tree to the right, node $A$ is set as the root with all nodes having $x$-coordinates less than $A$ to the left, and all nodes having $x$-coordinates greater than or equal to $A$ to the right. The second level of the tree, in blue, is constructed by splitting the half-planes on either side of $A$ at nodes $B$ and $C$. The splits for each half-plane are independent and occur in the $y$-dimension. Similarly, the third and fourth levels (red and black) split the space based on $x$ and $y$ dimensions respectively.    

The splitting value for each level of Figure~\ref{fig:kdtree_example} is chosen to be the coordinate of the median node in each half-plane. 

An alternative applied to RBFs in \cite{WendlandBook} chooses the splitting dimension based on the maximum spread in data for each sub-volume. Note that the maximum spread allows for consecutive cuts in a single dimension. 
 

$k$-D Tree most frequently partitions each dimension at the median value in the data. For Point $k$-D Tree this corresponds to sorting the points by coordinate value and selecting the median. 



This assumption allows simplification in the binary tree to have each node of the tree 




Due to the assumption that all splits correspond to a node coordinate, the corresponding tree can be constructed as 


Frequently, the terms \emph{$k$-D Tree} and \emph{Point $k$-D Tree} are used synonymously; a convention assumed here. 


In this example, splitting planes occur at median nodes in each subset. 





 For this reason, the term $k$-D Tree has come to refer to a general class of data structures. 
$k$-D Tree  unique construction. 


A half-plane is 

, a $k$-D Tree  the domain consists of $k$-dimensional points, then the $k$-D Tree partitions the nodes into a binary tree with branches of the tree corresponding to cuts along a single dimension of the space. The basic assumption for $k$-D Tree is that nodes are 

There is no unique way to construct a $k$-D Tree---in fact, \cite{Samet2005} presents more than a dozen different cases including \emph{Point $k$-D Trees}, \emph{PR $k$-D Trees}, \emph{BD Trees}, etc. This work considers only the \emph{Point $k$-D Tree}. 

In a Point $k$-D Tree, hereafter referred to as $k$-D Tree, 

The splitting dimension, the splitting value and the 


The $k$-D tree implementation compared in this work is the \emph{$kd$-Tree Matlab} library from Tagliasacchi \cite{TagliasacchiMFE,TagliasacchiGC}. Originally posted to the Matlab FileExchange and now maintained as a Google Code project. Until the addition of $KNNsearcher$ in Matlab 

A $k$-D Tree partitions data in $k$-dimensions. 
Given a set of nodes in $d$-dimensional space, a $k$-D Tree partitions the data 
kD Trees partition 

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{rbffd_methods_content/neighbors/kdTree_Start.png}
\caption{An example $k$-D Tree in 2-Dimensions. Nodes are partitioned with a cyclic dimension splitting rule (i.e., splits occur first in $X$, then $Y$, then $X$, etc.); all splits occur at the median node in each dimension.}
\label{fig:kdtree_example}
\end{figure}

A modification to the tree definition restricts the dimensionality to 3. 


Tagliasacchi \cite{TagliasacchiGC} uses the standard split by median in each dimension, with alternating chosen by incrementing the dimension by one, modulo the number of dimensions (e.g., in 2D the order is $X, Y, X, Y, ...$). Wendland \cite{WendlandBook} suggests a number of more advanced median cuts such as 

\begin{itemize} 
\item cutting by the median with dimension determined by the maximum spread in each dimension, which allows dimensions to be repeated consecutively. 
\item 
\end{itemize}


$k$-D Trees can be constructed in $O(N \log N)$ time with each query resulting in a $O(\log N)$ cost \cite{WendlandBook}.


 when Wendland mentioned $k$-D tree as an option for ball queries in compact RBF partition of unity approximations---ironically the author opted to test two other data structures instead. Wendland later returned in \cite{WendlandBook} to fully describe $k$-D Tree for . 


In addition to $k$-D Trees, the author of \cite{WendlandBook} recommends two other data-structures that would be appropriate for RBF methods: a fixed-grid method, and $bd$-trees. While justification for both alternatives is sound, to our knowledge neither have been applied to RBF-FD or other RBF PDE methods. Until now that is. This chapter introduces a novel neighbor query for the $k$-NN problem in RBF-FD based on the fixed-grid method. 

\cite{TagliasacchiMFE}. 
\cite{MatlabKDTreeSearcher}




\begin{itemize} 
\item Tree build algorithm
\item Query algorithm
\item Total complexity due to the fact that we only need to query stencils once, so the build and query times are joint. 
\end{itemize}
\cite{Fasshauer2007}

$k$-D Trees can be extended to use the concept of buckets \cite{Friedman1977}, although these have not been applied in the RBF community. 



\section{A Fixed-Grid Hashing Algorithm}

The implementation runs entirely on the CPU (\cite{Krog2010} runs on the GPU). 


 can be described as a \emph{fixed grid ``bucket" method with one-dimensional spatial ordering} as classified by \cite{Samet2005}. 

Our method can handle both ball queries and $k$-NN. For the former simply include all cells that lie within the radius. It will continue to be approximate due to rasterization effects, but the radius could be enforced by sorting nodes by distance and truncating by the actual radius. The $k$-NN is a more challenging problem, because it requires additional effort in the query step to expand the search range until the desired $n$ neighbors have been found. 

Our benchmarks and testing focus on the more challenging problem of $k$-NN


The algorithm introduced here is the same as the Fixed-Grid Method in \cite{WendlandBook}.     


\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{../figures/chapter2/hashing_example/LSH_Concept.png}
\caption{High level overview of the \emph{Fixed-Grid Bucket} (a.k.a. Hash) Algorithm. A coarse regular grid is overlaid on the domain. Nodes coordinates are hashed to containing cell indices and pushed onto a list for the appropriate cell. The cells are reordered in memory according to a space filling curve (i.e., Raster (IJK), Morton (Z), Graycode (U), etc.). Stencil queries start search with the cell containing the stencil center and expand to neighboring cells until at least $n$ candidate nodes are found. The candidate list is truncated to the $n$ closest neighbors. }
\end{figure} 






Hashing, shown in Figure~\ref{fig:hashing_example} overlays a regular grid. This is equivalent to an axis aligned bounding box AABB, with refinement. In other words, we form a quad-tree in 2D, an octree in 3D. The neighbor query starts with the cell in which $x_c$ resides. Since we use an axis aligned bounding box, this cell index is easily calculated given the coordinate and number of subdivisions in each dimension. Once the cell index is resolved, the stencil is populated by taking the $n$ nearest neighbors from within the current cell. If the cell does not contain sufficient number of nodes to fill the stencil, the search for neighbors expands to include the cells immediately adjoining the center cell, taking only the nearest nodes in the provided cells. The search continues to expand outward in a rasterized circle/sphere until $n$ is satisifed. This search is considered approximate because it can happen that a true nearest neighbor would lie in a cell that is is not included in the rasterized circle, and other nodes are substituted from the far reaches of the discretized grid.

%TODO: cite lagrangian sph knn
The complexity of the method is still higher than the more efficient implementations used by Lagrangian methods, but as demonstrated in Figure~\ref{sten_methods_compare} the savings are significant. Generating stencils for RBF-FD is a preprocessing cost, so we do not dedicate an excessive amount of attention to this algorithm. However, a few ideas that would improve: hilbert ordering, choose AABB resolution based on $N$ not user parameters, faster sorting, GPU implementation




\subsection{$k$-NN vs $k$-ANN} 

Start at the center cell and add the 8 cells immediately surrounding it. Add to this the 16 cells in the second halo for a total of 25 cells. 

So while a $k$-NN stencil of size $n=8$ in Figure~\ref{fig:nearest_neighbor_example} would contain the black node in the right-most column of the grid. The $k$-ANN generated a stencil of size $n=8$ would opt for the black node to the top right because it is in the second halo and accepted as ``closer" because the stencil can be satisfied without querying the extra 24 cells on the outermost halo. 


One of the easiest ways to reduce complexity of 


Therefore, it is not essential that stencils contain only nearest neighbors. Instead, one can acquire the \emph{approximate nearest neighbors}. Figure~\ref{fig:ann_example} demonstrates a case where a does not contain all nearest neighbors. As illustrated in the Figure, the ANN stencil and true nearest neighbor stencil differ by one node. THis is not dire


The query is approximate because nodes such as the white node in Figure~\ref{fig:nearest_neighbor_example} could be included in the stencil before 


RBF-FD operates on general node distributions. Historically, stencils are uniform in size ($n$) and generated by selecting the $(n-1)$ true nearest neighbors to a node $x_c$. This is a $k$-NN query. 

Alternative queries are possible: ball query and approximate nearest neighbor. The approximate is of particular interest because nodes closest to the stencil will always be selected, whereas the nodes further away have minimal influence so swapping out cant hurt. The justification in altering the selection is for reduced complexity in neighbor queries.  


\begin{figure}
\centering
\includegraphics[width=8.5cm]{rbffd_methods_content/neighbors/neighbor_incorrect.png}
\caption{A stencil generated with $k$-ANN satisfies the required stencil size, but is not guaranteed to choose the true nearest neighbors.}
\label{fig:approximate_nearest_neighbors}
\end{figure}




\subsection{Integer Dilation and Node Reordering}

\cite{MellorCrummey2001} found that reodering nodes via RCM and space filling curves offer similar benefits in terms of reduced TLB misses and better cache coherency. 

\begin{figure}
\centering
\begin{subfigure}{0.425\textwidth}
\includegraphics[width=1.0\textwidth]{../figures/chapter2/hashing_example/bruteforce_N6400_n50-eps-converted-to.png}
\caption{k-D Tree} 
\end{subfigure} 
\begin{subfigure}{0.425\textwidth}
\includegraphics[width=1.0\textwidth]{../figures/chapter2/hashing_example/lsh_N6400_n50-eps-converted-to.png}
\caption{LSH}
\end{subfigure}
\caption{Example effects of node reordering within neighbor query algorithm for MD node set $N=6400$ with $n=50$. Matrix is $0.78\%$ full. k-D Tree maintains original ordering of the nodes and deceptively appears nearly dense. LSH algorithm reorders nodes according to raster ordering and reveals sparsity of the problem.  }
\end{figure} 

\cite{Samet2005} labels this type of algorithm as a \emph{fixed grid bucket algorithm} with the twist that it also has \emph{one dimensional ordering}
\begin{itemize} 
\item Algorithm to hash each node, push onto queue for each hash, sort by hash
\item Query by jumping to hash, building raster spheres
\item Need a good handle on the complexity. 
\item possibility to use space filling curves
\end{itemize}





\section{Performance Comparison}


At the start of this dissertation, the most widely used $k$-D Tree implementation in the RBF community was \cite{TagliasacchiMFE}. As a MEX-compiled 



\section{Conclusion}
\begin{itemize}
\item Performance improvement
\item SpMV impact
\item impact from space filling curves
\end{itemize}





Many algorithms exist to query the $k$-nearest neighbors (equivalently all nodes in the minimum/smallest enclosing circle). Some algorithms overlay a grid similar to Locality Sensitive Hashing and query such as... \cite{HarPeledMazumdar2003}.

Throughout the RBF community, one commonly finds $k$D-Trees in use for nearest neighbor queries. The data structure is recommended for its $O(N)$ complexity in querying nearby neighbors. 



For the purpose of generating RBF-FD stencils the $k$D-Tree is built and all queries are made once. There is no reuse of the tree so the build and query times are combined into one. 

TODO:
\begin{itemize}
\item Finish KDTree description
\item Hashing neighbor query algorithm description
\item Cite refs on multi-dim spatial data structures
\item Complexity of algorithms
\item impact from ordering on matrix sparsity. Bandwidth impact. Bandwidth impact on condition considered in future chapter. 
\item what is best overlay resolution? based on time to generate. choose resolution as n/2, n/9? etc? 
\item perform neighbor queries on raster order because its easiest to jump cells. then each cell can be reordered according to z,x,u, etc. for better memory locality. Matlab script to do this (can be ported to C)
\end{itemize}

With a grid present, stencils are generated by querying $n$ neighbors for each $N$ total nodes in the domain. While not required, one typically assumes a stencil of $n$ nodes must be the nearest neighbors to a stencil center. Afterall, as the distance between stencil nodes and the stencil center decreases, the accuracy of derivatives increases. 

Brute force searching for neighbors---computing the distance between every pair of nodes and then selecting the $n$ nearest---is discouraged due to its $O(N^2)$ complexity. Common practice in the RBF community is to construct $k$D-Trees to decrease the cost of queries (e.g., \cite{Fasshauer2007, FlyerLehto11, FornbergLehto11}). 

\authnote{Incomplete here to end of section}
\url{http://www.vincentgarcia.org/data/Garcia_2010_ICIP.pdf}

Leveraging $k$D-Trees involves two costs: a) the initial tree construction, and b) $k$-nearest neighbor queries. 

GPU version of Locality Sensitive Hashing could reduce complexity further \cite{Pan2011}

This can be done efficiently using neighbor query algorithms or spatial partitioning data-structures such as Locality Sensitive Hashing (LSH) and $k$D-Tree. Different query algorithms often have a profound impact on the DM structure and memory access patterns. We choose a Raster ($ijk$) ordering LSH algorithm \cite{Bollig2011} leading to the matrix structure in Figures~\ref{fig:oneThreadPerStencil} and \ref{fig:oneWarpPerStencil}. While querying neighbors for each stencil is an embarrassingly parallel operation, the node sets used here are stationary and require stencil generation only once. Efficiency and parallelism for this task has little impact on the overall run-time of tests, which is dominated by the time-stepping. We preprocess node sets and generate stencils serially, then load stencils and nodes from disk at run-time. In contrast to the RBF-FD view of a static grid, Lagrangian/particle based PDE algorithms promote efficient parallel variants of LSH in order to accelerate querying neighbors at each time-step \cite{Pan2011, Goswami2010}. 


At the onset of our work on RBF-FD, the most commonly used KDTree implmentation used by the RBF community was \cite{Tagliasacchi2008}. Recently, improvements were made to the KDTree algorithm to reduce the cost of building the KDTree to $O(N log^2 N)$. 

Figure~\ref{fig:stencil_query_old_and_new} compares the total time to generate $N$ stencils of size $n=50$ with three methods: \cite{Tagliasacchi2008}, our hash-based neighbor query, and the improved KDTree from \cite{Tagliasacchi2012}. 
Until the new release of KDTree, our algorithm was a major improvement to the performance of stencil generation. The hash-based approach achieved greater than 


%RBF methods are traditionally described as general and meshless in that they apply to unstructured clouds of points in arbitrary dimensions. However, although the term meshless implies a method capable of operating with no node connectivity, all numerical methods---meshless RBF methods included---connect nodes in the domain. For example, the ``meshless'' global RBF method connects every node in the domain to all other nodes. Compact support or local RBF methods like RBF-FD limit connections to nodes that lie within a predetermined radius.

%The connections between nodes form a directed adjacency graph with edges that dictate the paths along which data/phenomena can travel. For example, a plus shaped stencil of five points with a center node and four neighboring nodes allows values to propagate north, south, east and west; not northeast, southeast, etc.


%They are robust and function on scattered point clouds. RBF-FD in particular requires stencils to be generated from $n$ nearest neighbors to a stencil center. The cost of these neighbor queries can vary greatly depending on the choice of algorithm or data-structure used to make the query. 

For example, in general brute force is inefficient 
The author of \cite{Fasshauer2007} queries $n$ nearest neighbors for a compact-support RBF partition of unity example with a $k$-D tree. In \cite{FlyerLehto11,FornbergLehto11} a $k$-D Tree is leveraged for all neighbor queries for RBF-FD. 

Our work in \cite{BolligFlyerErlebacher2012} leveraged an alternative to $k$-D tree, based loosely on space-filling curve orderings common in Lagrangian schemes like Smoothed Particle Hydrodynamics (e.g., \cite{IanThesis}, \cite{Kelager}). %TODO: I have used Locality Sensitive Hashing \cite{Indyk} in the past. According to Samet \cite{Samet "Multidimensional and Metric..."}, LSH uses a set of random hashes to assign indices for each node. Our case uses space-filling curve, so we use . 

Rather than iterate through all $N$ nodes to find the true neighbors, or step through a k-D tree in something like $O(log N)$ that requires extra built-out, ANN allows us to use a set of nodes that satisfy


\subsubsection{KDTree}
Most of the RBF community leverages the $k$-D tree, due to its low computational complexity for querying neighbors and its wide availability as standalone software in the public domain (e.g., matlab central has a few implementations for download, and the MATLAB Statistics Toolbox includes an efficient k-D Tree). 

The complexity of assembling he tree is

The Matlab central $k$-D Tree is MEX compiled and efficient. We integrated the standalone C++ code into our library.  

While the $k$-D Tree functions well for queries, its downfall is a large cost in preprocessing to build the tree. For moving nodes, such as in Lagrangian schemes, this cost is prohibitively high. In an attempt to reduce the cost, lagrangian schemes introduced approximate nearest neighbor queries based on 


\subsubsection{Hashing}

\section{Neighbor Queries} 

As part of the preprocessing stage for RBF-FD, the scattered point cloud must
be analyzed to generate stencils. To generate a stencil, any collection of
nodes can be selected. However, by choosing nodes close to the stencil center
and well balanced around it, we stand to get the best possible approximations
to derivatives. 

Why? well, the approximations are based on differences. Similar to classic FD,
draw a secant connecting two nodes of a stencil. The slope of the secant
determines the gradient at either point or a point in between. In the limit as
the points are moved closer to the same spot, the approximation to the
derivative at that point becomes exact. 

So ideally every RBF-FD stencil will operate on nearest neighbors. One way to
generate nearest neighbors is brute force $O(N^{2})$; very inefficient. 

Alternatives exist including k-D Tree

\subsection{k-D Tree}
Build complexity, seek times. Internal ordering 

We use implementation provided by Andrea Tagliasacchi \cite{tagliasacchi} as a MEX compiled set of routines for Matlab. These same routines are then linked into C/C++. 

\subsection{Locality Sensitive Hashing}

\cite{Connor2009} provide a fast parallel 

\cite{Henke2012} is working on parallel generation. \cite{IanJohnsonThesis} has OpenCL neighbor queries

We started with a CPU implementation to test appropriateness. 




Approximate nearest neighbors will be nearly balanced. 
We observe that RBF-FD functions as well on stencils of true nearest neighbors as it does on approximate nearest neighbors.

To demonstrate the savings in choice of stencil generation method, we provide Figure~\ref{fig:sten_methods_compare}. 
 

The impact of our neighbor query also extends influence on the structure of the RBF-FD DMs.
has is to To quantify the sparsity of a Differentiation Matrix we consider the ratio of non-zeros ($N*n$) to total elements in the matrix ($N^2$). For example, a problem of size $N = 10,000$ with stencil size $n=31$ has a ratio of $0.0031$ and is $99.69\%$ empty. 

Querying neighbors requires searching at least the immediate cell one layer of neighbors. by including one extra layer we ensure that small stencils near the border of the immediate cell can pick up neighbors in adjacent cells.


%TODO: sparsity
%TODO: Node ordering
%TODO: show blocks for boundary conditions (annulus)

A prototype implementation of this method allowed for a variety of space filling curves to reorder the cells. The curves are created through integer dilation \cite{IntegerD} 


\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{rbffd_methods_content/hashing/originalorder_regulargrid-eps-converted-to.pdf}
\caption{In order: a) node ordering test cases; b) original ordering of regular grid (raster); c) coarse grid overlay for hash functions ($hnx = 6$); d) example stencil ($n=31$) spanning multiple Z's; e) spy of DM after orderings. \authnote{REGENERATE FIGURES WITH RANDOM/HALTON NODES}}
\label{fig:orderings}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{rbffd_methods_content/hashing/overlay_regulargrid-eps-converted-to.pdf} \includegraphics[width=0.45\textwidth]{rbffd_methods_content/hashing/stencil_regulargrid-eps-converted-to.pdf}
\caption{In order: a) node ordering test cases; b) original ordering of regular grid (raster); c) coarse grid overlay for hash functions ($hnx = 6$); d) example stencil ($n=31$) spanning multiple Z's; e) spy of DM after orderings. }
\label{fig:orderings}
\end{figure}

The KDTree implementation used in this work is from 

Original data showed our algorithm as wildly successful against a version  

The Cuthill McKee algorithms can be equated to a breadth-first search. The algorithm queues nodes in order of degree at each level of the search and traverses the lowest degree priority. The Reverse variant of Cuthill-McKee inverts the node order so that the lowest degree and top level node are at the end of the matrix rather than the beginning. Aside from ordering, the Reverse and Standard Cuthill McKee algorithms are identical processes. RCM is the more popular of the variants though, due to storage savings and reduced fill-in for some decompositions \cite{LiuSherman1976}.	. 
%TODO: get the compareKDTreeLSH.m output; this was for my C++ code and compared queries on 
%\begin{figure}
%\centering
%\includegraphics[width=0.65\textwidth]{rbffd_methods_content/hashing/
%\end{figure}



%TODO: \cite{Indyk "Algorithms for Nearest Neighbor Search"} for reference on quad vs kd vs lsh
%TODO: \cite{ChenChang "Neighbor-Finding Based on Space-Filling Curves"} 
%TODO: \cite{Stocco and Schrack "On Spatial Orders and Location Codes"}

Obviously, the ideal case for bandwidth is when all rows contain the $\frac{n}{2}$ nodes corresponding to solution value to either side of $u_j$. In 1-D this corresponds to every node containing the $\frac{n}{2}$ nodes to the left and right of $x_j$. In 2-D this is only possible if the nodes in the domain are properly indexed such that stencils contain the proper set of neighbors---a stringent requirement that will 


\begin{figure}
\centering
\includegraphics[width=9.5cm]{../figures/stencils/kdtree_old_reg_subsets_4m_stencil_gen_time.png}
%\includegraphics[width=9.5cm]{../figures/stencils/reg_subsets_4m_stencil_gen_time.png}
\includegraphics[width=9.5cm]{../figures/stencils/sphere_subsets_1m_stencil_gen_time.png}
\caption{Querying the $n=50$ nearest neighbors on a regular grid up to $N=160^3$ demonstrates the significant gains achieved by our spatially binned neighbor query. While KDTree queries grow as $O(N log N)$}
\label{fig:hash_results}
\end{figure}


The early implementation of $k$-D tree had an $O(n^2)$ growth in complexity. This algorithm was developed to alleviate that cost. It took a few hours to implement but has had some surprising impacts. Note that the complexity of $k$-D tree was reduced to $O(N \log^2 N)$ in 2012. On a regular grid (generated with raster/IJK ordering), the cost of $k$-D tree grows at the same rate as the hashing method. 

At $N=32000$ the cost of hashing drops below $k$-D Tree due to the decreasing number of empty hash cells. Likewise, at $N=1000000$ and beyond, the gap between hashing and $k$-D Tree begins to close as cells contain more than one 


Q: why does the curve drop for $hnx=100$? 
Q: the complexity of the algorithm? 
Q: the sphere I understand: its localizing the search to small patches on the sphere, and 





\begin{figure}
\centering
\includegraphics[width=9.5cm]{../figures/stencils/kdtree_old_reg_subsets_4m_stencil_gen_speedup.png}
\includegraphics[width=9.5cm]{../figures/stencils/reg_subsets_4m_stencil_gen_speedup.png}
\caption{Querying the $n=50$ nearest neighbors on a regular grid up to $N=160^3$ demonstrates the significant gains achieved by our spatially binned neighbor query. While KDTree queries grow as $O(N log N)$}
\label{fig:hash_results}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=9.5cm]{../figures/stencils/reg_subsets_4m_spmv_speedup.png}
\caption{Querying the $n=50$ nearest neighbors on a regular grid up to $N=160^3$ demonstrates the significant gains achieved by our spatially binned neighbor query. While KDTree queries grow as $O(N log N)$}
\label{fig:hash_results}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=7.5cm]{../figures/stencils/sphere_subsets_1m_stencil_gen_speedup.png}
\includegraphics[width=7.5cm]{../figures/stencils/sphere_subsets_1m_spmv_speedup.png}
\caption{Generating stencils for increasing subsets of the $N=1e6$ CVT nodes mesh.}
\label{fig:hash_results}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=10.5cm]{../figures/stencils/sphere_stencil_gen_speedup.png}
\includegraphics[width=10.5cm]{../figures/stencils/sphere_spmv_speedup.png} 
\caption{Based on the proper choice of overlay resolution, the hash stencil query can accelerate stencil generation, but the sophistication of the algorithm is low enough that negative impact is more likely. On the other hand, the impact on SpMV performance is always positive with the routine accelerated up to 4.9x faster.}
\label{fig:hash_results}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=7.5cm]{../figures/stencils/cvt1m_stencil_gen_speedup.png}
\includegraphics[width=7.5cm]{../figures/stencils/cvt1m_spmv_speedup.png} 
\caption{As the coarse grid resolution increases the hashing algorithm achieves both 2x faster than KDTree in stencil generation, with greater than 4x gain in SpMV performance (for free).}
\label{fig:hash_results}
\end{figure}

For every $N$ there is an optimal $hnx$. This is depicted for $N=500000$ CVT and $hnx=100$. 

\section{On Space Filling Curves and Other Orderings} 


\subsection{Integer Dilation}
One frequently hears that ordering via space filling curves like Morton Ordering and/or gray codes can benefit memory access patterns. 

(Related? \url{http://publish.uwo.ca/~shaque4/presentationSONAD.pdf} \url{http://www.cs.duke.edu/~alvy/papers/sc98/} \url{http://www.cs.indiana.edu/~dswise/Arcee/Papers/medea06.pdf} \url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.7720&rep=rep1&type=pdf} \url{http://stackoverflow.com/questions/4260002/benefits-of-nearest-neighbor-search-with-morton-order})

\cite{Saad2003} mentions the impact of ordering on conditioning.

Algorithms like Reverse Cuthill McKee and Approximate Minimum Degree ordering allow general restructuring of matrices. 

\authnote{NEed to compare conditioning of LSH and other algorithms in Matlab}

Q: what is an ideal ordering?
Q: what is the best conditioning from ordering?
Q: what is the relative cost of ordering?



\begin{figure}
\centering
\includegraphics[width=0.65\textwidth]{rbffd_methods_content/hashing/node_orderings-eps-converted-to.pdf} \\
\includegraphics[width=0.65\textwidth]{rbffd_methods_content/hashing/spy_regulargrid-eps-converted-to.pdf} 
\caption{In order: a) node ordering test cases; b) original ordering of regular grid (raster); c) coarse grid overlay for hash functions ($hnx = 6$); d) example stencil ($n=31$) spanning multiple Z's; e) spy of DM after orderings. }
\label{fig:orderings}
\end{figure}


\section{Conclusions on Stencil Generation}



\ifstandalone
\bibliographystyle{plain}
\bibliography{merged_references}
\end{document}
\else
\expandafter\endinput
\fi