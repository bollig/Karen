
\makeatletter
\@ifundefined{standalonetrue}{\newif\ifstandalone}{}
\@ifundefined{section}{\standalonetrue}{\standalonefalse}
\makeatother
\ifstandalone
\documentclass{report}

\input{all_usepackages} 
\usepackage[margin=1.25in]{geometry}

\begin{document}
\fi


\chapter{Distributed RBF-FD (incomplete)}
\label{chap:distributed_rbffd}


%TODO: Similar Overlapping algorithm: \url{http://arxiv.org/pdf/1101.0091v1.pdf}
 %Imbalanced computation can be the source of excessive delays as processors sit idle and wait for other processors to catch up. 

Parallelizing applications in a distributed computing environment requires three
design decisions \cite{Saad2003}: 
\begin{enumerate} 
\item Partition the problem to distribute work across multiple processes. Intelligent partitioning
impacts load balancing and communication latency.
\item Determine the subset of node information,
solution values, etc. that are visible to each process, and establish index mappings that translate between a local context and the global problem. 
\item Establish a local ordering of data. The right local order can improve solver performance and/or simplify data movement. %Of particular interest here is how to minimize data movement at each iteration. In Chapter~\ref{chap:multigpu_rbffd} the local ordering will help minimize data transfer between CPU and GPU. 
\end{enumerate}

This chapter details the first implementation of RBF-FD designed for distributed computing environments and a few optimizations that help scale computation across a thousand processes. The discussion in this chapter applies to both multi-CPU and multi-GPU implementations of RBF-FD. Since each GPU is used as an accelerator, details of partitioning, local and global index mappings, and node ordering are managed on the CPU.  

We operate under the assumption that inter-process communication is managed by the \emph{Message Passing Interface (MPI)} \cite{MPI}. Detail on how communication collectives are used is included below in Section~\ref{sec:mpi_collectives}. We use the term \emph{MPI process} to refer to a process that occupies one CPU core and is associated with at most one GPU accelerator. MPI processes on multiple cores of the same CPU behave the same as processes on independent compute nodes in a cluster; the only difference being the fabric used for communication (i.e., fast shared memory versus a slower QDR Infiniband). MPI always uses the fastest interconnect possible to connect processes. 


\section{Partitioning}

Parallelization of RBF-FD relies on \emph{domain decomposition}. Domain decomposition methods simply subdivide/partition the computational domain of a PDE into \emph{subdomains}, which are then solved independently. Subdomains can be solved sequentially by the same MPI process or in parallel across multiple processes. A number of methods exist for domain decomposition including Multiplicative Schwarz, Additive Schwarz, and Restricted Additive Schwarz (see related work in \cite{Yokota2010,StCyr2007}). The choice of method determines how the domain is subdivided, and how often data is passed between subdomains (i.e., how parallelizable the method is). Domain decomposition methods are well known as a form of preconditioning (see e.g., \cite{Beatson2000, StCyr2007}), but the two Additive variants of Schwarz decomposition are inherently parallel and naturally extend to distributed architectures \cite{Yokota2010, Gropp1990}. 

The parallelization strategy employed for RBF-FD in this work is equivalent to a Restricted Additive Schwarz (RAS), although our strategy was implemented prior to formal knowledge of the RAS method. In this case we assume that partitioning occurs within the spatial domain. At each time-step the PDE is first solved independently within subdomains, and then information is shared across subdomain boundaries to ensure a consistent global solution entering the next time-step. We also assume that subdomains are always assigned to independent processes, even though each process could operate on multiple subdomains in theory. 

Following the notation in \cite{StCyr2007}, consider domain, $\Interior$, decomposed into overlapping subdomains $\Interior_1$ and $\Interior_2$, with a global boundary $\partial\Interior = \partial\Interior_1 \cup \partial\Interior_2$. At each time-step we seek the simultaneous solution to
\begin{center}
\begin{minipage}{0.4\linewidth}
\begin{align*}
\diffop{u_{1}^{n+1}} & = f \ \ \text{ in }\Interior_1,\\
\boundop{u_{1}^{n+1}} &= g \ \ \text{ on }\partial\Interior_1, \\
u_{1}^{n+1} & = u_2^{n} \ \ \text{ on }\Boundary_{12}, 
\end{align*}
\end{minipage}
\begin{minipage}{0.4\linewidth} 
\begin{align}
\diffop{u_{2}^{n+1}} & = f \ \ \text{ in }\Interior_2 \nonumber \\
\boundop{u_{2}^{n+1}} & = g \ \ \text{ on }\partial\Interior_2, \label{eq:rasm_example} \\
u_{2}^{n+1} & = u_1^{n} \ \ \text{ on }\Boundary_{21},  \nonumber
\end{align}
\end{minipage}
\end{center}
\ 

\noindent where $\Boundary_{ij}$ indicates overlap between subdomains and is read as the subset of subdomain $j$ required by subdomain $i$ (i.e., $\Boundary_{ij} = \partial\Interior_i \cap \Interior_j$). A Dirichlet boundary condition connects subdomains at the interfaces. Although not considered here, future investigations may find Robin-type conditions of interest due to their convergence accelerating properties in the context of Schwarz methods for elliptic PDEs (\cite{StCyr2007}). 

Equation~\ref{eq:rasm_example} illustrates how nicely a PDE can be split in half, and then solved by independent processes. To enforce the Dirichlet conditions on $\Boundary_{ij}$, MPI transfers values $u_j^{n}$ across interfaces at each time-step. Note that as the number of subdomains increases additional constraints for $\Boundary_{ij}$ must be imposed on each subdomain. The worst case a partitioning for $p$ subdomains requires $p-1$ conditions (i.e., $\Boundary_{ij}$, for $i,j=1,2,...,p$ and $i \neq j$). Ideally each $\Interior_i$ should only overlap with a small number of neighboring subdomains to minimize MPI communication. 


The continuous form of decomposition in Equation~\ref{eq:rasm_example} allows complete freedom in choosing how subdomains are actually partitioned. In this work we assume partitioning of the geometry on which the PDE is solved. Of particular interest here is how to partition the unit sphere, but the challenge of more general geometries is also kept in mind. 

For ease of development and parallel debugging, partitioning is initially
assumed to be linear along one spatial dimension (in this case the $x$-axis). Each partition/subdomain is then contiguous and overlaps with neighboring subdomains to the left and right. This approach is not uncommon when solving PDEs on rectangular domains (see e.g., \cite{Divo2007, Thibault2009}). 

% A similar decomposition occurs in \cite{Divo2007} where the authors apply a local RBF collocation scheme to model viscous flow and heat transfer on rectangular domains. Thibault and Senocak  \cite{Thibault2009} partition a rectangular domain along the $z$-dimension for a variety of 3-D fluid benchmarks. 

Figure~\ref{fig:decomposed_sphere} illustrates the partitioning of
$N=10,201$ nodes on the unit sphere for four processes. 
Each partition, illustrated as a unique color, contains many stencil centers.  
A center's coordinates easily identify the containing partition. In cases where stencils in one partition depend on stencil nodes in another, we say that the dependencies (i.e., the nodes in $\Boundary_{ij}$) are \emph{ghost nodes}. 
In many ways ghost nodes are treated the same as any other stencil node. The MPI process in charge of a partition is fully aware of the ghost node coordinate, current solution value(s), etc. However, ghost nodes are managed by another subdomain/process, so any changes must be explicitly synchronized in order to maintain consistency. 

In Figure~\ref{fig:decomposed_sphere}, alternating representations between node points and interpolated surfaces illustrates the
overlap regions where ghost nodes reside. Due to stencil dependencies, the representations of overlap regions are double-wide---i.e., they contain a set of ghost nodes for both the left and right partitions. 

The width of the overlap in Figure~\ref{fig:decomposed_sphere} is between five and six nodes. As the stencil size increases, the width of the
overlap grows as $\sqrt{n}$. Since stencils need not have symmetric dependencies (i.e., if stencil $s_1$ depends on $s_2$, $s_2$ need not depend on $s_1$), the number of ghost nodes for each partition can vary. 

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=2.5in]{../figures/paper1/figures/vortex_rollup/4procs_N10K_n31.pdf}
\caption{Partitioning of $N=10,201$ nodes to span four processors with stencil size $n=31$. }
\label{fig:decomposed_sphere}
\end{center}
\end{figure}

Linear partitioning is simple and easy to code. Each MPI process has a left and right neighbor, so communication is straightforward. There are two concerns with this approach: 
\begin{enumerate} 
\item Scaling the number of processors quickly decreases the width of each partition, and can result in stencils spanning more than one partition in each direction. This introduces the need for complex communication collectives. 
\item In the case of the sphere and other irregular geometries, the partitioning of near uniform node distributions results in subdomains that contain unequal number of stencils and processor loads become imbalanced. 
\end{enumerate}

Alternative sphere partitionings exist. In atmospheric and geophysical communities for example, one often finds the cubed-sphere \cite{Ivan2011, Katta2012}, which transcribes a subdivided cube onto the sphere, and assigns projected rectangular subdivisions to individual processors. Another option is the icosahedral geodesic grid \cite{Randall2002}, which evenly balances the computational load by distributing equal sized geodesic triangles across processors. A complete list of options for partitioning the sphere are outside the scope of this work, but many involve recursive subdivisions based on simple geometric elements (i.e., triangle, rectangle, hexagon, etc.). 


\subsection{Graph Partitioning with METIS}

One of the major concerns with partitioning for a distributed environment is load balancing. Consider, for example, the situation where $p-1$ processors have equal sized partitions, each with $W_1$ amount of work, and a $p$-th processors is allocated some larger amount of work, $W_2$. In that case, the maximum possible speedup on $p$ processors obeys \cite{Gropp1990}:  
\begin{align}
S_p = \frac{(p-1) W_1 + W_2}{W_2} = 1 + (p-1)\frac{W_1}{W_2}.
\label{eq:load_balance}
\end{align}
The take-away from Equation~\ref{eq:load_balance} is that potential gains in parallelism are limited by the disproportionality of workloads. Balancing the loads equally allows the ideal factor $p$ gain from parallelization. 

To ensure load balancing, one commonly turns to \emph{graph partitioning} algorithms. Formally, graph partitioning algorithms attempt to solve the $k$-way partitioning problem \cite{Karypis1999}: given a graph $G = (V,E)$ with vertices ($V$) and any number of edges ($E$), partition $V$ into $k$ subsets, $V_1, V_2, ..., V_k$ such that $V_i \cap V_j = \emptyset$ for $i \neq j$, the size of each partition $|V_i| \approx \frac{N}{k}$, $\bigcup_i V_i = V$, and the number of edges whose incident vertices belong to different $V_i$ is minimized. In other words, we seek a partitioning that results in roughly equal sized subgraphs connected by as few edges as possible. In this case we apply the algorithms to the adjacency graph produced by RBF-FD stencils (see \S~\ref{subsec:adjacency}).

The output from graph partitioning algorithms are ideal for distributed applications for two reasons. First, partitions of roughly equal size ensure a balanced workload across processors. Second, edges correspond to ghost node dependencies, so minimizing those connections also ensures minimized data transfer via MPI. 

A number of libraries exist for graph partitioning including Chaco \cite{CHACO1995}, SCOTCH \cite{SCOTCH1996}, and the METIS family of algorithms (e.g., METIS, ParMETIS, hMETIS) \cite{Karypis1999}. The partitioning algorithm varies by library, but in all cases the libraries provide options for partitioning, vertex reordering, and graph coloring.

In addition to the linear partitioning in $x$, our work in \cite{BolligRBFFDCode} also employs METIS for partitioning via the library's included \emph{gpmetis} executable. METIS performs a multi-level recursive bisection algorithm for partitioning. At its core the algorithm first coarsens graphs into a sequence of smaller resolutions, applies a split to the smallest, and then pops back through the levels of recursion to project the coarse split onto finer graphs, smoothing/correcting the split at each level \cite{Karypis1999}. As input, METIS requires an undirected adjacency graph and the desired number of partitions. 
In return for the graph, METIS writes a file containing one integer per vertex, indicating the partition to which each stencil center is assigned. Note that METIS does not guarantee partitions will be contiguous. While this is often the case, METIS may decide that the alternative is more fitting and assign disjoint subgraphs to the same CPU process for reduced communication. 

The adjacency graph for RBF-FD is directed, so symmetry is induced in the associated matrix with $A+A^T$. The added connectivity is harmless to RBF-FD as symmetry is only meant for partitioning purposes and does not impact actual DMs. With respect to load balancing: cutting an edge connecting stencil nodes $s_1$ and $s_2$, is equivalent to cutting its transpose, so METIS is equally resistant apply a cut whether the edge is directed or undirected. 

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{rbffd_methods_content/decompositions/gpmetis_decomp_sphere_4parts.png}
\caption{METIS partitioning of $N=10,201$ nodes to span four processors with stencil size $n=31$. }
\label{fig:metis_decomposed_sphere}
\end{center}
\end{figure}

Figure~\ref{fig:metis_decomposed_sphere} provides an example of a METIS generated partitioning of $N=10,201$ MD nodes for four processors. Three camera angles show the sphere viewed from the $(-x)$-axis (left), $(+x)$-axis (top-right), and $(-z)$-axis (bottom-right). 

%TODO: give overview of how boost graph library is used to construct a sparse adjacency graph





%TODO: \authnote{show example of 4 or 8 partitions in table to see how Q, O, R can be disproportionate.}
%\begin{table}
%\centering
%\caption{Comparison }
%\begin{tabular}{c|c|c|c|c|c|c}
%     & Linear & METIS  \\ \hline
%     &  $\min$  &  $\max$  &  stddev  &  min  & max  &  stddev   \\ \hline
% max &   &   &   &    &    \\
%\end{tabular}
%\end{table}



\section{Index Mappings}

Once a partitioning is available, each MPI process is responsible for its own subset of nodes. 
To simplify accounting, we track nodes in two ways. Each node is assigned
a global index, that uniquely identifies it. This index follows the node 
and its associated data as it is shuffled between processors. In addition, 
it is important to treat the nodes on each CPU/GPU in an identical manner. 
Implementations on the GPU are more efficient when node indices
are sequential. Therefore, we also assign a local index for the nodes on 
a given MPI process, which run from 1 to the maximum number of nodes on that process. 


It is convenient to break up the nodes on a given CPU into various sets
according to whether they are sent to other processors, are retrieved from 
other processors, are permanently on the processor, etc. Note as well, 
that each node has a home processor since the RBF nodes are partitioned into 
multiple domains without overlap.
Table~\ref{tbl:stencil_sets}, defines the collection of index lists that each CPU must maintain for both multi-CPU and multi-GPU implementations.  

        \begin{table}[t]
            \begin{center}
                \begin{tabular}{l l}
                    \hline
                    $\setAllNodes$ &: all nodes received and contained on the CPU/GPU $g$ \\
                    $\setCenters$ &: stencil centers managed by $g$ 
					(equivalently, stencils computed by $g$) \\
                    $\setBoundary$ &: stencil centers managed by $g$ that
                    require nodes from another CPU/GPU \\
                    $\setProvide$ &: nodes managed by $g$ that are sent to other CPUs/GPUs  \\
                    $\setDepend$ &: nodes required by $g$ that are managed by another CPU/GPU \\
                    \hline
                \end{tabular}
                \caption{Sets defined for stencil distribution to multiple CPUs}
                            \label{tbl:stencil_sets}
            \end{center}
        \end{table}
        
                \begin{figure}[ht] 
            \centering
            \includegraphics[width=3.5in]{../figures/paper1/figures/omnigraffle/SimpleExample.pdf} 
            \caption{Partitioning, index mappings and memory transfers for nine stencils ($n=5$) spanning two CPUs and two GPUs. Top: the directed graph created by stencil edges is partitioned for two CPUs. Middle: the partitioned stencil centers are reordered locally by each CPU to keep values sent to/received from other CPUs contiguous in memory. Bottom: to synchronize GPUs, CPUs must act as intermediaries for communication and global to local index translation. Middle and Bottom: color coding on indices indicates membership in sets from Table~\ref{tbl:stencil_sets}: $\setCenters \backslash \setBoundary$ is white, $\setBoundary \backslash \setProvide$ is yellow, $\setProvide$ is green and $\setDepend$ is red.
            }
            \label{fig:stencilSets2CPU}
        \end{figure}	

Refer to Figure~\ref{fig:stencilSets2CPU}, which illustrates a configuration with two 
CPUs and two GPUs, and 9 stencils, four on CPU1, and five on CPU2, separated
by a vertical line in the figure. Each stencil
has size $n=5$. At the top of Figure~\ref{fig:stencilSets2CPU}, stencils are laid out
with blue arrows pointing to stencil neighbors and creating the edges of the directed adjacency graph. Note that the connections between nodes are not 
always bidirectional. For example, node 6 is in the stencil of node 3, but 
node 3 is excluded from the stencil around 6. 
Gray arrows point to stencil neighbors outside the small window and are irrelevant in the following discussion focused only on data flow between two processes on CPU1 and CPU2. 
Since each process is responsible for the derivative evaluation and solution updates for any stencil center, it is clear that edges intersecting the vertical line point to ghost node dependencies. For example, node 8 on CPU1 has a stencil comprised of
nodes 4,5,6,9, and itself. The data associated with node 6 must be retrieved
from CPU2. Similarly, the data from node 5 must be sent to CPU2 to 
complete calculations at the center of node 6.
        
        
       
The center of Figure~\ref{fig:stencilSets2CPU} assigns the nine nodes to local sets for each process. The set of all nodes that a process interacts with is denoted by $\setAllNodes$, which includes both the stencil centers within the partition ($\Interior_i$), and all ghost nodes required to complete the calculations: $\bigcup_{j} \Boundary_{ij}$.  
We let $\setCenters\in\setAllNodes$ contain indices for all nodes within the partition $\Interior_i$. 
Then the set $\setDepend = \setAllNodes \backslash \setCenters$ contains indices of all ghost nodes. 
The set $\setProvide\in\setCenters$ indexes nodes that are needed in other partitions (i.e., $\bigcup_j \Boundary_{ji}$). The set $\setBoundary\in\setCenters$ consists of nodes dependent on values from $\setDepend$. Note that $\setProvide$ and $\setBoundary$ overlap, but can differ in size due to lack of symmetry in the adjacency matrix. To capture the difference, set $\setBoundary \backslash \setProvide$ are nodes that depend on $\setDepend$ but are not sent to other processes, while $\setCenters \backslash \setBoundary$ are nodes that have no dependency on information from other processes.

Constructing these index mappings allows each process to ignore nodes/stencils outside their subdomain and focus only on their subset of the problem. Processes avoid both the cost of storing a complete RBF-FD DM as well as full solution vectors. A rectangular sub-DM is kept for $\setAllNodes$, with compressed solution vectors to match. Our implementation follows a workflow that first generates stencils, and then partitions the adjacency graph in serial. In parallel, each processor loads its assigned partition, constructs index mappings, and solves for RBF-FD weights to generate a local DM.  

\section{Local Ordering}

Figure~\ref{fig:stencilSets2CPU} lists global node indices contained in $\setAllNodes$ for each MPI process. Global indices are paired with a local mapping to indicate the internal node ordering for each process. The structure of set $\setAllNodes$,
   \begin{equation}
 		\setAllNodes = \{ \mathcal{Q}\backslash\mathcal{B} \ \ \mathcal{B}\backslash\mathcal{O} \ \ \mathcal{O} \ \ \setDepend \},
            \label{eq:decompose_g}
        \end{equation}
 is designed to simplify both CPU-CPU and CPU-GPU memory transfers by grouping nodes of similar type. The color of the global and local indices in the figure
 indicate the sets to which they belong. They are as follows: white represents $\setCenters \backslash \setBoundary$, 
 yellow represents $\setBoundary \backslash \setProvide$, green indices 
 represent $\setProvide$, and red represent $\setDepend$.  


 The structure of $\setAllNodes$ offers two benefits: first, solution values in $\setDepend$ and $\setProvide$ are contiguous in memory and can be copied to or from the GPU without the filtering and/or re-ordering normally required in preparation for efficient data transfers. Second, asynchronous communication allows for the overlap of communication and computation. This will be investigated in Chapter~\ref{chap:multigpu_rbffd}, where distinguishing the set $\mathcal{B} \backslash \mathcal{O}$ allows the computation of $\mathcal{Q}\backslash \mathcal{B}$ without waiting on MPI communication to send and receive $\mathcal{O}$ and $\mathcal{R}$. 

When targeting the GPU, communication of solution or intermediate values is a four step process:
   \begin{enumerate}
    \item Transfer $\mathcal{O}$ from GPU to CPU
	\item Distribute $\mathcal{O}$ to other CPUs, receive $R$ from other CPUs
	\item Transfer $\mathcal{R}$ to the GPU
	\item Launch a GPU kernel to operate on $\mathcal{Q}$
   \end{enumerate} 
The data transfers involved in this process are illustrated at the bottom of Figure~\ref{fig:stencilSets2CPU}.
    Each GPU is only aware of the local indexing in Equation~\ref{eq:decompose_g}. Benefiting from the local structure, set 
$\setProvide$ is copied off the GPU and into CPU memory as one contiguous memory block. The CPU then maps local to global indices and transfers $\setProvide$ to other processes. As the set $\setDepend$ is received from one or more processes, the data is reordered into local order before copying a contiguous block of memory to the GPU. 


\begin{figure}
\begin{center}
\includegraphics[width=0.45\textwidth]{rbffd_methods_content/decompositions/MatrixDecompositionSets_RBF-FD_Bowed.pdf} 
\caption{Decomposition for one processor selects a subset of rows from the DM. Blocks corresponding to node sets $\setCenters \backslash \setBoundary$, $\setProvide$, and $\setDepend$ are labeled for clarity. The subdomain/partition is outlined by dashed lines.}
\label{fig:decomp_matrix_view}
\end{center}
\end{figure}

The next step is to translate the local ordering of nodes to a local DM. Consider Figure~\ref{fig:decomp_matrix_view} which shows the index sets for a single partition in context of a global RBF-FD differentiation matrix. Here the gray area is zero, and non-zeros exist between bowed lines. The partition is illustrated as the contiguous set of rows between dashed lines. Set membership for each row is determined based on how the off-diagonal non-zeros in a row match vertically with the center diagonal. When an MPI process constructs the local DM for the partition, the two sets of $\mathcal{B}$ in Figure~\ref{fig:decomp_matrix_view} are compressed into one contiguous set. An example of this effect is demonstrated in Figure~\ref{fig:decomp_spy}. 

Figure~\ref{fig:decomp_spy} shows a local DM for a single partition with the labels indicating the components of Equation~\ref{eq:decompose_g}. The local DM represents the third of four partitions from an original matrix with $N=4096$ rows and $n=31$ non-zeros per row. The RBF-FD stencils were generated on the $N=4096$ MD node set, and partitioned with METIS. 
\begin{figure}
\begin{center}
\includegraphics[width=9cm]{rbffd_methods_content/decompositions/spy_metis_stencil_example_labels.png}
%\includegraphics[width=0.45\textwidth]{rbffd_methods_content/decompositions/spy_metis_stencil_example_part_2_of_4.pdf}
\caption{Spy of the local DM on processor 3 of 4 from a METIS partitioning of $N=4096$ nodes with stencil size $n=31$ and stencils generated with Algorithm~\ref{alg:hash_stencils} ($hnx=100$). Blocks are highlighted to distinguish node sets $\setCenters \backslash \setBoundary$, $\setBoundary$, and $\setDepend$. $\setDepend$ Stencils involved in MPI communications have been permuted to the bottom of the matrix. The split in $\setDepend$ indicates communication with two neighboring partitions. }
\label{fig:decomp_spy}
\end{center}
\end{figure}



%TODO: Domain boundary nodes appear at beginning of the list 

\section{Communication Collectives}
\label{sec:mpi_collectives}

With a partitioning and local ordering complete, the next task is to establish communication collectives for transferring data between MPI processes. Two types of collectives are possible: all-to-all and all-to-subset. 

All-to-all collectives pass data from every processor to all other processors. The most basic MPI implementation of an all-to-all, the MPI\_Alltoall routine, is illustrated in Figure~\ref{fig:mpi_alltoall_visual}. For a cluster of $p$ processes, MPI\_Alltoall assumes that every process intends to send $N_p$ bytes to each of its $p-1$ neighbors. On the left, an output buffer with $p*N_p$ bytes, \emph{sendBuf}, is assembled locally on each process. Block sizes in Figure~\ref{fig:mpi_alltoall_visual} are indicative of the number of bytes sent to each process. Example data A, B, C are labelled with subscripts to indicate the destination.  When the collective executes, MPI\_Alltoall scatters $N_p$ bytes from each process to each process to effectively interchange/transpose all data as it is received in \emph{recvBuf} on the right of Figure~\ref{fig:mpi_alltoall_visual}. 

\begin{figure}
\centering
\includegraphics[width=10cm]{../figures/omnigraffle/MPI_Alltoall_Visual.png}
\caption{An ``all-to-all" communication collective interchanges/transposes data across processes. All processors (e.g., 0, 1, 2) connect and transmit data subsets (e.g., A, B, C) to every other processor.}
\label{fig:mpi_alltoall_visual}
\end{figure}

All-to-all collectives are only useful when data needs to be simultaneously interchanged between all processes. Since RBF-FD partitioning typically restricts overlap in subdomains to a subset of neighbors, it makes sense to adopt an all-to-subset approach and avoid the overhead of unnecessary connections and memory padding required by MPI\_Alltoall. 
Two types of all-to-subset are illustrated in Figures~\ref{fig:mpi_alltoallv_visual} and \ref{fig:mpi_isendirecv_visual}. 


\begin{figure}
\centering
\includegraphics[width=10cm]{../figures/omnigraffle/MPI_Alltoallv_Visual.png}
\caption{RBF-FD interchanges a variable number bytes between processes. An MPI\_Alltoallv collective compresses the interchange to allow variable message sizes between processors.}
\label{fig:mpi_alltoallv_visual}
\end{figure}

The first example in Figure~\ref{fig:mpi_alltoallv_visual} shows the behavior of an MPI\_Alltoallv. The MPI\_Alltoallv collective interconnects processes and transposes data similar to MPI\_Alltoall, yet it allows variable message sizes per connection. With no padding on messages, fewer bytes are transmitted and the overall communication time drops. Modern implementations of MPI detect when zero-byte messages are intended for processes and skip those connections. This gives MPI\_Alltoallv an all-to-subset behavior. Note that MPI\_Alltoallv should be used with caution: while zero-byte connections are avoided, the overhead in checking message sizes grows as the number of processors scales. For large $p$ the communication time can be dominated by those operations, defeating the gains in all-to-subset connection (see e.g., \cite{Balaji2010}).


\begin{figure}
\centering
\includegraphics[width=10cm]{../figures/omnigraffle/MPI_IsendIrecv_Visual.png}
\caption{A true ``all-to-subset" collective allows for variable message sizes, and strictly truncates the number of connections between processors to only required connections. This collective is implemented with MPI\_Send/MPI\_Recv or MPI\_Isend/MPI\_Irecv.}
\label{fig:mpi_isendirecv_visual}
\end{figure}

Figure~\ref{fig:mpi_alltoallv_visual} presents an all-to-subset collective implemented with sends and receives that are either blocking (MPI\_Send, MPI\_Recv) or non-blocking (MPI\_Isend, MPI\_Irecv). In this example processes strictly communicate with the subset of neighbors necessary; no zero-byte messages impact communication times for large $p$. 

The difference between MPI\_Send/MPI\_Recv and MPI\_Isend/MPI\_Irecv
The first improvement on Alltoallv collectives is to truncate the number of connections made between processes. Compact stencils implies an overlap region for each processor that draws values from a limited number of neighboring processors. 


%TODO: encoding into a sendBuf
%TODO: decoding out of recvBuf

%TODO: here I need figure showing isend/irecv in context of encode decode


\section{Scaling Results} 
This section provides evidence that our implementation is scalable. The term scalable is used loosely as there are two types of scaling: weak and strong. Weak scaling implies the total problem size and processor count can increase. The performance scalability of the code depends on the problem size and the MPI collective. 

To demonstrate the effectiveness of our decomposition and indexing, we perform scaling experiments. 

Strong scaling tests the growth in time for a fixed total problem size, and a variable number of processors. 

Weak scaling considers the amount of time for a fixed problem size per process and variable number of processors. That is to say, each processor has roughly the same amount of work, so as we scale to a large number of processors, changes in time will be the result of increased communication overhead. 



In order to manage expectations of parallel performance in a distributed environment, we turn to Amdahl's law, which states that the total speedup possible for $p$ processors is limited by the fraction of code that must run in serial:
\begin{align}
S_p = \frac{T_s + T_p}{T_s + \frac{T_p}{p}},  \nonumber
\end{align}
where $T_s$ and $T_p$ are the time spent in serial and parallelizable portions of code respectively. Scaling to a very large number of processors may make the time in parallelizable portions of code vanish, but $T_s$ remains constant. In any distributed implementation communication is always included in estimate for $T_s$, so it is expected for  pertinent to choose an MPI collective with minimal overhead and latency. 

As the number of processors grows, more messages enter the infiniband network, increasing communication times. 

%TODO: copy down, encode, send/recv, decode, copy up


\subsection{Basic Scaling (Keeneland)}


%TODO: show evidence from paper (keeneland) on scaling as send/recv with blocking. 

The benchmarks in our first paper (\cite{BolligFlyerErlebacher2012}) demonstrated the initial scaling of our implementation for a distributed RK4 iteration using primitive blocking MPI\_Send/MPI\_Recv communication routines. The collective was a round-robin all-to-subset connection between processors, each taking a turn send data (0 or more bytes) to all other processes and otherwise waiting to receive data. Blocking communication was implemented for code verification. Data in \cite{BolligFlyerErlebacher2012} was acquired on the multi-GPU cluster named Keeneland \cite{Vetter2011}. The implementation demonstrated poor scaling behavior as blocking collectives serialized transmissions, and every process had to wait until the end of the collective to proceed. The objective in \cite{BolligFlyerErlebacher2012}, however, was not to present a well tuned distributed GPU implementation of RBF-FD. Instead we focused on the design decisions for partitioning, index mapping, etc. and verification of both single and distributed GPU implementations as inefficient as they were. 


\subsubsection{MPI\_Alltoallv Keeneland}
%TODO: non-blocking on keeneland
With the distributed multi-CPU and mult-GPU implementations were verified we were able to focus our efforts on reducing the collective bottlenecks. The first step was to test an MPI\_Alltoallv on Keeneland. Results are as follows

Scaling tests were performed on the unit sphere with the linear-in-$x$ partitioning. 



\subsubsection{MPI\_Alltoallv Spear}
Similar numbers are obtained on Spear, the multi-GPU cluster at FSU. Again, results are for strong scaling across 10 GPUs

However, in the case of Spear it was found that the performance of the interconnect


As an aside, some benchmark on Spear highlight the imbalanced workloads on processors. Excessive wait times on MPI collectives meant that processes were not entering communication stages at similar times. In response launched a new effort to load-balance and tune MPI for a large number of processes.

At this point scaling the code allows us to reach a higher number of nodes. To confirm this  


\subsection{Tuned Scaling (Itasca)}


%TODO: how does \cite{Schubert2011} do it? \cite{Yokota2010}. 


By issuing the receives before encoding the send buffers we ensure that no processes wait unnecessarily long for their collectives to start.



communicate with essential neighbors that need/provide data

MPI\_Isend/MPI\_Irecv also allows for overlapping communication and computation by posting receives early 

\subsubsection{Irecv pre-fill}


\subsubsection{No Decode}
\authnote{figure: per iteration stacked bar for n=50 and 16 processes to show cost of decode}
\authnote{figure: algorithm for collective}

\authnote{figure: alltoall to isend improvement. justify comm\_combo for up to 16 procs.} 
\authnote{figure: comm\_combo gains}
\authnote{figure: algorithm for collective}

\subsubsection{Immediate Isend on Encode}
\authnote{figure: algorithm for collective}

\authnote{back to section: figure: improvement on all CPU collectives (n=50)}

\authnote{table: show percentage of comm time for actual mpi time. busy network can cause slower comm times. but the decode cost is gone. it can also be an issue if we have saturated comm pipes}



\section{Conclusion}

      Our approach is scalable to a very large number of 
		processors as the individual processes do not require the 
		full mapping between RBF nodes and CPUs. 
		
Here scalable is used loosely as there are two types of scaling: weak and strong. Weak scaling implies the total problem size and processor count can increase. The performance scalability of the code depends on the problem size and the MPI collective. In Figure~\ref{fig:strong_scaling} the strong scaling of $N=10^6$ nodes is tested on Itasca, a supercomputer at the Minnesota Supercomputing Institute.   


To test and demonstrate scaling of our method, we consider an idealized regular grid in three dimensions. 

verification here is only significant to ensure we are applying all weights. 
We apply weights to calculate derivatives of a test function in X, Y, Z, and the Laplacian. 
the grid is regular and 3D.
We test strong scaling on a $N=160^3$ grid, and weak scaling with $N_p=4000$. This way at $p=1024$ processes we have weak scaling testing the full $N=160^3$ grid. 


Although our weak scaling results are promising, they also contain a problem. First, since we are subsampling a $160^3$ regular grid to get the first $N=p*4000$ nodes, many of the tests consider domains that are ``L" shaped and have odd partitions with limited connectivity.

\authnote{table showing the min and max Osize,Rsize}


\subsection{Bandwidth}

To understand the impact of MPI on these benchmarks we calculate the average and aggregate collective bandwidths. The average bandwidth considers the MPI throughput from the perspective of one processor. 

The aggregate bandwidth reveals when processes saturate the interconnects. 

We consider a simple idealized problem where derivatives are computed over a regular grid generated in 3-D. The experiment computes the SpMV one thousand times. At the end of each SpMV the MPI\_Alltoallv collective is used to synchronize the local derivative vectors. After one thousand iterations, each process computes the local norm of the resulting vector and an MPI\_Reduce collective dra

In Figure~\ref{fig:strong_scaling} the strong scaling of $N=10^6$ nodes is tested on Itasca, a supercomputer at the Minnesota Supercomputing Institute.   

\begin{figure} 
\centering
\includegraphics[width=0.45\textwidth]{performance_content/scaling/strong_scaling_4M_regular_alltoallv.png}  
\includegraphics[width=0.45\textwidth]{performance_content/scaling/strong_scaling_4M_regular_alltoallv_speedup.png} \\
\includegraphics[width=0.45\textwidth]{performance_content/scaling/strong_scaling_4M_regular_spmvOnly.png}
\includegraphics[width=0.45\textwidth]{performance_content/scaling/strong_scaling_4M_regular_alltoallv_commOnly.png} 
\caption{Strong scaling the distributed SpMV for $N=4096000$ nodes (i.e., a $160^3$ regular grid) and various stencil sizes. Here the MPI\_Alltoallv collective operation is used. (Left) Strong scaling of SpMV (including cost of communication). (Center) Strong scaling of computation only. (Right) Strong scaling of communication only.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{performance_content/scaling/strong_scaling_4M_regular_n17comparison_commOnly.png}
\caption{Scaling comparison of MPI\_Alltoallv and two types of MPI\_Isend/MPI\_Irecv collectives: one with MPI\_Irecv issued after filling the MPI\_Isend send buffer (post-fill), and the other issued before filling the MPI\_Isend buffer (pre-fill).}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{performance_content/scaling/weak_scaling_np4000_regular_alltoallv_commOnly.png}
\includegraphics[width=0.45\textwidth]{performance_content/scaling/weak_scaling_np4000_regular_alltoallv.png}   \\
\includegraphics[width=0.45\textwidth]{performance_content/scaling/weak_scaling_np4000_regular_spmvOnly.png} 
\includegraphics[width=0.45\textwidth]{performance_content/scaling/weak_scaling_np4000_regular_n17_compare.png}
\caption{Weak scaling of the SpMV}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{performance_content/scaling/weak_scaling_np4000_regular_n17_compare_commOnly.png}
\caption{Communication times for weak scaling SpMV. The difference between MPI\_Alltoallv and MPI\_Irecv times is attributed to the overhead in checking for zero-byte messages for the large number of processors. Similar behavior for MPI\_Alltoallv is found in \cite{Balaji2010}.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{performance_content/scaling/strong_scaling_4M_regular_isend.png}  
\includegraphics[width=0.45\textwidth]{performance_content/scaling/weak_scaling_np4000_regular_isend.png}  
\caption{Scaling of SpMV with MPI\_Isend/MPI\_Irecv}
\end{figure}


\chapter{Distributed GPU SpMV (incomplete)}
\label{chap:multigpu_rbffd}

Distributing SpMV across multiple GPUs poses a new problem: as previous mentioned, the data sent and received via MPI collectives must be copied from device to host and vice-versa. To amortize this cost we introduce a novel overlapping algorithm to hide the cost of communication behind the cost of a concurrent SpMV on the GPU. 


Petascale computing centers around the world are leveraging GPU accelerators to achieve peak performance. In fact, many of today's high performance computing installations boast significantly more GPU accelerators than CPU counterparts. The Keeneland project is one such example, currently with 240 CPUs accompanied by 360 NVidia Fermi class GPUs with at least double that number expected by the end of 2012 \cite{Vetter2011}. 

Such throughput oriented architectures require developers to decompose problems into thousands of independent parallel tasks in order to fully harness the capabilities of the hardware. To this end, a plethora of research has been dedicated to researching algorithms in all fields of computational science. Of interest to us are methods for atmospheric- and geo-sciences. 


Similar approaches to overlapping communication and computation can be found in \cite{Schubert2011} and \cite{Thibault2009}.

\section{Overlapped Queues}

\section{Avoiding Copy Out}
When operating on multiple GPUs we avoid the copy-out or decode phase by requiring that the local ordering of nodes sort the set $\setDepend$ by the rank of the process sending each node. This way, when the MPI collective finishes and all values arrive contiguous by provider, the data can be copied directly to the GPU without reordering.




\subsection{Avoiding Copy-Out on CPU}

\section{Scaling}
We scale the SpMV across the GPUs on Cascade.

\subsection{Fermi}
\subsection{Kepler}
\subsection{Multiple Kernel Scheduling}
describe fermi's ability to schedule multiple kernels, what it means for our queues. Do we need multiple queues, or just one that is non-blocking. How do we indicate we are done communicating if there is no queue to add markers to? 


\section{HPC Spear Cluster} 

\section{Keeneland}

\subsection{Shared K20s}



\section{Future Work}

One of the problems with choosing to work in OpenCL is the fact that the standard offers the lowest common denominator of features from the various hardware vendors that support it. Many vendor specific features never make it into the language. 

Take for example GPUDirect, a technology introduced first CUDA v3.1 for NVidia hardware. GPUDirect allows direct access to GPU memory addresses from various sources including other GPUs. The technology allows GPUs to bypass copies to host memory en-route to another GPU on the same compute node. Combine GPUDirect with the new MPI aware features in CUDA v5.0 and data can pass directly from a GPU onto the infiniband fabric and up to another GPU \cite{NvidiaGPUMPI}. This type of feature may never be available in OpenCL. 




\ifstandalone
\bibliographystyle{plain}
\bibliography{merged_references}
\end{document}
\else
\expandafter\endinput
\fi

