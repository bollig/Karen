
\makeatletter
\@ifundefined{standalonetrue}{\newif\ifstandalone}{}
\@ifundefined{section}{\standalonetrue}{\standalonefalse}
\makeatother
\ifstandalone
\documentclass{report}

\input{all_usepackages} 
\usepackage[margin=1.25in]{geometry}

\begin{document}
\fi

\chapter{GPU SpMV}
\label{chap:gpu_rbffd}

As $N$ grows larger, it behooves us to work on parallel architectures, be it CPUs or GPUs. GPUs \authnote{finish intro: gpus provide high flop-to-watt yield, etc etc} 
%TODO: gpus benefit from faster memory bandwidth, parallel compute units and sharing across threads.
%TODO: GPU hardware
%TODO: GPU software (OpenCL vs CUDA)
%TODO: targeting the GPU for SpMV (how to write a kernel as forloop without the for)
%TODO: 

 
%TODO: list all known work on SpMV for GPU
%TODO: emphasize that the GPU SpMV has been studied since before 2006.
%TODO: SpMV condenses sparse matrix to dense form. 
%TODO: each of the dense forms are unique (describe each)
%TODO: newer forms are available




In the last few years, GPUs transitioned from hardware dedicated to the embarrassingly parallel tasks involved in graphics rendering (e.g., 
rasterization) into multi-core co-processors for high performance scientific computing.  Thanks to the highly profitable and always demanding gaming industry, what began as a static rendering pipeline, was molded to allow fully dynamic execution with a SIMD-like programming model (Single Instruction Multiple Threads or SIMT). Changes in hardware have been 
followed closely by evolving programming languages. Today, GPUs can be targeted from C/C++, FORTRAN, Java, Python, MATLAB, and more. The list seems endless, and (to our knowledge) encompasses all languages applicable to scientific computation. 

%TODO: we will discuss how GPU hardware and languages evolved to exceptionally higher compute capability than traditional CPUs, and became a popular platform for high performance computing.



General Purpose GPU (GPGPU) computing is one of today's hottest trends within scientific computing. 
The release of NVidia's CUDA at the end of 2006 marked both a 
redesign of GPU architecture, plus the addition of a new software layer that finally made GPGPU accessible to the general public. The CUDA API includes routines for memory control, interoperability with graphics contexts (i.e., 
OpenGL programs), and provides GPU implementation subsets of BLAS and FFTW libraries \cite{CudaGuide2011}. After the undeniable success of CUDA for C, new projects emerged to encourage GPU programming in languages like FORTRAN (see e.g., HMPP \cite{HMPP2009} and Portland Group Inc.'s CUDA-FORTRAN \cite{CudaFortran2009}). 

In early 2009, the Khronos Group--the group responsible for maintaining OpenGL--announced a new specification for a general 
parallel programming lanugage referred to as the Open Compute Language (OpenCL) \cite{OpenCL2009}. Similar in design to the CUDA language---in many ways it is a simple refactoring of the predecessor---the goal of OpenCL is to provide a mid-to-low level API and language to control any multi- or many-core processor in a uniform fashion. Today, OpenCL drivers exist for a variety of hardware including NVidia GPUs, AMD/ATI CPUs and GPUs, and Intel CPUs. 

This \textit{functional portability} is the cornerstone of the OpenCL language. However, functional portability does not imply performance portability. That is, OpenCL allows developers to write kernels capable of running on all types of target hardware, but optimizing kernels for one type of target (e.g., GPU) does not guarantee the kernel will run efficiently on another target (e.g., CPU).
% Already, today, CPUs are tending toward many core architectures. Simultaneously, the once specialized many-core GPUs now offer general purpose functionality. New architectures like the AMD Fusion \cite{AMDFusion} join CPU and GPU on the same chip proving that 
%, and It is easy to see that soon the CPU and GPU will meet somewhere in the middle as general purpose many-core architectures. OpenCL is an attempt to standardize programming before this intersection occurs. 
%TODO: clean. ATI, INTEL, NVIDIA all have units. 
With CPUs tending toward many cores, and the once special purpose, many-core GPUs offering general purpose functionality, it is already possible to see the CPU and GPU converging into general purpose many-core architectures. Already, ATI has introduced the Fusion APU (Accelerated Processing Unit) which couples an AMD CPU and ATI GPU within a single die. OpenCL is an attempt to standardize programming ahead of this intersection. 

%Home computers, smart-phones and other devices containing many- or multi-core compute units internally have driven the generalization of the accelerator language to provide a unified approach to targeting any available hardware. 

Petascale computing centers around the world are leveraging GPU accelerators to achieve peak performance. In fact, many of today's high performance computing installations boast significantly more GPU accelerators than CPU counterparts. The Keeneland project is one such example, currently with 240 CPUs accompanied by 360 NVidia Fermi class GPUs with at least double that number expected by the end of 2012 \cite{Vetter2011}. 

Such throughput oriented architectures require developers to decompose problems into thousands of independent parallel tasks in order to fully harness the capabilities of the hardware. To this end, a plethora of research has been dedicated to researching algorithms in all fields of computational science. Of interest to us are methods for atmospheric- and geo-sciences. 

%TODO: ALL SPMV related work.
\cite{Bell2009} 
%TODO: \cite{Kreuzer2012} in distributed GPU. 
\cite{Vuduc2005} etc. 

\section{GPGPU}
GPGPU evolution
\subsection{OpenCL}
OpenCL is chosen with the future in mind. Hardware changes rapidly and vendors often leapfrog one another in the performance race. By selecting OpenCL, we hedge our bets on the functional portability

\subsection{Hardware Layout}
Modern GPUs have a memory hierarchy and hardware layout. 

\section{Performance}
\subsection{GFLOP Throughput}
In order to quantify the performance of our implementation, we can measure two
factors. First, we can check the speedup achieved on the GPU relative to the
CPU to get an idea of how much return of investment is to be expected by all
the effort in porting the application to the GPU. Speedup is measured as the
time to execute on the CPU divided by the time to execute on the GPU. 

The second quantification is to check the throughput of the process. By
quantifying the GFLOP throughput we have a measure that tells us two things:
first, a concrete number quantifying the amount of work performed per second by
either hardware, and second because we can calculate the peak throughput possible on
each hardware, we also have a measure of how occupied our CPU/GPU units are.
With the GFLOPs we can also determine the cost per watt for computation and
conclude on what problem sizes the GPU is cost effective to target and use. 

Now, as we parallelize across multiple GPUs, these same numbers can come into
play. However we are also interested in the efficiency. Efficiency is the
speedup divided by the number of processors. With efficiency we have a measure
of how well-utilized processors are as we scale either the problem size (weak)
or the number of processors (strong). As the efficiency diminishes we can
conclude on how many stencils/nodes per processor will keep our processors
occupied balanced with the shortest compute time possible (i.e., we are
maximizing return of investment). 

\subsection{Expectations in Performance}
Many GPU applications claim a 50x or higher speedup. This will never be the case for RBF-FD for the simple reason that the method reduces to an SpMV. The SpMV is a low computational complexity operation with only two operations for every one memory load. 


\section{Targeting the GPU}

\subsection{OpenCL}
\subsection{Naive Kernels}
\subsection{SpMV Formats/Kernels}

CSR Bytes:Flop ratio: \url{http://arxiv.org/pdf/1101.0091v1.pdf}

\section{Performance Comparison}
\subsection{Performance of Cosine CL vs VCL}
\subsection{VCL Formats Comparison}

Our assumption with RBF-FD in this manuscript is that all stencils will have equal size. Due to this, the ELL format is preferred as the default. 
 



As part of future work into accelerating RBF methods, investigations are underway into the new Intel Phi architecture. 

 investigating optimizations that target both GPUs and Phi cards for a class of numerical methods based on Radial Basis Functions (RBFs) to solve Partial Differential Equations. RBF methods are increasingly popular across disciplines due to their low complexity, natural ability to function in higher dimension with minimal requirements for an underlying mesh, and high-order---in many cases, spectral---accuracy. RBF methods can be viewed as generalizations of many traditional methods such as Finite Difference and Finite Element to allow for truly unstructured grids. This generalization allows one to reuse many of the same techniques (e.g., sparse matrices, iterative solvers, domain decompositions, etc.) to efficiently obtain solutions. The variety of hardware available on Cascade will help us establish a clear argument in the choice of accelerator type and resolve the dilemma between choosing Phi vs GPU for our method. Since RBFs generalize other methods, our results should have broad reaching impact to answer similar questions for related methods.



With the generalization of RBF-FD derivative computation formulated as a sparse matrix multiplication, we can 
% TODO: mention CUSP as alternative but concentrate on VCL
consider the various sparse formats provided by CUSP and ViennaCL. 

%TODO later: \item All stencils with non-uniform size
%TODO: What is the optimal choice of sparse container? How do the sparse containers compare in performance to each other, and to our custom kernels? What can we conclude? 

Compare formats: 
\begin{itemize}
\item ELL
\item COO
\item CSR
\item Other formats such as HYB, JAD, DIA are considered on the GPU
\end{itemize}

How is communication overlap handled with each format? 


Conclude: sparse containers allow increased efficiency compared to our custom kernels. The custom kernels compete with CSR and COO. 


From the definition of RBF-FD we can formulate the problem computationally in two ways. First, stencil operations are independent. Therefore, we can write kernels with perfect parallelism by dedicating a single thread per stencil or a group of threads per stencil.  

Unfortunately, perfect concurrency does not imply perfect or even ideal concurrency on the GPU. 

We first demonstrate the case where one thread is dedicated to each stencil. This is followed by dedicating a group of thread to the stencil. In each case we are operating under the assumption that each stencil is independent on the GPU. 

To further optimize RBF-FD on the GPU, we formulate the problem in terms of a Sparse Matrix-Vector Mulitply (SpMV). When we consider the problem in this light we generate a single Differentiation Matrix that can see two optimizations not possible with our stencil-based view: 
\begin{itemize} 
\item First, the sparse containers used in SpMV allow for their own unique optimizations to compress storage and leverage hardware cache.
\item Evaluation of multiple derivatives can be accumulated by association into one matrix operation. This reduces the total number of floating point operations required per iteration. 
\end{itemize}



We compare the performance of our custom kernel to ViennaCL kernels (ELL, CSR, COO, HYB, DIAG), UBlas (COO, CSR) and Eigen (COO, CSR, ELL)


ViennaCL allows control of the number of work-items for each kernel. 

The library can tune itself to find the optimal number of work-items based on the device. 

%TODO: what is profile for each GPU type
%TODO: What is the significance of tuning on our problem $n=17, 31, 50, 101,$ etc. 
%TODO: experiment: SpMV on N=10^6, n = variable (5->105)
%TODO: idealized experiment: SpMV on N=10^6 regular grid with n=variable.




%TODO: finish fill-in
When matrix is sparse, a direct LU decomposition causes fill-in on factorization. In some cases the fill-in can be minimal, but in general one must assume that fill in can turn the sparse matrix into a dense matrix. To invert and solve Equation~\ref{eq:implicit_eq}, use an iterative solver like GMRES. The GMRES algorithm (described further in Chapter~\ref{chap:applications} applies successive SpMVs along with other vector operations to converge on a solution. Due to the dominance of SpMV in GMRES, the performance of RBF-FD reduces once again to SpMV.





%
%
%Hardware architecture%•	Memory layout%•	Processing cores%•	Trends in hardware since 2006 (additions and benfits)%Optimization%•	SpMV memory layout%•	Scheduling threads%•	Reductions%OpenCL%•	Why? %o	Cross platform support%o	Asynchronous Queuing with Dependencies%•	Implementations details%o	Kernel%o	Work-Item%o	Work-Group%o	NDRange%o	Queue%o	Etc.%•	How does it compare to CUDA? Phi%•	Latest trends%o	Phi: bind against MKL for optimized CPU and MIC%o	CUDA-MPI%o	CUDA Sub-Kernel calls%o	CUDA uptake %•	E.g., Matlab (MEX compiled kernel wrappers)%Conclusions on GPGPU%•	Benefits are good%o	Cheap to purchase < $1K%o	superior performance 1.2 TFLOPs possible in one card%o	was a trending technology (major uptake in supercomputing and national labs)%•	Downsides were varied%•	Overall Impression is that%o	Uptake was wide-spread for research projects%o	Focus was on determining limits of the hardware%•	Many studies focused on optimization of primitives which allow general use in applications such as RBF-FD without recreating the wheel when it comes to optimal algorithms. Allows researchers to concentrate on other investigations into application, preconditioning, data analysis, etc.%Newcomers to the field are interested USING gpgpu applications, rather than writing them  








\chapter{Distributed RBF-FD}
\label{chap:distributed_rbffd}


%TODO: Similar Overlapping algorithm: \url{http://arxiv.org/pdf/1101.0091v1.pdf}

Parallelizing RBF-FD in a distributed environment requires three
design decisions \cite{Saad2003}. First, the problem is partitioned in some fashion to distribute work across multiple processes. Intelligent partitioning
impacts load balancing of processors and the ratio of computation versus communication; imbalanced computation can result in excessive delay per
iteration as some processors tackle larger problem sizes with others sitting idle. Second, one must determine whether processes have access to all or a subset of node information,
solution values, etc. and establish index mappings that translate between a local context and the global problem. Third, the local ordering of indices is established to improve solver efficiency and/or simplify operations. Node
ordering is also significant in the future discussion of offloading computation to GPUs as it can help to minimize data transfer between CPU and GPU. 

The following sections detail the approach to distributed computing and a few optimizations that allow RBF-FD to scale over a thousand processes. In later chapters these same decisions will tie into the resulting performance of the distributed multi-GPU implementations. 

%TODO: why implement independent of PETSc, Deal.II, Trilinos, etc. 


%
%
%
%\section{Next}
%
%Parallelization of RBF-FD is achieved at two levels. First, the
%physical domain of the problem is partitioned
%into overlapping subdomains, each handled by a different MPI process. All CPUs
%operate independently to compute/load RBF-FD stencil weights, run diagnostic
%tests and perform other initialization tasks. A CPU computes only weights
%corresponding to stencils centered in the interior of its partition. After
%initialization, CPUs continue concurrently to solve the PDE. Communication
%barriers ensure that the CPUs execute in lockstep to maintain consistent
%solution values in regions where partitions overlap.  The second level of
%parallelization offloads time-stepping of the PDE to the GPU.  Evaluation of
%the right hand side of Equation~(\ref{eq:evaluation_with_hyperviscosity}) is
%data-parallel: the solution derivative at each stencil center is evaluated
%independently of the other stencils. This maps well to the GPU, offering decent
%speedup even in unoptimized kernels. Although the stencil weight calculation is
%also data-parallel, we assume that in this context that the weights are
%precomputed and loaded once from disk during the initialization phase. 

%TODO: Need partitioning, global and local index maps, and local ordering for a distributed matrix operation \cite{Saad2003} 

\section{Partitioning}

For ease of development and parallel debugging, partitioning is initially
assumed to be linear within one physical direction (typically the
$x$-direction). Figure~\ref{fig:decomposed_sphere} illustrates a partitioning of
$N=10,201$ nodes on the unit sphere onto four CPUs.

Each partition, illustrated
as a unique color, contains many \emph{stencil centers}. Although \emph{stencil centers} are contained within a partition, there is no requirement for all \emph{stencil nodes} to be contained within the same partition. As a result, many stencils require information updates from neighboring partitions for nodes that are referred to as \emph{ghost nodes} \cite{NeEDREF}. %TODO: need ghost node ref.
In many cases, \emph{ghost nodes} are treated the same as any other stencil node. The CPU process in charge of a partition is fully aware of the ghost node coordinate, current solution value(s), etc.. However, values at ghost nodes are modified by another process, so changes must be explicitly synchronized via an MPI collective for dependent processes to maintain consistency. 

In Figure~\ref{fig:decomposed_sphere}, alternating representations between node points and interpolated surfaces illustrates the
overlap regions where ghost nodes reside. Due to stencil dependencies in each partition, the overlap region representations are double-wide---i.e., they contain a set of ghost nodes for both the left and right partitions. 

As the stencil size increases, the width of the
overlap regions relative to total number of nodes on the sphere proportionally increases. In the case of the unit sphere from Figure~\ref{fig:decomposed_sphere}, the width of the overlap is roughly $\sqrt{n}$ for stencil size $n$. Figure~\ref{fig:decomposed_sphere} shows the case of $n=31$ nodes per stencil. Higher order RBF-FD stencils (i.e., larger stencil sizes) exacerbate the situation by further increasing the number of bytes that must be sent via MPI. Observe that since stencils need not have symmetric dependencies (i.e., if stencil $s_1$ depends on $s_2$, $s_2$ need not depend on $s_1$), the number of ghost nodes for each partition can vary.

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=2.5in]{../figures/paper1/figures/vortex_rollup/4procs_N10K_n31.pdf}
\caption{Partitioning of $N=10,201$ nodes to span four processors with stencil size $n=31$. }
\label{fig:decomposed_sphere}
\end{center}
\end{figure}

The choice for a linear partitioning is simple and easy to code. Each MPI process has a left and right neighbor, so communication is straightforward. On a high number of processors, there are two issues: 1) increasing the number of processors quickly reduces the width of each partition and can result in stencils dependent on more than one partition in each direction introducing the need for more complex collectives; and 2) with near uniform node distributions the resulting partitions are of unequal size and processors are improperly balanced. Thus, in the case of the sphere, linear partitioning is not ideal. 


Many other options for partitioning the sphere exist. In atmospheric and geophysical communities for example, one often finds the cubed-sphere \cite{Ivan2011, Katta2012}, which transcribes a subdivided cube onto the sphere and assigns projected rectangular elements to individual processors. Another option is the icosahedral geodesic grid \cite{Randall2002}, which evenly balances the computational load by distributing equal sized geodesic triangles across processors. The options for partitioning the sphere are endless, and are outside the scope of this work. 



Other interesting partitionings can be generated with software libraries such as the METIS \cite{Karypis1999} family of algorithms, capable of partitioning and reordering directed graphs produced by RBF-FD stencils. 



%TODO: what is the percentage overlap for $n$? $\frac{1}{2} n^{\frac{1}{d}}$ gives depth into neighbor since n is uniformly sampled we expect a cube shape. (SHould be literature on this...no?)} % assume sphere, sqrt of N for scaling area covered by stencil


In order to partition our nodes, METIS requires an undirected adjacency graph representing the edges that connect nodes. In this case the adjacency graph represents edges connecting nodes in a mesh. For RBF-FD there is no well-defined mesh. Rather, every node is connected to multiple stencil centers.  of connecting stencil nodes to stencil centers. An undirected  To produce this we generate 

The undirected graph is used only for partitioning and subsequently discarded. 

METIS divides stencils into contiguous partitions of nearly equivalent size. 
%TODO: Need to flesh detail on METIS

\begin{figure}[ht!]
\begin{center}
\begin{subfigure}[b]{0.425\textwidth}
\includegraphics[width=2.5in]{../figures/omnigraffle/DirectedAdjacencyGraph.png}
\caption{Directed (Unsymmetric)}
\label{fig:directed_graph}
\end{subfigure}
\begin{subfigure}[b]{0.425\textwidth}
\includegraphics[width=2.5in]{../figures/omnigraffle/UndirectedAdjacencyGraph.png}
\caption{Undirected (Symmetric)}
\label{fig:undirected_graph}
\end{subfigure}
\caption{A simple adjacency graph and corresponding matrices. Edges connecting nodes of RBF-FD stencils produce (a) a directed adjacency matrix. To partition RBF-FD stencils, METIS requires conversion to (b) an undirected graph/matrix.}
\label{fig:adjacency_matrix}
\end{center}
\end{figure}

In Figure~\ref{fig:undirected_graph} new non-zeros are introduced to induce symmetry. Since the goal is to partition the physical domain, this added connectivity is harmless to RBF-FD. However, with respect to load balancing, this may not be ideal. For every new nonzero introduced to a row, METIS assumes an additional node utilized by the partition. When the node is a false connection, it is one fewer centers that METIS will assign to the partition in an attempt to keep all partitions balanced. \authnote{show example of 4 or 8 partitions in table to see how Q, O, R can be disproportionate.}



\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.8\textwidth]{rbffd_methods_content/decompositions/gpmetis_decomp_sphere_4parts.png}
\caption{METIS partitioning of $N=10,201$ nodes to span four processors with stencil size $n=31$. }
\label{fig:metis_decomposed_sphere}
\end{center}
\end{figure}

%TODO: related work that uses linear partitioning
%Thibault et al. \cite{Thibault200*} use a linear partitioning in the $z$-direction of a 3D cube domain.  
%TODO: related work that uses METIS.




\section{Index Mappings and Local Node Ordering}


\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.45\textwidth]{rbffd_methods_content/decompositions/MatrixDecompositionSets_RBF-FD_Bowed.pdf} 
\caption{Decomposition for one processor selects a subset of rows from the DM. Blocks corresponding to node sets $\setCenters \backslash \setBoundary$, $\setProvide$, and $\setDepend$ are labeled for clarity. The subdomain for the processor is outlined by dashed lines.}
\label{fig:decomp_matrix_view}
\end{center}
\end{figure}



\section{Local node ordering}

After partitioning, each CPU/GPU is responsible for its own subset of nodes. 
To simplify accounting, we track nodes in two ways. Each node is assigned
a global index, that uniquely identifies it. This index follows the node 
and its associated data as it is shuffled between processors. In addition, 
it is important to treat the nodes on each CPU/GPU in an identical manner. 
Implementations on the GPU are more efficient when node indices
are sequential. Therefore, we also assign a local index for the nodes on 
a given CPU, which run from 1 to the maximum number of nodes on that CPU. 

It is convenient to break up the nodes on a given CPU into various sets
according to whether they are sent to other processors, are retrieved from 
other processors, are permanently on the processor, etc. Note as well, 
that each node has a home processor since the RBF nodes are partitioned into 
multiple domains without overlap.
Table~\ref{tbl:stencil_sets}, defines the collection of index lists that each CPU must maintain for both multi-CPU and multi-GPU implementations.  


\begin{figure}[ht!]
\begin{center}
\includegraphics[width=9cm]{rbffd_methods_content/decompositions/spy_metis_stencil_example_labels.png}
%\includegraphics[width=0.45\textwidth]{rbffd_methods_content/decompositions/spy_metis_stencil_example_part_2_of_4.pdf}
\caption{Spy of the sub-DM view on processor 3 of 4 from a METIS partitioning of $N=4096$ nodes with stencil size $n=31$ and stencils generated with Algorithm~\ref{alg:hash_stencils} ($hnx=100$). Blocks are highlighted to distinguish node sets $\setCenters \backslash \setBoundary$, $\setBoundary$, and $\setDepend$. $\setDepend$ Stencils involved in MPI communications have been permuted to the bottom of the matrix. The split in $\setDepend$ indicates communication with two neighboring partitions. }
\label{fig:decomp_spy}
\end{center}
\end{figure}


Figure~\ref{fig:stencilSets2CPU} illustrates a configuration with two 
CPUs and two GPUs, and 9 stencils, four on CPU1, and five on CPU2, separated
by a vertical line in the figure. Each stencil
has size $n=5$. In the top part of the figures, the stencils are laid out
with blue arrows pointing to stencil neighbors and creating the edges of a directed adjacency graph. Note that the connection between two nodes is not 
always bidirectional. For example, node 6 is in the stencil of node 3, but 
node 3 {\em is not\/} a member of the stencil of node 6. 
Gray arrows point to stencil neighbors outside the small window and are not relevant to the following discussion, which focuses only on data flow between 
CPU1 and CPU2. 
Since each CPU is responsible for the derivative evaluation and solution updates for any stencil center, it is clear that some nodes have a stencil with nodes that are on a different CPU. For example, node 8 on CPU1 has a stencil comprised of
nodes 4,5,6,9, and itself. The data associated with node 6 must be retrieved
from CPU2. Similarly, the data from node 5 must be sent to CPU2 to 
complete calculations at the center of node 6.


The set of all nodes that a CPU interacts with is denoted by $\setAllNodes$, which includes not only the nodes stored on the CPU, but the nodes required from other CPUs to complete the calculations. 
The set $\setCenters\in\setAllNodes$ contains the nodes at which the CPU 
will compute derivatives and apply solution updates. 
The set $\setDepend = \setAllNodes \backslash \setCenters$ 
is formed from the set of nodes whose values must be retrieved from another CPU. 
For each CPU, the set $\setProvide\in\setCenters$ is sent to other CPUs. The set $\setBoundary\in\setCenters$ consists of nodes that depend on  values from $\setDepend$ in order to evaluate derivatives. Note that $\setProvide$ and $\setBoundary$ can overlap, but differ in size, since the directed adjacency graph produced by stencil edges is not necessarily symmetric. The set $\setBoundary \backslash \setProvide$ represents nodes that depend on $\setDepend$ but are not sent to other CPUs, while $\setCenters \backslash \setBoundary$ are nodes that have no dependency on information from other CPUs.
The middle section Figure~\ref{fig:stencilSets2CPU} lists global node indices contained in $\setAllNodes$ for each CPU. Global indices are paired with local indices to indicate the node ordering internal to each CPU. The structure of set $\setAllNodes$,
   \begin{equation}
 		\setAllNodes = \{ \mathcal{Q}\backslash\mathcal{B} \ \ \mathcal{B}\backslash\mathcal{O} \ \ \mathcal{O} \ \ \setDepend \},
            \label{eqn:decompose_g}
        \end{equation}
 is designed to simplify both CPU-CPU and CPU-GPU memory transfers by grouping nodes of similar type. The color of the global and local indices in the figure
 indicate the sets to which they belong. They are as follows: white represents $\setCenters \backslash \setBoundary$, 
 yellow represents $\setBoundary \backslash \setProvide$, green indices 
 represent $\setProvide$, and red represent $\setDepend$.  


        \begin{table}[t]
            \begin{center}
                \begin{tabular}{l l}
                    \hline
                    $\setAllNodes$ &: all nodes received and contained on the CPU/GPU $g$ \\
                    $\setCenters$ &: stencil centers managed by $g$ 
					(equivalently, stencils computed by $g$) \\
                    $\setBoundary$ &: stencil centers managed by $g$ that
                    require nodes from another CPU/GPU \\
                    $\setProvide$ &: nodes managed by $g$ that are sent to other CPUs/GPUs  \\
                    $\setDepend$ &: nodes required by $g$ that are managed by another CPU/GPU \\
                    \hline
                \end{tabular}
                \caption{Sets defined for stencil distribution to multiple CPUs}
                            \label{tbl:stencil_sets}
            \end{center}
        \end{table}

        \begin{figure}[ht] 
            \centering
            \includegraphics[width=3.5in]{../figures/paper1/figures/omnigraffle/SimpleExample.pdf} 
            \caption{Partitioning, index mappings and memory transfers for nine stencils ($n=5$) spanning two CPUs and two GPUs. Top: the directed graph created by stencil edges is partitioned for two CPUs. Middle: the partitioned stencil centers are reordered locally by each CPU to keep values sent to/received from other CPUs contiguous in memory. Bottom: to synchronize GPUs, CPUs must act as intermediaries for communication and global to local index translation. Middle and Bottom: color coding on indices indicates membership in sets from Table~\ref{tbl:stencil_sets}: $\setCenters \backslash \setBoundary$ is white, $\setBoundary \backslash \setProvide$ is yellow, $\setProvide$ is green and $\setDepend$ is red.
            }
            \label{fig:stencilSets2CPU}
        \end{figure}	

 The structure of $\setAllNodes$ offers two benefits: first, solution values in $\setDepend$ and $\setProvide$ are contiguous in memory and can be copied to or from the GPU without the filtering and/or re-ordering normally required in preparation for efficient data transfers. Second, asynchronous communication allows for the overlap of communication and computation. This will be considered as part of future research on algorithm optimization.  Distinguishing the set $\mathcal{B} \backslash \mathcal{O}$ allows the computation of $\mathcal{Q}\backslash \mathcal{B}$ while waiting on $\mathcal{R}$. 

\authnote{} The local index set is ordered as ${QmB, BmO, O, R}$ 

\authnote{} Domain boundary nodes appear at beginning of the list 

Figure~\ref{fig:decomposed_sphere} illustrates a partitioning of
$N=10,201$ nodes on the unit sphere onto four CPUs. Each partition, illustrated
as a unique color, represents set $\setAllNodes$ for a single CPU.  Alternating
representations between node points and interpolated surfaces illustrates the
overlap regions where nodes in sets $\mathcal{O}$ and $\mathcal{R}$ (i.e., nodes
requiring MPI communication) reside. As stencil size increases, the width of the
overlap regions relative to total number of nodes on the sphere also increases. 




When targeting the GPU, communication of solution or intermediate values is a four step process:
        \begin{enumerate}
    \item Transfer $\mathcal{O}$ from GPU to CPU
	\item Distribute $\mathcal{O}$ to other CPUs, receive $R$ from other CPUs
	\item Transfer $\mathcal{R}$ to the GPU
	\item Launch a GPU kernel to operate on $\mathcal{Q}$
     \end{enumerate} 
The data transfers involved in this process are illustrated at the bottom of Figure~\ref{fig:stencilSets2CPU}.
    Each GPU operates on the local indices ordered 
according to Equation~(\ref{eqn:decompose_g}). The set 
$\setProvide$ is copied off the GPU and into CPU memory as one contiguous memory block. The CPU then maps local to global indices and transfers $\setProvide$ to other CPUs. CPUs send only the subset of node values from $\setProvide$ that is required by the destination processors, but it is important to note that node information might be sent to several destinations. 
As the set $\setDepend$ is received, the CPU converts back from global to local indices before copying a contiguous block of memory to the GPU. \authnote{remember distributed case: no decode}

        This approach is scalable to a very large number of 
		processors, since the individual processors do not require the 
		full mapping between RBF nodes and CPUs. 
		
		By scalable here we imply total problem size and processor count. The performance scalability of the code depends on the problem size and the MPI collective. In Figure~\ref{fig:strong_scaling} the strong scaling of $N=10^6$ nodes is tested on Itasca, a supercomputer at the Minnesota Supercomputing Institute.   



\section{Test Case}

To test and demonstrate scaling of our method, we consider an idealized regular grid in three dimensions. 

verification here is only significant to ensure we are applying all weights. 
We apply weights to calculate derivatives of a test function in X, Y, Z, and the Laplacian. 
the grid is regular and 3D.
We test strong scaling on a $N=160^3$ grid, and weak scaling with $N_p=4000$. This way at $p=1024$ processes we have weak scaling testing the full $N=160^3$ grid. 





\section{Communication Collectives} 

MPI collectives allow information sharing between processes. Our code leverages three collectives: MPI\_Alltoall, MPI\_Alltoallv and MPI\_Isend/MPI\_Irecv. 

The collective operation is essentially transposing information as seen in Figure~\ref{fig:mpi_alltoallv_visual}. 

MPI\_Alltoall requires that all processors send and receive an equivalent number of bytes to one another. Since the size must be equivalent for all processors, the send and receive buffers are padded to the maximum message size for any one connection between processors. MPI\_Alltoallv reduces the number of bytes sent and received by allowing processors to specify variable message sizes when communicating. For a small number of processors the variable message size will function well. However, MPI\_Alltoallv requires all processes to connect with every other process, even in the event that 0 bytes are to be sent. Based on the grid decomposition, processors compute on contiguous partitions with a small number of neighboring partitions. By replacing the MPI\_Alltoallv with a MPI\_Isend/MPI\_Irecv combination, the number of collective connections are truncated such that processors only connect to and communicate with essential neighbors that need/provide data. 

The actual implementation of MPI\_Alltoall and MPI\_Alltoallv likely use Isend and Irecv internally. 

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=9cm]{../figures/omnigraffle/MPI_Alltoall_Visual.png}
\caption{The MPI\_Alltoall collective allows processors to interchange/transpose data by passing an equivalent number of bytes to every other processor.}
\label{fig:mpi_alltoall_visual}
\end{center}
\end{figure}
\begin{figure}[ht!]
\begin{center}
\includegraphics[width=9cm]{../figures/omnigraffle/MPI_Alltoallv_Visual.png}
\caption{The MPI\_Alltoallv collective compresses the interchange from MPI\_Alltoall by allowing for variable message sizes between all processors. Assume message sizes are proportional to square size in figure. When packet sizes are null MPI\_Alltoallv has undefined behavior.}
\label{fig:mpi_alltoallv_visual}
\end{center}

Figure~\ref{fig:mpi_alltoallv_visual}: \url{http://www.mcs.anl.gov/papers/P1699.pdf} observes that the zero-byte messages still add up due to processing required to analyze send-buffer and determine when connection is required. 


\end{figure}
\begin{figure}[ht!]
\begin{center}
\includegraphics[width=9cm]{../figures/omnigraffle/MPI_IsendIrecv_Visual.png}
\caption{The ``subset-to-subset" (MPI\_Isend/MPI\_Irecv) collective allows for variable message sizes, and truncates the number of connections between processors to only required connections.}
\label{fig:mpi_isendirecv_visual}
\end{center}
\end{figure}

MPI\_Isend/MPI\_Irecv also allows for overlapping communication and computation by posting receives early 

\subsection{Alltoallv}
As a baseline for scaling we start with MPI\_Alltoallv. 

\authnote{figure: alltoall visual} 

\subsection{Isend/Irecv}

The first improvement on Alltoallv collectives is to truncate the number of connections made between processes. Compact stencils implies an overlap region for each processor that draws values from a limited number of neighboring processors. 

\authnote{figure: isend visual}

\subsection{No Decode}
\authnote{figure: per iteration stacked bar for n=50 and 16 processes to show cost of decode}
\authnote{figure: algorithm for collective}

\authnote{figure: alltoall to isend improvement. justify comm\_combo for up to 16 procs.} 
\authnote{figure: comm\_combo gains}
\authnote{figure: algorithm for collective}

\subsection{Immediate Isend on Encode}
\authnote{figure: algorithm for collective}

\authnote{back to section: figure: improvement on all CPU collectives (n=50)}

\authnote{table: show percentage of comm time for actual mpi time. busy network can cause slower comm times. but the decode cost is gone. it can also be an issue if we have saturated comm pipes}



\section{CPU Scaling}

\authnote{Show the strong and weak scaling here}

To demonstrate the effectiveness of our decomposition and indexing, we perform scaling experiments. 

\subsection{Strong Scaling}
Strong scaling tests the growth in time for a fixed total problem size, and a variable number of processors. 

\subsection{Weak Scaling}
Weak scaling considers the amount of time for a fixed problem size per process and variable number of processors. That is to say, each processor has roughly the same amount of work, so as we scale to a large number of processors, changes in time will be the result of increased communication overhead. 

Although our weak scaling results are promising, they also contain a problem. First, since we are subsampling a $160^3$ regular grid to get the first $N=p*4000$ nodes, many of the tests consider domains that are ``L" shaped and have odd partitions with limited connectivity.

\authnote{Here and strong scaling: table showing the min and max Osize,Rsize}


\subsection{Bandwidth}

To understand the impact of MPI on these benchmarks we calculate the average and aggregate collective bandwidths. The average bandwidth considers the MPI throughput from the perspective of one processor. 

The aggregate bandwidth reveals when processes saturate the interconnects. 

We consider a simple idealized problem where derivatives are computed over a regular grid generated in 3-D. The experiment computes the SpMV one thousand times. At the end of each SpMV the MPI\_Alltoallv collective is used to synchronize the local derivative vectors. After one thousand iterations, each process computes the local norm of the resulting vector and an MPI\_Reduce collective dra


\begin{figure} 
\centering
\includegraphics[width=5cm]{performance_content/scaling/strong_scaling_4M_regular_alltoallv.png}  
\includegraphics[width=5cm]{performance_content/scaling/strong_scaling_4M_regular_alltoallv_speedup.png} \\
\includegraphics[width=5cm]{performance_content/scaling/strong_scaling_4M_regular_spmvOnly.png}
\includegraphics[width=5cm]{performance_content/scaling/strong_scaling_4M_regular_alltoallv_commOnly.png} 
\caption{Strong scaling the distributed SpMV for $N=4096000$ nodes (i.e., a $160^3$ regular grid) and various stencil sizes. Here the MPI\_Alltoallv collective operation is used. (Left) Strong scaling of SpMV (including cost of communication). (Center) Strong scaling of computation only. (Right) Strong scaling of communication only.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=5cm]{performance_content/scaling/strong_scaling_4M_regular_n17comparison_commOnly.png}
\caption{Scaling comparison of MPI\_Alltoallv and two types of MPI\_Isend/MPI\_Irecv collectives: one with MPI\_Irecv issued after filling the MPI\_Isend send buffer (post-fill), and the other issued before filling the MPI\_Isend buffer (pre-fill).}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=5cm]{performance_content/scaling/weak_scaling_np4000_regular_alltoallv_commOnly.png}
\includegraphics[width=5cm]{performance_content/scaling/weak_scaling_np4000_regular_alltoallv.png}  
\includegraphics[width=5cm]{performance_content/scaling/weak_scaling_np4000_regular_n17_compare_commOnly.png} \\
\includegraphics[width=5cm]{performance_content/scaling/weak_scaling_np4000_regular_spmvOnly.png} 
\includegraphics[width=5cm]{performance_content/scaling/weak_scaling_np4000_regular_n17_compare.png}
\caption{Weak scaling of the SpMV}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=10cm]{performance_content/scaling/strong_scaling_4M_regular_isend.png}  
\includegraphics[width=10cm]{performance_content/scaling/weak_scaling_np4000_regular_isend.png}  
\caption{Scaling of SpMV with MPI\_Isend/MPI\_Irecv}
\end{figure}


\chapter{Distributed GPU SpMV}
\label{chap:multigpu_rbffd}

Distributing SpMV across multiple GPUs poses a new problem: as previous mentioned, the data sent and received via MPI collectives must be copied from device to host and vice-versa. To amortize this cost we introduce a novel overlapping algorithm to hide the cost of communication behind the cost of a concurrent SpMV on the GPU. 

\section{Overlapped Queues}

\section{Avoiding Copy Out}

\subsection{Avoiding Copy-Out on CPU}

\section{Scaling}
We scale the SpMV across the GPUs on Cascade.

\subsection{Fermi}
\subsection{Kepler}

\subsection{Shared K20s}


\ifstandalone
\bibliographystyle{plain}
\bibliography{merged_references}
\end{document}
\else
\expandafter\endinput
\fi

