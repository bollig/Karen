\makeatletter
\@ifundefined{standalonetrue}{\newif\ifstandalone}{}
\@ifundefined{section}{\standalonetrue}{\standalonefalse}
\makeatother
\ifstandalone
\documentclass{report}

\input{all_usepackages} 
\usepackage[margin=1.25in]{geometry}

\begin{document}
\fi

\chapter{GPU SpMV}
\label{chap:gpu_rbffd}

General Purpose GPU (GPGPU) computing is one of today's hottest trends in scientific computing. As problem sizes grow larger, it behooves us to work on parallel architectures, be it across multiple CPUs or one or more GPUs. With over 1 TFLOP/s peak possible throughput in double precision on a single GPU \cite{KeplerFactSheet}, a compelling argument is made for the latter. 

In the last decade, CPU designers have been forced to transition to multi- and many-core chips in order to keep up with demand for continued growth in computational performance. 
%TODO: Moore's law states that 
Power constraints make further frequency scaling on CPUs almost impossible, which leaves increased core counts as the only viable solution for designers to continue satisfying Moore's Law \cite{Owens2007}. 


GPUs originated as dedicated hardware for video games and computer graphics. 
Due to the inherent task parallelism in computer graphics  (e.g., for rasterization), early GPU designers adopted a many-core strategy for hardware, which persists today. Computer graphics traditionally involved simple operations, so the majority of transistors on a GPU are dedicated to computation in vector pipelines with limited control for logic. The earliest GPUs had static rendering pipelines with fixed functions. In contrast to this, CPUs have always focused on fast serial and scalar operations. This means they allocate less transistors to computation in order to free space for the needs of advanced logic tasks like branching, caching, etc. \cite{Owens2007,CudaGuide2013}. %Figure~\ref{fig:gpu-devotes-more-transistors-to-data-processing} (Courtesy of NVidia, \cite{CudaGuide2013}) illustrates the difference between CPUs and GPUs at a high level. The high density of Arithmetic Logic Units (ALUs) with simple control and caching units is signature design feature of GPUs. 

Thanks to the highly profitable and always demanding game industry, the static rendering pipeline of yore was molded into a fully programmable and dynamic execution platform with a SIMD-like flavor. Prior to 2006, the GPU had already evolved away from fixed-function pipelines and into limited programmable functionality with control over a subset of phases in the rendering process. At the time researchers were able---with substantial effort---to trick the GPU into solving scientific problems by encoding solutions within the graphics rendering process (see e.g., \cite{Trendall2000,Jansen2007,Harris2005,Owens2007}).
GPUs, miles ahead in the many-core arena, only required a bit of flexibility in control and logic to compete against the CPU for general purpose computing.  

The release of NVidia's CUDA architecture and software stack \cite{CudaGuide2013} at the end of 2006, reduced substantially the need for tricks as developers could suddenly target the GPU with the ease of writing a generic C-like program. The monumental new hardware bypassed the graphics rendering pipeline and allowed programs to execute as though the GPU were just another compute device. The CUDA software stack included NVidia's compiler, \emph{nvcc}, for developers to compile custom GPU codes. Since day one, developers who wanted to quickly transition existing code-bases to the GPU had access to the CUBLAS API, an implementation of the Basic Linear Algebra Subprograms (BLAS) suite on the GPU. CUFFT, a CUDA implementation of the Fast Fourier Transform (FFT), was also provided with the same justification. In many ways NVidia ensured that GPGPU computing was easily accessible to the general public. 


The success of GPGPU is largely due to the incredible gap between compute capabilities of the GPU versus a CPU. Individual cores on a GPU operate at a lower clock frequency than CPU cores, but the sheer number of them leads to an astounding level of throughput in terms of floating point operations per second (FLOPs). Figure~\ref{fig:floating-point-operations-per-second} (from \cite{CudaGuide2013}) illustrates the historical growth in terms of billions of FLOP/s (GFLOP/sec) achieved by NVidia GPUs versus Intel CPUs. Over the last decade the peak GFLOP/sec for a single GPU has sky-rocketed and consistently leads CPUs by a full order of magnitude. To supply much needed data to parallel cores, GPUs have also evolved a memory hierarchy with bandwidth up to five times higher than CPUs \cite{CudaGuide2013}. %Figure~\ref{fig:memory-bandwidth} (courtesy of NVidia; \cite{CudaGuide2013}) compares the evolving memory bandwidth for NVidia and Intel. 

%TODO: update with latest GFLOPs (up to TFLOPs now)
\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{../figures/prospectus/GPU_Evolution_opencl.pdf}
\caption{Performance comparison for Graphics Processing Units (GPUs). GFLOP/sec data based on \cite{CudaGuide2013}.}
\label{fig:floating-point-operations-per-second}
\end{figure}

In the years since CUDA was released, a vast number of libraries and language extensions have surfaced to leverage the GPU architecture for scientific computing.  



%TODO verify date of open source
In 2012 NVidia open sourced CUDA to lower level virtual machines. This opened the possibility of directly compiling other languages like Python (Anaconda) to build kernels directly within the Python language for CUDA. 

GPUs currently support single and double precision. RBF-FD certainly requires double precision at the moment. Solving for the weights in single precision might work, but the lower precision would not be able to handle the ill-conditioned system. Projecting double precision weights into single precision results in weights that sum to $\approx 10^{-4}$, a poor approximation to zero. 
%
%the addition of a new software layer that finally made GPGPU accessible to the general public. The CUDA API includes routines for memory control, interoperability with graphics contexts (i.e., 
%OpenGL programs), and provides GPU implementation subsets of BLAS and FFTW libraries \cite{CudaGuide2013}. After the undeniable success of CUDA for C, new projects emerged to encourage GPU programming in languages like FORTRAN (see e.g., HMPP \cite{HMPP2009} and Portland Group Inc.'s CUDA-FORTRAN \cite{CudaFortran2009}). 
%
%This transition was 
%followed closely by evolving programming languages. Today, GPUs can be leveraged from C/C++, FORTRAN, Java, Python, MATLAB, and more. The list seems endless, with new developments appearing every day. 



%\section{Test Environments}
%
%\begin{itemize}	
%\item Early testing was completed on the Keeneland GPU cluster. At the time Keeneland was in the Keeneland Initial Deployment System (KIDS) had 320 NVidia M2070 GPUs (Fermi). 240 CPUs. 6GB of memory. 
%%\item The Spear cluster at FSU has 16x NVIDIA Tesla m2050 GPUs. 3GB of memory. 
%\item The Cascade cluster at the Minnesota Supercomputing Institute (UMN) has 32x NVidia M2070 GPUs. 8 nodes, 12 cores each with four GPUs attached to each. 6GB of memory each. In addition, Cascade has 8 NVidia Kepler K20 GPUs. 
%\end{itemize}

\section{Managing Expectations for the GPU}
\begin{figure} 
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../figures/spmv/roofline_m2050_m2070-eps-converted-to.pdf}
\caption{Roofline Model for NVidia Fermi class GPUs, M2050 and M2070. SpMV Peak: 36 GFLOP/sec.}
\label{fig:roofline_m2070}
\end{subfigure}
\quad
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../figures/spmv/roofline_k20-eps-converted-to.pdf}
\caption{Roofline Model for NVidia Kepler K20. SpMV Peak: 52 GFLOP/sec.}
\label{fig:roofline_k20}
\end{subfigure}
\caption{Roofline models manage expectations for maximum possible GFLOP/sec based on the Operational Intensity of kernels. The SpMV needed by RBF-FD has an operational intensity of $0.25$. }
\end{figure}


Success stories abound on the GPU citing massive gains in performance. However, many like \cite{Phillips2009} appear to be impossible

RBF-FD is embarrassingly parallel with stencils applied independently. Unfortunately, maximum concurrency does not always imply perfect performance. %TODO: (SHOW RBF-FD benchmarks for Vortex and Cosine as an example of 1x and 2x SpMV on GPU; state that the generation GPU they were run on has a peak of xxx which means performance was xxx\% (dismal).).

On the Fermi GPUs (Figure~\ref{fig:roofline_m2070}) at 144 GB/s you need an OI of 4 to reach the peak of 515 GFLOP/sec per card. On the K20 (Figure~\ref{fig:roofline_k20}) you need 6 to hit the peak 1.17 TFLOP/s (208 GB/s). But with an OI of 1/16 you could get 9 GFLOP/sec on the Fermi and 13 on the K20 (compared to the 2 GFLOP/sec on Itasca). 

%
%\begin{verbatim}
%"__kernel void vec_mul(\n"
%"          __global const unsigned int * row_indices,\n"
%"          __global const unsigned int * column_indices, \n"
%"          __global const float * elements,\n"
%"          __global const float * vector,  \n"
%"          __global float * result,\n"
%"          unsigned int size) \n"
%"{ \n"
%"  for (unsigned int row = get_global_id(0); row < size; row += get_global_size(0))\n"
%"  {\n"
%"    float dot_prod = 0.0f;\n"
%"    unsigned int row_end = row_indices[row+1];\n"
%"    for (unsigned int i = row_indices[row]; i < row_end; ++i)\n"
%"      dot_prod += elements[i] * vector[column_indices[i]];\n"
%"    result[row] = dot_prod;\n"
%"  }\n"
%"}\n"
%\end{verbatim}
%
%\begin{verbatim}
%"__kernel void vec_mul(\n"
%"    const __global int* coords,\n"
%"    const __global float* elements,\n"
%"    const __global const float * vector,\n"
%"    __global float * result,\n"
%"    const unsigned int row_num,\n"
%"    const unsigned int col_num,\n"
%"    const unsigned int internal_row_num,\n"
%"    const unsigned int items_per_row,\n"
%"    const unsigned int aligned_items_per_row\n"
%"    )\n"
%"{\n"
%"    uint glb_id = get_global_id(0);\n"
%"    uint glb_sz = get_global_size(0);\n"
%"    for(uint row_id = glb_id; row_id < row_num; row_id += glb_sz)\n"
%"    {\n"
%"        float sum = 0;\n"
%"        \n"
%"        uint offset = row_id;\n"
%"        for(uint item_id = 0; item_id < items_per_row; item_id++, offset += internal_row_num)\n"
%"        {\n"
%"            float val = elements[offset];\n"
%"            if(val != 0.0f)\n"
%"            {\n"
%"                int col = coords[offset];    \n"
%"                sum += (vector[col] * val);\n"
%"            }\n"
%"            \n"
%"        }\n"
%"        result[row_id] = sum;\n"
%"    }\n"
%"}\n"
%\end{verbatim}



%TODO: competition in the market led to similar architectures from vendors like AMD and ever-expanding CPUs

CUDA is not the only solution for GPU computing. 

%TODO: in 2009 enter OpenCL as a standard as a parallel language on top of all those hardware

In early 2009, the Khronos Group announced a new specification for a general 
parallel programming lanugage referred to as the Open Compute Language (OpenCL) \cite{OpenCL2009}. Similar in design to the CUDA language---in many ways it is a simple refactoring of the predecessor---the goal of OpenCL is to provide a mid-to-low level API and language to control any multi- or many-core processor in a uniform fashion. Today, OpenCL drivers exist for a variety of hardware including NVidia GPUs, AMD/ATI CPUs and GPUs, and Intel CPUs. 

This \textit{functional portability} is the cornerstone of the OpenCL language. However, functional portability does not imply performance portability. That is, OpenCL allows developers to write kernels capable of running on all types of target hardware, but optimizing kernels for one type of target (e.g., NVidia GPUs) does not guarantee that the kernel will run efficiently on another target (e.g., CPU).

With CPUs tending towards many cores, and the once special purpose, many-core GPUs offering general purpose functionality, it is already possible to see the CPU and GPU converging into general purpose many-core architectures. Already, ATI has introduced the Fusion APU (Accelerated Processing Unit) which couples an AMD CPU and ATI GPU within a single die. OpenCL is an attempt to standardize programming ahead of this intersection. 

%Home computers, smart-phones and other devices containing many- or multi-core compute units internally have driven the generalization of the accelerator language to provide a unified approach to targeting any available hardware. 

\section{Sparse Matrix Libraries}

One thing missing from the original release of CUDA was sparse matrix algebra support. 
This led to investigations like \cite{Bell2009,SuKeutzer2012} and resulted in third party libraries like CUSP \cite{Cusp2012} and ViennaCL \cite{Rupp2010,Rupp2010a}, which are sparse matrix libraries developed for the GPU. The APIs of CUSP and ViennaCL are used like 

Algorithms from \cite{Bell2009} have since been included in the CUDA SDK as the cuSPARSE library. 

Also, ViennaCL provides seamless interoperability with the Boost::UBLAS, EIGEN and MTL libraries via C++ templates. We test the performance of our algorithm on one or more CPUs with the Boost::UBLAS library. 

%TODO: get date
NVidia eventually included a library named CUSPARSE as part of the CUDA SDK. Initial releases were 
Developers either implemented their own kernels (as we did in \cite{BolligFlyerErlebacher2012}) or   


%TODO: CUDA MPI not a feature provided by OpenCL. 

Performance of SpMV on GPU will not be optimal. Often one finds in literature examples of applications that have been accelerated by huge factors. In fact, speedup factors greater than 10x should never be possible for SpMV. In order to manage expectations of how much faster tasks will operate on the GPU we can turn to a \emph{roofline model} \cite{Williams2009}. The roofline model 
%TODO: GPU memory layout
%TODO: GPU multi-processor units

%TODO: GPU software (OpenCL vs CUDA)
%TODO: targeting the GPU for SpMV (how to write a kernel as forloop without the for)

 
%TODO: list all known work on SpMV for GPU
%TODO: emphasize that the GPU SpMV has been studied since before 2006.
%TODO: SpMV condenses sparse matrix to dense form. 
%TODO: each of the dense forms are unique (describe each)
%TODO: newer forms are available




%TODO: ALL SPMV related work.
\cite{Bell2009} 
%TODO: \cite{Kreuzer2012} in distributed GPU. 
\cite{Vuduc2005} etc. 




\section{Performance}
\subsection{GFLOP/sec}
In order to quantify the performance of our implementation, we can measure two
factors. First, we can check the speedup achieved on the GPU relative to the
CPU to get an idea of how much return of investment is to be expected by all
the effort in porting the application to the GPU. Speedup is measured as the
time to execute on the CPU divided by the time to execute on the GPU. 

The second quantification is to check the throughput of the process. By
quantifying the GFLOP throughput we have a measure that tells us two things:
first, a concrete number quantifying the amount of work performed per second by
either hardware, and second because we can calculate the peak throughput possible on
each hardware, we also have a measure of how occupied our CPU/GPU units are.
With the GFLOP/sec we can also determine the cost per watt for computation and
conclude on what problem sizes the GPU is cost effective to target and use. 

Now, as we parallelize across multiple GPUs, these same numbers can come into
play. However we are also interested in the efficiency. Efficiency is the
speedup divided by the number of processors. With efficiency we have a measure
of how well-utilized processors are as we scale either the problem size (weak)
or the number of processors (strong). As the efficiency diminishes we can
conclude on how many stencils/nodes per processor will keep our processors
occupied balanced with the shortest compute time possible (i.e., we are
maximizing return of investment). 

\subsection{Expectations in Performance}
Many GPU applications claim a 50x or higher speedup. This will never be the case for RBF-FD for the simple reason that the method reduces to an SpMV. The SpMV is a low computational complexity operation with only two operations for every one memory load. 



\section{Targeting the GPU}

\subsection{OpenCL}
In our initial implementation, published in \cite{BolligFlyerErlebacher2012}, all bets were hedged in favor of OpenCL as the future of GPU computing. Custom kernels were developed to apply RBF-FD weights in the equivalent of a CSR SpMV. 

OpenCL is chosen with the future in mind. Hardware changes rapidly and vendors often leapfrog one another in the performance race. By selecting OpenCL, we hedged our bets on the functional portability. 

Apple, Intel, AMD, NVidia all support OpenCL.  Unfortunately Apple overrides vendor specific drivers and offers its own driver without support for concurrent kernel execution. 

One serious limitation exists with the choice of OpenCL. Since the language is intended to function across a slew of platforms, the language itself is limited to a subset of hardware features common to the different vendors. Features like 2-D and 3-D textures, which are part of the CUDA standard, are vendor provided extensions in OpenCL. 

We leverage the OpenCL language for functional portability. 

Our dedication to OpenCL is a hedged bet that the future architectures will merge in the middle between many and multi-core architectures with co-processors alongside CPUs. By selecting an open standard parallel programming language, we increase the likelihood for future support of our programs. 


While the nomenclature used in this paper is typically associated with CUDA programming, the names \textit{thread} and \textit{warp} are used to clearly illustrate kernel execution in context of the NVidia specific hardware used in tests. OpenCL assumes a lowest common denominator of hardware capabilities to provide functional portability. However, intimate knowledge of hardware allows for better understanding of performance and optimization on a target architecture. For example, OpenCL assumes all target architectures are capable at some level of SIMD (Single Instruction Multiple Data) execution, but CUDA architectures allow for Single Instruction Multiple Thread (SIMT). SIMT is similar to traditional SIMD, but while SIMD immediately serializes on divergent operations, SIMT allows for a limited amount of divergence without serialization. 

At the hardware level, a \textit{thread} executes instructions on the GPU. On Fermi level GPUs, groups of 32 threads are referred to as \textit{warps}. A warp is the unit of threads executed concurrently on a single \textit{multi-processor}. 
In OpenCL (i.e., software), a collection of hardware threads performing the same instructions are referred to as a \textit{work-group} of \textit{work-items}. Work-groups execute as a collection of warps constrained to the same multiprocessor. Multiple work-groups of matching dimension are grouped into an \textit{NDRange}. Figure~\ref{fig:grid-of-thread-blocks} shows how threads/work-items are grouped into NDRanges. The \textit{kernel} provides a master set of instructions
for all threads in an NDRange \cite{OpenCL2009}. 


NVidia GPUs have a tiered memory hierarchy related to the grouping of threads described above. 
In multiprocessors, each computing core executes a thread with a limited set of registers. The number of registers varies with the generation of hardware, but always come in small quantities (e.g., 32K shared by all threads of a multiprocessor on the Fermi). Accessing registers is free, but keeping data in registers requires an effort to maintain balance between kernel complexity and the number of threads per block. Threads of a single work-group can share information within a multiprocessor through \textit{shared memory}. With only 48 KB 
available per 
multiprocessor \cite{CudaGuide2011}, shared memory is another fast but limited resource on the GPU. OpenCL refers to shared memory as \textit{local memory}. 
Sharing information across multiprocessors is possible in \textit{global device memory}---the largest and slowest memory space on the GPU. To improve seek times into global memory, Fermi level architectures include L1 on each multiprocessor and a shared L2 cache for all multiprocessors.


%\textit{constant} and \textit{texture} memory caches are available for read-only access. 
%To use shared memory, 
%Threads must 
%explicitly copy intermediate computational results or data in and out. 

%Shared memory is divided evenly into 16 \textit{memory 
%banks} or 
%regions of memory that can be accessed simultaneously. Memory and registers can be accessed equally fast as long as no two 
%threads in the 
%same half-warp (i.e., first 16 threads or last 16 threads of the warp) are accessing the same memory bank. If this occurs, the 
%access is 
%considered to be a \textit{bank conflict} and the multiprocessor is forced to serialize memory access, increasing the delay until 
%the warp can 
%proceed with execution \cite{CudaGuide:2008}. 

%\begin{figure}[t] %  figure placement: here, top, bottom, or page
%   \centering
%   \includegraphics[width=4.5in]{../figures/paper1/figures/opencl_memory_model.pdf} 
%   \caption{Comparison of GPU and CPU implementations underlying the OpenCL memory model for a single compute device.}
%   \label{fig:opencl_memory}
%\end{figure}
%


% and the place to which all data and kernels from the 
%CPU are copied for 
%execution. Note 
%that multiprocessors maintain read-only \textit{caches} for \textit{constants} and \textit{textures}---constants are values that 
%never change during kernel execution, 
%while textures are interpreted as read only arrays and provide a sophisticated cache mechanisms for certain types of non-sequential access. The real memory for constants and textures is part of the global device 
%memory, so reading 
%from texture and constant memory is faster than reads from global memory when no cache misses \cite{CudaGuide:2008}. 
%Also, if a kernel 
%uses too many registers per thread, \textit{local memory} is reserved in the global device memory to temporarily store register 
%contents for 
%later use with an associated penalty in efficiency \cite{CudaGuide:2008}. 

Under OpenCL, the equivalent to CUDA local memory is called \textit{private memory} and is also reserved read-only per work-item in global device memory. CUDA's shared memory is labeled \textit{local memory} and shared by threads of a work-group. The CUDA texture and constant caches are known as \textit{Global/Constant Cache}, and similar to CUDA, exist outside of developer control. OpenCL refers to \textit{global memory} and \textit{constant memory} when referring to read/write and read-only sections of CUDA's global device memory (respectively). Finally, CUDA textures are equivalent to OpenCL \textit{buffers} (1-D arrays) or \textit{images} (2-D/3-D arrays) \cite{OpenCL2009}.

%Based on \cite{Behr2009, OpenCL2009}, Figure~\ref{fig:opencl_memory} compares the underlying implementation of the OpenCL memory hierarchy on GPUs and CPUs. Notably, on current CPUs most of the memory is reserved in System RAM, whereas the hierarchy naturally fits on the deep memory hierarchy of the GPU. 



\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{../figures/prospectus/opencl_execute_model.pdf}
\caption{The OpenCL execution model executes a kernel in parallel across many work-items. A collection of work-items execute in SIMD as a work-group. The work-groups are further bundled into an NDRange representing the full set of tasks executed within a kernel. } 
\label{fig:grid-of-thread-blocks}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{../figures/prospectus/opencl_memory_model.pdf}
\caption{The OpenCL memory model provides a hierarchy of increasingly faster memories, each with smaller size and more limited scope. A single GPU contains multiple multiprocessors and a large global memory shared by all multiprocessors. Within each multiprocessor, the many cores/threads execute work-items and have access to local and cache memory shared with other work-items of the same multiprocessor. Individual work-items also private memory independent from other work-items. The OpenCL memory model generically abstracts the memory architectures for both CPUs and GPUs.} 
\label{fig:opencl_memory_model}
\end{figure}


\subsection{OpenCL vs CUDA}
The market is volatile. Companies survive by investing margins in their next great product. If a product fails or the company faces a recall, their survival may come into question. Thus far, NVidia's CUDA has been wildly popular, but for the longest time (until May 2012) it was closed source. The closed source limited the language to NVidia hardware. As such, the OpenCL language gained popularity due to its support for AMD, Intel, mobile devices, web browsers, etc. NVidia's push to provide an open source compiler may be an attempt to regain the market share, but OpenCL appears to be on good footing. One other point: with an open source NVidia compiler, OpenCL can be optimized by the more mature NVidia compiler for their proprietary hardware. OpenCL compilers are also becoming more sophisticated at auto-optimization. 

\subsection{Asynchronous Queuing} 
Provide details and simple example of how asynchronous queueing can be used. 

Need a figure showing the overlapping comm and comp in a general process with the wait points marked. 

The two level parallelization can even be extended to three level parallelism
with pThreads or OpenMP (see code samples attached to \cite{CudaToolkitDoc}). While OpenCL provides
the means to target parallelism on either multi-core CPUs or many-core GPUs, it
does not allow a parallel kernel on one hardware interact with a parallel kernel
on the another. That is to say, an OpenCL kernel on the CPU cannot launch
kernels on the GPU.  


\subsection{Custom Kernels}
\label{sec:custom_gpu_kernels}

From the definition of RBF-FD we can formulate the problem computationally in two ways. First, stencil operations are independent. Therefore, we can write kernels with perfect parallelism by dedicating a single thread per stencil or a group of threads per stencil.  

Unfortunately, perfect concurrency does not imply perfect or even ideal concurrency on the GPU. 

Since our focus within this work is to lay the foundation for parallel computing
with RBF-FD, we have made several simplifying assumptions in our code design.
Libraries like PETsc, Hypre, Trilinos and Deal.ii distribute sparse matrix
operations in similar fashion to our approach. When work initially began on this
dissertation, none of these competing libraries contained support for the GPU.
PETsc is currently developing support for the GPU, but we have not had the
chance to consider it yet. 

Our codebase began as a prototype demonstrating the feasibility of RBF-FD
operating on the GPU. The initial code, published in \cite{BolligFlyerErlebacher2012}, was developed as $N$ independent dot
products of weights and solution values to approximate derivatives. Weights were
stored linearly in memory, with the solution values read randomly. On the GPU,
stencils were evaluated independently by threads, or shared by a warp of
threads. Operating on stencils in this way implied that GPU kernels were to be
hand written, tuned and optimized. 

Much later, our perspective evolved to see derivative approximation and
time-stepping as sparse matrix operations. This opened new possibilities for
optimization and allowed us to forego hand optimization and fine-tuning of GPU
kernels. With all of the effort put into optimizing sparse operations within
libraries like CUSP \cite{Bell2009}, ViennaCL \cite{Rupp2010} and even the
NVidia provided CUSPARSE \cite{CudaToolkitDoc}, formulating the problem in terms of
sparse matrix operations allows us to quickly prototype on the GPU and leverage
all of the optimizations available within the third party libraries. 


\subsection{Explicit Solvers}

%TODO: add explicit equation formula

Our initial implementation in \cite{BolligFlyerErlebacher2012} leverages the GPU for acceleration of the standard fourth order Runge-Kutta (RK4) scheme. Stencil weights are calculated once at the beginning of the simulation, and reused in every iteration. Subsequent runs load weights from disk. Ignoring code initialization, the cost of the algorithm is simply the explicit time advancement of the solution. 

Figure~\ref{fig:multi_GPU_flow} summarizes the time advancement steps for the multi-CPU/GPU implementation. The RK4 steps are: 
\begin{eqnarray} 
\mathbf{k}_1 &=& \Delta t f(t_n, \mathbf{u}_n) \nonumber \\
\mathbf{k}_2 &=& \Delta t f(t_n+\frac{1}{2}\Delta t, \mathbf{u}_n + \frac{1}{2}\mathbf{k}_1) \nonumber \\
\mathbf{k}_3 &=& \Delta t f(t_n+\frac{1}{2}\Delta t, \mathbf{u}_n + \frac{1}{2}\mathbf{k}_2)  \label{eqn:rk4}\\
\mathbf{k}_4 &=& \Delta t f(t_n+\Delta t, \mathbf{u}_n + \mathbf{k}_3) \nonumber \\
\mathbf{u}_{n+1} &=& \mathbf{u}_{n} + \frac{1}{6}(\mathbf{k}_1 + 2\mathbf{k}_2 + 2\mathbf{k}_3 +\mathbf{k}_4), \nonumber
\end{eqnarray}
where each equation has a corresponding kernel launch. To handle a variety of Runga-Kutta implementations, steps $\mathbf{k}_{1\rightarrow4}$ correspond to calls to the same kernel with different arguments. The evaluation kernel returns two output vectors: 
\begin{enumerate} 
\item $\mathbf{k}_i = \Delta t f(t_n + \alpha_{i} \Delta t, \mathbf{u}_n + \alpha_{i} \mathbf{k}_{i-1})$, for steps $i=1,2,3,4$, and
\item  $\mathbf{u}_n + \alpha_{i+1} \mathbf{k}_i$
\end{enumerate} 
We choose $\alpha_{i}=0, \frac{1}{2}, \frac{1}{2}, 1, 0$ and $\mathbf{k}_{0} = \mathbf{u}_n$. The second output for each $\mathbf{k}_{i=1,2,3}$ serves as input to the next evaluation, $\mathbf{k}_{i+1}$. In an effort to avoid an extra kernel launch---and corresponding memory loads---the SAXPY that produces the second output uses the same evaluation kernel. Both outputs are stored in global device memory. When the computation spans multiple GPUs, steps $\mathbf{k}_{1\rightarrow3}$ are each followed by a communication barrier to synchronize the subsets $\mathcal{O}$ and $\mathcal{R}$ of the second output (this includes copying the subsets between GPU and CPU). An additional synchronization occurs on the updated solution, $\mathbf{u}_{n+1}$, to ensure that all GPUs share a consistent view of the solution going into the next time-step.

\begin{figure}[t]
      \centering
       \includegraphics[width=5in]{../figures/paper1/figures/omnigraffle/RK4_multi_GPU_flow.pdf}
      \caption{Workflow for RK4 on multiple GPUs. }
      \label{fig:multi_GPU_flow}
\end{figure}


To evaluate $\mathbf{k}_{1\rightarrow4}$ in Equation~\ref{eqn:rk4}, the discretized operators from Equation~(\ref{eq:evaluation_with_hyperviscosity}) are applied using sparse matrix-vector multiplication. If the operator $D$ is composed of multiple derivatives, a differentiation matrix for each derivative is applied independently, including an additional multiplication for the discretized $H$ operator.
 On the GPU, the kernel parallelizes across rows of the DMs, so all derivatives for stencils are computed in one kernel call.


For the GPU, the OpenCL language \cite{OpenCL2009} assumes a lowest common denominator of hardware capabilities to provide functional portability. For example, all target architectures are assumed to support some level of SIMD (Single Instruction Multiple Data) execution for kernels. Multiple \textit{work-items} execute a kernel in parallel. 
A collection of work-items performing the same task is called a \textit{work-group}. While a user might think of work-groups as executing all work-items simultaneously, the work-items are divided at the hardware level into one or more SIMD \textit{warps}, which are executed by a single multiprocessor. On the family of Fermi GPUs, a warp is 32 work-items \cite{CudaGuide2011}. 
OpenCL assumes a tiered memory hierarchy that provides fast but small \textit{local memory} space that is shared within a work-group \cite{OpenCL2009}. Local memory on Fermi GPUs is 48 KB per multiprocessor \cite{CudaGuide2011}. The \textit{global device memory} allows sharing between work-groups and is the slowest but most abundant memory. 
In the GPU computing literature, the terms \textit{thread} and \textit{shared memory} are synonymous to \textit{work-item} and \textit{local memory} respectively, and are preferred below. 

%Although the primary focus of this paper is the implementation 
%and verification of the RBF-FD method across multiple CPUs and GPUs, 
%we have nonetheless tested two approaches to the computation of derivatives 
Our initial implementation of RBF-FD on multiple GPUs (\cite{BolligFlyerErlebacher2012}) tested two approaches to computation of derivatives. 
%assess the potential for further improvements in performance. 
In both cases, the stencil weights are stored in CSR format \cite{Bell2009}, 
a packed one-dimensional array in global memory with all the weights 
of a single stencil in consecutive memory addresses. Each operator is stored as an independent CSR matrix. The consecutive ordering on the weights implies that the solution vector %structured according to the ordering of set $\mathcal{G}$ 
is treated as random access. 

All the computation on the GPU is performed in 8-byte double precision. 




\subsubsection{Naive Approach: One thread per stencil}

In this first implementation, each thread computes 
the derivative at one stencil center  (Figure~\ref{fig:oneThreadPerStencil}). 
The advantage of this approach is trivial concurrency.  Since each stencil has the same number of neighbors, each derivative has an identical number of computations. As long as the number of stencils is a multiple of the warp size, there are no idle threads. Should the total number of stencils be less than a multiple of the warp size, the final warp would contain idle threads, but the impact on efficiency would be minimal assuming the stencil size is sufficiently large. 

Perfect concurrency from a logical point of view does not 
imply perfect efficiency in practice. 
Unfortunately, the naive approach 
is memory bound. When threads access weights in global memory, 
a full warp accesses a 128-byte segment in a single memory operation \cite{CudaGuide2011}.
Since each thread handles a single stencil, the various threads in a warp access data in very disparate areas of global memory, rather than the same segment. This leads to very large slowdowns as extra memory operations are added for each 128-byte segment that the threads of a warp must access.
However, with stencils sharing many common nodes, and the Fermi hardware providing caching, some weights in the unused portions of the segments might remain in cache long enough to hide the cost of so many additional memory loads. 



\begin{figure}[htbp]
      \centering
       \includegraphics[width=3in]{../figures/paper1/figures/omnigraffle/oneThreadPerStencil.pdf}
      \caption{Naive approach to sparse matrix-vector multiply. Each thread is responsible for the sparse vector dot product of weights and solution values for derivatives at a single stencil.  }
      \label{fig:oneThreadPerStencil}
\end{figure}


\subsubsection{Alternate Approach: One warp per stencil} 

\begin{figure}[htbp]
      \centering
       \includegraphics[width=3in]{../figures/paper1/figures/omnigraffle/oneWarpPerStencil.pdf}
      \caption{Alternative approach. A full warp (32 threads) collaborate to apply weights  and compute the derivative at a stencil center. }
      \label{fig:oneWarpPerStencil}
\end{figure}


An alternate approach, illustrated in Figure~\ref{fig:oneWarpPerStencil}, dedicates a full warp of threads to a single stencil. Here, 32 threads load the weights of a stencil and the corresponding elements of the solution vector. As the 32 threads each perform a subset of the dot product, their intermediate sums are accumulated in 32 elements of shared memory (one per thread).
Should  a stencil be larger than the warp size, the warp iterates over the stencil in increments of the warp size until the full dot product is complete. Finally, the first thread of the warp performs a sum reduction across the 32 (warp size)  intermediate sums stored in shared memory and writes the derivative value to global memory. 

By operating on a warp by warp basis, weights for a single stencil are loaded with a reduced number of memory accesses. Memory loads for the solution vector remain random access but see some benefit when solution values for a stencil are in a small neighborhood in the memory space. Proximity in memory can be controlled by node indexing (see Chapter~\ref{chap:stencils}). 

For stencil sizes smaller than 32, some threads in the warp always remain idle. Idle threads do not slow down the computation within a warp, but under-utilization of the GPU is not desirable. For small stencil sizes, caching on the Fermi can hide some of the cost of memory loads for the naive approach, with no idle threads, making it more efficient. The real strength of one warp per stencil is seen for large stencil sizes. 
As part of future work on optimization, we will consider a parallel reduction in shared memory, as well as assigning multiple stencils to a single warp for small  $n$. 





\subsection{ViennaCL} 

In the time since the publication of \cite{BolligFlyerErlebacher2012} custom OpenCL kernels have been dropped in favor of a library called ViennaCL. 

Sparse formats supported by ViennaCL are shown in Figure~\ref{fig:sparse_format}.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{gpu_content/omnigraffle/SparseStorage.pdf}
\caption{Sparse format example demonstrating a sparse matrix and corresponding storage data structures. ELL format assumes two non-zeros per row. }
\label{fig:sparse_format}
\end{figure}

Other formats exist such as the Hybrid ELL plus CSR (HYB) format \cite{Bell2009}, Diagonal (DIAG) and Jagged Diagonal (JAD) variants, and a number of variants on the ELL format that operate in blocks (BELL), slices (SELL), or both (SBELL) \cite{SuKeutzer2012}. The HYB format stores the bulk of a matrix in an ELL container and the excess non-zeros per row in a CSR format. HYB benefits from reduced storage and memory loads in the ELL, and maintains the generality of a CSR matrix. 

%\begin{figure}
%\centering
%\includegraphics[width=0.5\textwidth]{../figures/spmv/spmv_vcl_gflops-eps-converted-to.pdf}
%\caption{Single GPU ViennaCL SpMV throughput (GFLOP/sec) compared to Boost::uBLAS CSR. Stencil size $n=50$ for all cases.}
%\label{fig:spmv_vcl_gflops}
%\end{figure}

GFLOP/sec calculated as: 
\begin{align}
GFLOP/sec = ^{N * n * 2} /_{t_ms} * 10^{-9}. 
\end{align}



%TODO: paralution
Paralution is another library that provides sparse matrix containers, iterative solvers, preconditioners, etc. Paralution provides interfaces to plug into Deal.ii, OpenFOAM, and other packages, and runs on multi-core CPUs and GPUs. 


To further optimize RBF-FD on the GPU, we formulate the problem in terms of a Sparse Matrix-Vector Mulitply (SpMV). When we consider the problem in this light we generate a single Differentiation Matrix that can see two optimizations not possible with our stencil-based view: 
\begin{itemize} 
\item First, the sparse containers used in SpMV allow for their own unique optimizations to compress storage and leverage hardware cache.
\item Evaluation of multiple derivatives can be accumulated by association into one matrix operation. This reduces the total number of floating point operations required per iteration. 
\end{itemize}


%TODO: tell the story of ViennaCL. 
%TODO: what motivated the switch
The library includes a variety of sparse matrix formats, solvers, preconditioners, etc. Dense and sparse containers simplify targeting the GPU. 

At the onset of work with ViennaCL two things were apparent: 1) the library was incredibly powerful but limited to square matrices; and 2) the library had no support for distributed computing. Our implementation of RBF-FD (\cite{BolligRBFFDCode}) required rectangular matrices resulting from domain decomposition. The enhancements to the API have since been added to the main branch of ViennaCL. 


%TODO: CSR Bytes:Flop ratio: \url{http://arxiv.org/pdf/1101.0091v1.pdf}





%TODO: list example loops for sparse kernels


Due to the assumption that all stencils have equal size, the ELL format is preferred as the default. 
 

\section{Cascade}

The actual observed GFLOP/sec achieved on Cascade's M2070 GPUs with the ViennaCL package is shown in Figure~\ref{fig:gflops_cascade_m2070}. Figure~\ref{fig:gflops_cascade_k20} provides achieved GFLOP/sec for Cascade's K20 GPUs.  

\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n17.png}
\caption{Stencil size $n=17$}
%\label{fig:gflops_cascade_m2070_n17}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n31.png}
\caption{Stencil size $n=31$}
%\label{fig:gflops_cascade_m2070_n31}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n50.png}
\caption{Stencil size $n=50$}
%\label{fig:gflops_cascade_m2070_n50}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n101.png}
\caption{Stencil size $n=101$}
%\label{fig:gflops_cascade_m2070_n101}
\end{subfigure}
\caption{Performance comparison of CSR, COO and ELL matrix formats on Cascade's NVidia M2070 GPU and Intel Westmere CPU (Intel Xeon X5675).}
\label{fig:gflops_cascade_m2070}
\end{figure}


\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n17.png}
\caption{Stencil size $n=17$}
%\label{fig:gflops_cascade_k20_n17}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n31.png}
\caption{Stencil size $n=31$}
%\label{fig:gflops_cascade_k20_n31}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n50.png}
\caption{Stencil size $n=50$}
%\label{fig:gflops_cascade_k20_n50}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n101.png}
\caption{Stencil size $n=101$}
%\label{fig:gflops_cascade_k20_n101}
\end{subfigure}
\caption{Performance comparison of ViennaCL CSR, COO and ELL matrix formats on Cascade's NVidia K20 GPU versus the Boost::uBLAS CSR format on and Intel Sandy-Bridge CPU (Intel Xeon E5-2670).}
\label{fig:gflops_cascade_k20}
\end{figure}


Figures~\ref{fig:ell_gflops_cascade_m2070} and \ref{fig:ell_gflops_cascade_k20} summarize the best achieved GFLOP/sec (in all cases the ELL format) for each stencil size on the M2070's and K20's respectively. Figures~\ref{fig:ell_speedup_cascade_m2070} and \ref{fig:ell_speedup_cascade_k20} show the achieved speedup each GPU gained over their respective Westmere and Sandy-Bridge CPUs. In the case of the K20, Figure~\ref{fig:ell_speedup_cascade_k20} shows that the Sandy-Bridge architecture is closing the divide between CPUs and GPUs, although an order of magnitude difference in the GFLOP/sec still exists.

\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_cascade_m2070.png}
\caption{GFLOP/sec, NVidia M2070 GPU (Cascade)}
\label{fig:ell_gflops_cascade_m2070}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_cascade_k20.png}
\caption{GFLOP/sec, NVidia K20 GPU (Cascade)}
\label{fig:ell_gflops_cascade_k20}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_speedup_cascade_m2070.png}
\caption{Speedup, NVidia M2070 GPU (Cascade) vs. CSR format on Intel Westmere (Intel Xeon X5675)}
\label{fig:ell_speedup_cascade_m2070}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_speedup_cascade_k20.png}
\caption{Speedup, NVidia K20 GPU (Cascade) vs. CSR format on Sandy-Bridge (E5-2670)}
\label{fig:ell_speedup_cascade_k20}
\end{subfigure}
\caption{Comparison by stencil size for the ELL sparse matrix format.}
\end{figure}





\subsection{Intel Phi} 

As part of future work into accelerating RBF methods, investigations are underway into the new Intel Phi architecture. The variety of hardware available on Cascade will help us establish a clear argument in the choice of accelerator type and resolve the dilemma between choosing Phi vs GPU for our method. Since RBFs generalize other methods, our results should have broad reaching impact to answer similar questions for related methods.

%TODO: preliminary benchmarks on performance of ViennaCL SpMV. 

With the generalization of RBF-FD derivative computation formulated as a sparse matrix multiplication, we can 
% TODO: mention CUSP as alternative but concentrate on VCL
consider the various sparse formats provided by CUSP and ViennaCL. 

%TODO later: \item All stencils with non-uniform size
%TODO: What is the optimal choice of sparse container? How do the sparse containers compare in performance to each other, and to our custom kernels? What can we conclude? 


Figure~\ref{fig:gflops_cascade_intel_phi} is provided for comparison with Figures~\ref{fig:gflops_cascade_m2070} and \ref{fig:gflops_cascade_k20}. This stresses the point that although OpenCL offers functional portability, performance portability is still limited. 

\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n17.png}
\caption{Stencil size $n=17$}
%\label{fig:gflops_cascade_intel_phi_n17}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n31.png}
\caption{Stencil size $n=31$}
%\label{fig:gflops_cascade_intel_phi_n31}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n50.png}
\caption{Stencil size $n=50$}
%\label{fig:gflops_cascade_intel_phi_n50}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n101.png}
\caption{Stencil size $n=101$}
%\label{fig:gflops_cascade_intel_phi_n101}
\end{subfigure}
\caption{Performance comparison of ViennaCL CSR, COO and ELL matrix formats on Cascade's Intel Phi Accelerator, and the Boost::uBLAS CSR format on an Intel Sandy-Bridge CPU (Intel Xeon E5-2670). ViennaCL's kernels are optimized for NVidia GPUs, but function (albeit poorly) on the Intel Phi with the current release of Intel's OpenCL SDK (May, 2013). }
\label{fig:gflops_cascade_intel_phi}
\end{figure}




\section{Conclusions and Future Work}

Conclude: sparse containers allow increased efficiency compared to our custom kernels. The custom kernels compete with CSR and COO. 




We compare the performance of our custom kernel to ViennaCL kernels (ELL, CSR, COO), UBlas (CSR).


ViennaCL allows control of the number of work-items for each kernel. 

ViennaCL includes a set of auto-tuning options. %TODO: provide tuned parameters

%TODO: what is profile for each GPU type
%TODO: What is the significance of tuning on our problem $n=17, 31, 50, 101,$ etc. 
%TODO: experiment: SpMV on N=10^6, n = variable (5->105)
%TODO: idealized experiment: SpMV on N=10^6 regular grid with n=variable.








The latest version of OpenMP (v4.0) introduces pragmas for offloading computation to accelerators. These pragmas will function similarly to pragmas from PGI, OpenACC, and the Intel MIC. 


%
%
%Hardware architecture%•	Memory layout%•	Processing cores%•	Trends in hardware since 2006 (additions and benfits)%Optimization%•	SpMV memory layout%•	Scheduling threads%•	Reductions%OpenCL%•	Why? %o	Cross platform support%o	Asynchronous Queuing with Dependencies%•	Implementations details%o	Kernel%o	Work-Item%o	Work-Group%o	NDRange%o	Queue%o	Etc.%•	How does it compare to CUDA? Phi%•	Latest trends%o	Phi: bind against MKL for optimized CPU and MIC%o	CUDA-MPI%o	CUDA Sub-Kernel calls%o	CUDA uptake %•	E.g., Matlab (MEX compiled kernel wrappers)%Conclusions on GPGPU%•	Benefits are good%o	Cheap to purchase < $1K%o	superior performance 1.2 TFLOPs possible in one card%o	was a trending technology (major uptake in supercomputing and national labs)%•	Downsides were varied%•	Overall Impression is that%o	Uptake was wide-spread for research projects%o	Focus was on determining limits of the hardware%•	Many studies focused on optimization of primitives which allow general use in applications such as RBF-FD without recreating the wheel when it comes to optimal algorithms. Allows researchers to concentrate on other investigations into application, preconditioning, data analysis, etc.%Newcomers to the field are interested USING gpgpu applications, rather than writing them  


%TODO: put somewhere else
% When matrix is sparse, a direct LU decomposition causes fill-in on factorization. In some cases the fill-in can be minimal, but in general one must assume that fill in can turn the sparse matrix into a dense matrix. To invert and solve Equation~\ref{eq:implicit_eq}, use an iterative solver like GMRES. The GMRES algorithm (described further in Chapter~\ref{chap:applications} applies successive SpMVs along with other vector operations to converge on a solution. Due to the dominance of SpMV in GMRES, the performance of RBF-FD reduces once again to SpMV.



\ifstandalone
\bibliographystyle{plain}
\bibliography{merged_references}
\end{document}
\else
\expandafter\endinput
\fi


