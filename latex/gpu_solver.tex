\makeatletter
\@ifundefined{standalonetrue}{\newif\ifstandalone}{}
\@ifundefined{section}{\standalonetrue}{\standalonefalse}
\makeatother
\ifstandalone
\documentclass{report}

\input{all_usepackages} 
\usepackage[margin=1.25in]{geometry}

\begin{document}
\fi

\chapter{GPU SpMV}
\label{chap:gpu_rbffd}

The most essential operation for RBF-FD derivative calculations is the sparse matrix-vector multiply (SpMV). 
Unfortunately, the SpMV is an inherently memory bound problem (\cite{Bell2009, SuKeutzer2012, Kreutzer2012}). The general SpMV multiplies nonzero elements of each matrix row against corresponding elements of a dense vector. Each row is only applied once as a sparse dot product before it is discarded from memory. The sparse dot product performs two floating point operations (i.e., a multiply and add) for each element of the matrix. %A major concern for SpMV is that either a random set of weights per row are multiplied against a dense, structured vector, or the set of weights for each row are condensed for structure, but then multiplied against a random-access vector. In either case, 
Due to the low number of operations, it is the structure of the sparse matrix (i.e., how regular or random the weights are distributed in the rows) that governs how well the operation performs on the GPU. %GPU utilizes cache and what performance can be achieved overall. 

When GPU computing first took off in 2006 with the release of CUDA, it lacked support for sparse linear algebra. Bell and Garland \cite{Bell2009} launched the first study of the general SpMV problem on the GPU in 2009, and introduced a set of five sparse matrix data structures, or \emph{sparse formats} for the GPU, which compress the matrix in memory based on the presence of certain types of structure in the nonzeros. 
A number of alternative formats have also been proposed in \cite{SuKeutzer2012,Kreutzer2012,LiSaad2010} with the same goal of capitalizing on available structure. By embracing the structure of the matrix, sparse formats reduce the total number of bytes required to store the matrix and thus the number of bytes that must be loaded by the memory bound SpMV kernels. 

Ultimately, the sparse formats in \cite{Bell2009} were included included in the CUDA Toolkit (\cite{CudaToolkitDoc}) to provide basic sparse matrix operations (e.g., multiplication, addition, etc.). The formats are also included in the third party sparse linear algebra libraries CUSP \cite{Cusp2012} and ViennaCL \cite{Rupp2010,Rupp2010a}, which expand beyond the basic matrix operations to include features like dense matrix algebra, iterative Krylov solvers, preconditioners, etc. 

Unfortunately, the original absence of SpMV led our earliest efforts to port RBF-FD onto the GPU with a custom set of codes to perform the same SpMV operation, albeit with lower performance. Later efforts to improve performance eventually turned to leveraging the tuned SpMV implementations from the ViennaCL library. Details of both efforts are presented here. 

The content of this chapter is as follows. First, GPU computing is introduced with an outline of GPU features and benefits for solving scientific problems. Second, the OpenCL language is introduced along with a few hardware features that tie in to the performance of the SpMV. Third, the Roofline Model (\cite{Williams2009}) is presented in an effort to develop reasonable  expectations for the level of performance RBF-FD can achieve on the GPU. Following this, our custom GPU kernels are presented, along with details of how ViennaCL is utilized in the most recent iteration of RBF-FD on the GPU. Finally, the chapter concludes with a look at the actual performance of RBF-FD on a single GPU. 

\section{Introduction to GPU Computing} 

General Purpose GPU (GPGPU) computing is one of today's hottest trends in scientific computing. As problem sizes grow larger, it behooves us to work on parallel architectures, be it across multiple CPUs or one or more GPUs. With over 1 TFLOP/s peak possible throughput in double precision on a single GPU \cite{KeplerFactSheet}, a compelling argument is made for the latter. 

In the last decade, CPU designers have been forced to transition to multi- and many-core chips in order to keep up with demand for continued growth in computational performance. Power constraints make further frequency scaling on CPUs almost impossible, which leaves increased core counts as the only viable solution for designers to continue satisfying Moore's Law \cite{Owens2007}. 


GPUs originated as dedicated hardware for video games and computer graphics. 
Due to the inherent task parallelism in computer graphics  (e.g., for rasterization), early GPU designers adopted a many-core strategy for hardware, which persists today. Computer graphics traditionally involved simple operations, so the majority of transistors on a GPU are dedicated to computation in vector pipelines with limited control for logic. The earliest GPUs had static rendering pipelines with fixed functions. In contrast to this, CPUs have always focused on fast serial and scalar operations. This means they allocate less transistors to computation in order to free space for the needs of advanced logic tasks like branching, caching, etc. \cite{Owens2007,CudaGuide2013}. %Figure~\ref{fig:gpu-devotes-more-transistors-to-data-processing} (Courtesy of NVidia, \cite{CudaGuide2013}) illustrates the difference between CPUs and GPUs at a high level. The high density of Arithmetic Logic Units (ALUs) with simple control and caching units is signature design feature of GPUs. 

Thanks to the highly profitable and always demanding game industry, the static rendering pipeline of yore was molded into a fully programmable and dynamic execution platform with a SIMD-like flavor. Prior to 2006, the GPU had already evolved away from fixed-function pipelines and into limited programmable functionality with control over a subset of phases in the rendering process. At the time researchers were able---with substantial effort---to trick the GPU into solving scientific problems by encoding solutions within the graphics rendering process (see e.g., \cite{Trendall2000,Jansen2007,Harris2005,Owens2007}).
GPUs, miles ahead in the many-core arena, only required a bit of flexibility in control and logic to compete against the CPU for general purpose computing.  

The release of NVidia's CUDA architecture and software stack \cite{CudaGuide2013} at the end of 2006, reduced substantially the need for tricks as developers could suddenly target the GPU with the ease of writing a generic C-like program. The monumental new hardware bypassed the graphics rendering pipeline and allowed programs to execute as though the GPU were just another compute device. The CUDA software stack included NVidia's compiler, \emph{nvcc}, for developers to compile custom GPU codes. Since day one, developers who wanted to quickly transition existing code-bases to the GPU had access to the CUBLAS API, an implementation of the Basic Linear Algebra Subprograms (BLAS) suite on the GPU. CUFFT, a CUDA implementation of the Fast Fourier Transform (FFT), was also provided with the same justification. In many ways NVidia ensured that GPGPU computing was easily accessible to the general public, and in the years since CUDA's release a vast number of libraries and language extensions have surfaced that leverage the GPU architecture for scientific computing in nearly every field of research (see e.g. \cite{NVidiaExamples}). 


The success of GPGPU is largely due to the incredible gap between compute capabilities of the GPU versus a CPU. Individual cores on a GPU operate at a lower clock frequency than CPU cores, but the sheer number of them leads to an astounding level of throughput in terms of floating point operations per second (FLOPs). Figure~\ref{fig:floating-point-operations-per-second} (based on data from \cite{Behr2009, OpenCL2009}) illustrates the historical growth in terms of billions of FLOP/s (GFLOP/sec) achieved by NVidia GPUs versus Intel CPUs. Over the last decade the peak GFLOP/sec for a single GPU has sky-rocketed and consistently tops CPUs by a full order of magnitude. Today, a single NVidia K20 (Kepler-class) GPU is advertised as capable of 1.17 TFLOP/sec in double precision \cite{KeplerFactSheet} and the previous generation NVidia M2070 (Fermi-class) GPU is rated at 515 GFLOP/sec \cite{Fermi2009}. Meanwhile, benchmarks (e.g., \cite{Vladimirov2012}) have found that Intel hardware peaks at an order of magnitude lower with at most 97 GFLOP/sec for double precision operations using all cores of a SandyBridge processor and at most 48 GFLOP/sec on all cores of a Westmere EP processor. To supply much needed data to parallel cores, GPUs have also evolved a memory hierarchy with bandwidth up to five times higher than Intel CPUs \cite{CudaGuide2013}.

%TODO: update with latest GFLOPs (up to TFLOPs now)
\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{../figures/prospectus/GPU_Evolution_opencl.pdf}
\caption{Performance comparison for Graphics Processing Units (GPUs). GFLOP/sec data based on \cite{Behr2009, OpenCL2009}. Although data in this figure was last added in 2010, the divide between NVidia and Intel hardware continues to persist today. Currently, the NVidia M2070 (Fermi-class) GPU is capable of 515 GFLOP/sec in double precision, and the NVidia K20 (Kepler-class) GPU is capable of 1.17 TFLOP/sec in double precision \cite{CudaGuide2013}.}
%Meanwhile, the Intel Westmere EP and SandyBridge chips tout 71.8 GFLOP/sec and 157.7 GFLOP/sec respectively, leaving the CPUs an order of magnitude behind NVidia \cite{IntelGFLOPs}}
\label{fig:floating-point-operations-per-second}
\end{figure}

\section{OpenCL}

Due to the success of CUDA (architecture, language, compiler, etc.), and partly because it was vendor specific and closed source, the OpenCL standard for GPU computing was proposed in 2009 \cite{OpenCL2009}. When CUDA was released in 2006, NVidia was not the only contender in the ring of GPU computing: AMD/ATI also had programmable GPUs that could be targeted by a high level language called Brook+ \cite{BrookGPU2004}. Distantly related, Intel was already working toward multi-/many-core architectures that could be targeted by Intel Threaded Building Blocks \cite{IntelTBB}. The OpenCL standard proposed a parallel computing language with the ability to run on any multi-/many-core architecture. The standard is maintained by the Khronos Group, an advisory committee with representatives from the biggest names in hardware and computing (e.g., Apple, AMD, Intel, NVidia, Samsung, Sony, etc.). 

At the onset of this work it was decided that utilizing OpenCL was preferred over CUDA. The GPU computing market was relatively young, and no single company had a dominant share of the HPC market. GPU hardware from both NVidia and ATI was evolving rapidly and OpenCL promised functional portability (i.e., it would run everywhere), so our bets were hedged on the most likely technology to be supported across vendors going into the future. Note that functional portability does not guarantee performance portability (i.e., optimal kernels for NVidia are not optimal for ATI). Some features like 2-D and 3-D textures, which are part of the CUDA standard, are vendor provided extensions in OpenCL and not supported everywhere. Although OpenCL does not leverage all NVidia specific hardware features, knowledge of the hardware generally helps
structure kernels for the best performance. 


%\subsection{OpenCL Memory and Execution Models}

Designing OpenCL kernels for the GPU follows a similar set of rules as in CUDA. Problems are broken into hundreds or thousands of parallel tasks, with each task involving as many operations as possible per element of memory in order to justify the high cost of memory access.

Figure~\ref{fig:opencl_execute_model} shows the execution model for OpenCL code. OpenCL assumes that all target architectures support some level of SIMD (Single Instruction Multiple Data) execution, and that compute intensive sections of an application are offloaded to the GPU as SIMD \emph{kernels} (callable routines). At the hardware level, a large number of \textit{work-items} (software threads) execute a kernel. For Fermi and Kepler level GPUs, groups of 32 hardware threads are referred to as \textit{warps} and represent the unit of hardware threads that execute concurrently on a single \textit{multi-processor} \cite{CudaGuide2013}. 
In OpenCL (i.e., software), bundles of work-items are called \textit{work-groups}, and execute as a collection of warps constrained to the same multiprocessor. In turn, multiple work-groups of matching dimension are grouped into an \textit{NDRange}, which represents the total number of work-items used to execute a kernel \cite{OpenCL2009}. In the GPU computing literature, the terms \textit{thread} and \textit{warp} have become the prominent vocabulary used to describe the execution of kernels, and are preferred below in reference to execution at the hardware level. 

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{../figures/prospectus/opencl_execute_model.pdf}
\caption{The OpenCL execution model executes a kernel in parallel across many work-items. A collection of work-items execute in SIMD as a work-group. The work-groups are further bundled into an NDRange representing the full set of tasks executed within a kernel. } 
\label{fig:opencl_execute_model}
\end{figure}

% For compute intensive problems, a large number of strategies exist to restructure computation and ensure SIMD processing of work-items (see e.g., \cite{CudaGuide2013}). Unfortunately, RBF-FD is a memory-bound problem. In such cases optimizations must focus solely on efficient use of memory and cache. 

The memory model for OpenCL, depicted in Figure~\ref{fig:opencl_memory_model}, follows a similar hierarchy to the execution model. On a multiprocessor, each multiprocessor core executes a work-item (thread) with a limited set of registers. The number of registers varies with the generation of hardware, but always come in small quantities (e.g., 32K per multiprocessor on the Fermi \cite{CudaGuide2013}). Accessing registers is essentially free, but they are shared across all work-items executing on the multiprocessor. Therefore, keeping data in registers requires one to balance the kernel complexity and the number of work-items per work-group. 


\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{../figures/prospectus/opencl_memory_model.pdf}
\caption{The OpenCL memory model provides a hierarchy of increasingly faster memories, each with smaller size and more limited scope. A single GPU contains multiple multiprocessors and a large global memory shared by all multiprocessors. Within each multiprocessor, the many cores/threads execute work-items and have access to local and cache memory shared with other work-items of the same multiprocessor. Individual work-items also private memory independent from other work-items. The OpenCL memory model generically abstracts the memory architectures for both CPUs and GPUs.} 
\label{fig:opencl_memory_model}
\end{figure}

Work-items of a single work-group share data with one another through a small \textit{local memory} that also doubles as an on-processor L1 cache. Fermi- and Kepler-class GPUs have 64 KB of configurable memory shared between the local and cache memory on each multiprocessor. The option to configure the size of local versus cache memory is CUDA specific and not offered as an extension in OpenCL, so the default split is assumed with 16 KB in cache and 48 KB of local memory \cite{CudaGuide2013}.
Sharing information across multiprocessors is possible in \textit{global device memory}---the largest and slowest memory space on the GPU. To improve seek times into global memory, Fermi and Kepler level architectures include a shared L2 cache for all multiprocessors in addition to the L1 on each multiprocessor. 

When reading and writing global memory on the GPU, the hardware operates on L1 cache lines that are 128 bytes wide and shared by a warp of work-items. Load or store operations on global memory result in an entire cache line being read or written in a single transaction. If warps operate on data spanning multiple cache lines, then the hardware attempts to coalesce operations into as few transactions as possible \cite{CudaGuide2013}. For example, if a warp loads 32 double precision variables (8 bytes each) that are consecutive in memory, then two transactions are needed. If the variables are not consecutive in global memory, but randomly dispersed in global memory, then up to 32 independent memory transactions could be required. The cost of a single transaction in global memory is estimated to take about 400 clock cycles on Fermi and Kepler class GPUs, whereas a single arithmetic instruction issued for for all 32 threads of a warp can take between 2 and 4 clock cycles to complete \cite{CudaGuide2013}. Thus, properly packing operations to utilize full cache lines and hide the latency in global memory access is key to performance in kernels. 

Techniques such as node reordering, presented in Chapter~\ref{chap:stencils}, is one effective way to increase cache line utilization. Although this work does not go into depth on the subject of optimal cache line usage, it is definitely part of future optimizations (see e.g., the work in \cite{ErlebacherSauleFlyerBollig2013}). 


%Our approach to solving PDEs with RBF-FD computes derivatives with a sparse matrix-vector multiply (SpMV). %Once a line has entered the L1 cache, the warp of work-items can access the cache line concurrently, although some rules exist on how quickly

\section{Managing Performance Expectations on the GPU}

Before launching into the discussion of how GPU kernels are actually implemented for RBF-FD, we first attempt to manage expectations of the type of performance the method can achieve on the GPU. To this end, we consider the \emph{roofline model} \cite{Williams2009}, which is useful for estimating the maximum possible performance (in terms of GFLOP/sec) for any kernel on the GPU based only on readily available hardware specifications (e.g., \cite{M2070FactSheet, KeplerFactSheet}). 
 
On the GPU, global memory bandwidth is the constraining resource in kernel performance. The roofline model relates processor performance to the global memory traffic based on the \emph{operational intensity} of a kernel. Operational intensity is defined as the ratio of floating point operations (FLOPs) to total number of bytes loaded by a kernel. The operational intensity depends of course on the precision assumed for calculations. For example, the operational intensity of an SpMV in double precision is two FLOPs (a multiply and add) for every two 8-byte words, resulting in a ratio of $\ ^{1}/_{8}$ FLOPs:Bytes. 

The roofline model states that the maximum possible performance of a GPU kernel is given as
\begin{align}
\text{Max Attainable GFLOP/sec} = \min \begin{cases} \text{Peak Performance (Double Precision)} \\ \text{Max Memory Bandwidth} \times \text{Operational Intensity}  \end{cases}
\label{eq:roofline}
\end{align}
where values for the ``DP Peak Performance" and ``Max Memory Bandwidth" are given by vendor provided hardware specifications. 

This work tests two generations of NVidia GPUs shown in Table~\ref{tbl:gpu_comparison}. The first GPU is the previous generation NVidia ``Fermi"-class M2070 with 6 GB global memory and a maximum memory bandwidth of 148 GB/sec \cite{M2070FactSheet}. The second GPU is the latest generation NVidia ``Kepler"-class K20, which has only 5 GB of global memory but 208 GB/sec bandwidth \cite{KeplerFactSheet}. Both GPUs are mounted on the Cascade cluster at the University of Minnesota Supercomputing Institute. 

Cascade is a heterogenous compute cluster built with a total of 14 compute nodes. The cluster is configured in such a way that the first eight nodes each have two 6-core Intel Xeon X5675 ``Westmere-EP" class CPUs (3.06 GHz), 96 GB of memory, and four M2070 GPUs (i.e., 32 Fermi-class GPUs in total). The next four compute nodes each contain dual 8-core Intel Xeon E5-2670 ``SandyBridge" class CPUs (2.60 GHz), 124 GB of memory, and two K20 GPUs (i.e., 8 total). The remaining nodes in the cluster are styled the same as the K20 compute nodes, except the GPUs are replaced by a single Intel Xeon Phi 5110P accelerator \cite{IntelXeonPhi2013}. The cluster is connected via a QDR-link InfiniBand network in order to promote its use as a distributed multi-GPU computing environment. In Chapter~\ref{chap:multigpu_rbffd} we present details of RBF-FD spanning multiple GPUs on Cascade, but at present are only concerned with the performance on a single GPU. 

\begin{table}[t]
\centering
\caption{Comparison of NVidia GPU features \cite{M2070FactSheet,KeplerFactSheet}.}
\label{tbl:gpu_comparison}
\begin{tabular}{c|c|c}
 & M2070  & K20 \\ \hline
Class & Fermi & Kepler \\ \hline
Global Memory & 6 GB & 5 GB \\ 
Max Memory Bandwidth & 148 GB/sec & 208 GB/sec \\ 
Peak Performance (Double Precision) & 515 GFLOP/sec & 1170 GFLOP/sec \\ 
Multiprocessors & 14 & 13 \\ 
Total Cores & 448 & 2496 \\ 
Cores per Multiprocessor & 32 & 192 \\ \hline
\end{tabular}
\end{table}

Figures~\ref{fig:roofline_m2070} and \ref{fig:roofline_k20} show the double precision roofline models for the M2070 and K20 GPUs. By drawing Equation~\ref{eq:roofline} as a function of the operational intensity we can get 

\begin{figure} 
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../figures/spmv/roofline_m2050_m2070-eps-converted-to.pdf}
\caption{Roofline Model for NVidia Fermi class GPUs, M2050 and M2070. SpMV Peak: 18.5 GFLOP/sec (double precision).}
\label{fig:roofline_m2070}
\end{subfigure}
\quad
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../figures/spmv/roofline_k20-eps-converted-to.pdf}
\caption{Roofline Model for NVidia Kepler K20. SpMV Peak: 26 GFLOP/sec (double precision).}
\label{fig:roofline_k20}
\end{subfigure}
\caption{Roofline models manage expectations for maximum possible GFLOP/sec based on the Operational Intensity of kernels. The SpMV needed by RBF-FD has an operational intensity of $\ ^{1}/_{8}$ FLOPS:Bytes. }
\end{figure}

Obviously the roofline model is idealistic, but it provides a nice upper bound on what to expect from the GPU. Actual measured performance will always be lower that anticipated, but if 80\% 

Measured performance of GPU kernels is computed as
\begin{align}
GFLOP/sec = ^{N * n * 2} /_{t_{ms}} * 10^{-9},
\end{align}
where $t_{ms}$ is the kernel execution time given in milliseconds, and the variables $N$ and $n$ describe the grid size and stencil size respectively. 

Performance of SpMV on GPU will not be optimal. Often one finds in literature examples of applications that have been accelerated by huge factors. In fact, speedup factors greater than 10x should never be possible for SpMV. In order to manage expectations of how much faster tasks will operate on the GPU we can turn to a \emph{roofline model} \cite{Williams2009}.

On the Fermi GPUs (Figure~\ref{fig:roofline_m2070}) at 144 GB/s you need an OI of 4 to reach the peak of 515 GFLOP/sec per card. On the K20 (Figure~\ref{fig:roofline_k20}) you need 6 to hit the peak 1.17 TFLOP/s (208 GB/s). But with an OI of 1/16 you could get 9 GFLOP/sec on the Fermi and 13 on the K20 (compared to the 2 GFLOP/sec on Itasca). 


%TODO: problem with SpMV is operational intensity 
%TODO: OI dictates GFLOP/sec (define GFLOP/sec)
%TODO: roofline model predicts that the stock operation can only achieve 18.5 GFLOP/sec
%TODO: the best way to optimize is through memory load reduction which leads to alternative formats
%TODO: alternative formats include ELL, CSR, COO 

%TODO: initially custom kernels attempted to implement efficiently
%TODO: custom kernels by thread and warp tested 
%TODO: data was structured as a CSR
%TODO: ended up with this convoluted set of operations that perform RK4 in a few kernels
%TODO: performance in Appendix C shows maximum of 3 GFLOP/sec, even when operating on two independent SpMVs

%TODO: in an effort to improve performance we adopted ViennaCL 
%TODO: ViennaCL support for sparse algebra was competitive.  
%TODO: design now includes backends for CUDA, OpenMP, although some operations require OpenCL
%TODO: included in PETSc
%TODO: easy to implement kernels by stepping out to matrix primitives (SpMV, AXPY, etc). Tuned kernels for each
%TODO: contributions to ViennaCL for rectangular matrices and distributed computation

%TODO: performance achieved in ViennaCL formats shown here
%TODO: test performance on cascade where two types of GPU and intel phi are available
%TODO: compared to CPU CSR which was fastest BOOST::uBLAS 
%TODO: surprising that COO performs better than CSR for larger n (explained as: ???; memory may already be loaded?)
%TODO: regardless of COO vs CSR, ELL is clear winner with 43\% of estimated peak
%TODO: expected M2070 to be slower than K20
%TODO: suspect that K20 is same performance because the hardware has one less multiprocessor 
%TODO: and OpenCL executes using NVidia driver from Fermi class, NVidia no longer actively maintaining
%TODO: private comm with other ViennaCL developers suspects NVidia throttles OpenCL so K20 data is less true
%TODO: for now accept 8 GFLOP/sec in ELL and verify the performance growth by stencil size
%TODO: growth by stencil size compares all stencil sizes on M2070 and K20. 
%TODO: comparison includes speedup against CPU CSR shows our expectations of 20x speedup not yet realized
%TODO: speedup actually decreasing because of the increasing performance of CPUs as they become many-core

%TODO: preliminary work on Intel Phi questioned whether it is a GPU killer
%TODO: phi features ring, cache, SIMD cores
%TODO: opencl portability allowed us to repeat matrix format tests with ViennaCL
%TODO: GFLOP/sec shown here. dismal results
%TODO: low low data, CSR better than ELL 
%TODO: two reasons: opencl driver and kernels do not express parallelism necessary 

%TODO: future GPU will investigate ELL alternatives BELL/SBELL/etc. 
%TODO: future work will find more appropriate OpenCL or leverage ViennaCL OpenMP backend on Phi
%TODO: give up on single SpMV and work toward SpMM and multiple SpMMs. 
%TODO: already working with MIC vector instructions for up to xxx GFLOP/sec for multiple SpMMs


\section{Custom Kernels}
\label{sec:custom_gpu_kernels}
 
%
%Our codebase began as a prototype demonstrating the feasibility of RBF-FD
%operating on the GPU. The initial code, published in \cite{BolligFlyerErlebacher2012}, was developed as $N$ independent dot
%products of weights and solution values to approximate derivatives. Weights were
%stored linearly in memory, with the solution values read randomly. On the GPU,
%stencils were evaluated independently by threads, or shared by a warp of
%threads. Operating on stencils in this way implied that GPU kernels were to be
%hand written, tuned and optimized. 
%
%Much later, our perspective evolved to see derivative approximation and
%time-stepping as sparse matrix operations. This opened new possibilities for
%optimization and allowed us to forego hand optimization and fine-tuning of GPU
%kernels. With all of the effort put into optimizing sparse operations within
%libraries like CUSP \cite{Bell2009}, ViennaCL \cite{Rupp2010} and even the
%NVidia provided CUSPARSE \cite{CudaToolkitDoc}, formulating the problem in terms of
%sparse matrix operations allows us to quickly prototype on the GPU and leverage
%all of the optimizations available within the third party libraries. 

%Stencil weights are calculated once by the CPU and copied to the GPU at the beginning of the simulation. The static weights are then reused in every iteration on the GPU. When weights are initially computed they are written to disk in the MatrixMarket format, a standard format for archiving GPU Subsequent runs load weights from disk. Ignoring code initialization, the cost of the algorithm is simply the explicit time advancement of the solution. 


Our initial implementation of RBF-FD on the GPU (published in \cite{BolligFlyerErlebacher2012}) accelerates the standard fourth order Runge-Kutta (RK4) scheme with OpenCL. Figure~\ref{fig:multi_GPU_flow} summarizes the time advancement steps for the multi-CPU/GPU implementation. Starting with code initialization, RBF-FD stencils and weights 

The RK4 steps are: 
\begin{eqnarray} 
\mathbf{k}_1 &=& \Delta t f(t_n, \mathbf{u}_n) \nonumber \\
\mathbf{k}_2 &=& \Delta t f(t_n+\frac{1}{2}\Delta t, \mathbf{u}_n + \frac{1}{2}\mathbf{k}_1) \nonumber \\
\mathbf{k}_3 &=& \Delta t f(t_n+\frac{1}{2}\Delta t, \mathbf{u}_n + \frac{1}{2}\mathbf{k}_2)  \label{eqn:rk4}\\
\mathbf{k}_4 &=& \Delta t f(t_n+\Delta t, \mathbf{u}_n + \mathbf{k}_3) \nonumber \\
\mathbf{u}_{n+1} &=& \mathbf{u}_{n} + \frac{1}{6}(\mathbf{k}_1 + 2\mathbf{k}_2 + 2\mathbf{k}_3 +\mathbf{k}_4), \nonumber
\end{eqnarray}
where each equation has a corresponding kernel launch. To handle a variety of Runga-Kutta implementations, steps $\mathbf{k}_{1\rightarrow4}$ correspond to calls to the same kernel with different arguments. The evaluation kernel returns two output vectors: 
\begin{enumerate} 
\item $\mathbf{k}_i = \Delta t f(t_n + \alpha_{i} \Delta t, \mathbf{u}_n + \alpha_{i} \mathbf{k}_{i-1})$, for steps $i=1,2,3,4$, and
\item  $\mathbf{u}_n + \alpha_{i+1} \mathbf{k}_i$
\end{enumerate} 
We choose $\alpha_{i}=0, \frac{1}{2}, \frac{1}{2}, 1, 0$ and $\mathbf{k}_{0} = \mathbf{u}_n$. The second output for each $\mathbf{k}_{i=1,2,3}$ serves as input to the next evaluation, $\mathbf{k}_{i+1}$. In an effort to avoid an extra kernel launch---and corresponding memory loads---the SAXPY that produces the second output uses the same evaluation kernel. Both outputs are stored in global device memory. When the computation spans multiple GPUs, steps $\mathbf{k}_{1\rightarrow3}$ are each followed by a communication barrier to synchronize the subsets $\mathcal{O}$ and $\mathcal{R}$ of the second output (this includes copying the subsets between GPU and CPU). An additional synchronization occurs on the updated solution, $\mathbf{u}_{n+1}$, to ensure that all GPUs share a consistent view of the solution going into the next time-step.

\begin{figure}[t]
      \centering
       \includegraphics[width=5in]{../figures/paper1/figures/omnigraffle/RK4_multi_GPU_flow.pdf}
      \caption{Workflow for RK4 on multiple GPUs. }
      \label{fig:multi_GPU_flow}
\end{figure}


To evaluate $\mathbf{k}_{1\rightarrow4}$ in Equation~\ref{eqn:rk4}, the discretized operators from Equation~(\ref{eq:evaluation_with_hyperviscosity}) are applied using sparse matrix-vector multiplication. If the operator $D$ is composed of multiple derivatives, a differentiation matrix for each derivative is applied independently, including an additional multiplication for the discretized $H$ operator.
 On the GPU, the kernel parallelizes across rows of the DMs, so all derivatives for stencils are computed in one kernel call.


For the GPU, the OpenCL language \cite{OpenCL2009} assumes a lowest common denominator of hardware capabilities to provide functional portability. For example, all target architectures are assumed to support some level of SIMD (Single Instruction Multiple Data) execution for kernels. Multiple \textit{work-items} execute a kernel in parallel. 
A collection of work-items performing the same task is called a \textit{work-group}. While a user might think of work-groups as executing all work-items simultaneously, the work-items are divided at the hardware level into one or more SIMD \textit{warps}, which are executed by a single multiprocessor. On the family of Fermi GPUs, a warp is 32 work-items \cite{CudaGuide2011}. 
OpenCL assumes a tiered memory hierarchy that provides fast but small \textit{local memory} space that is shared within a work-group \cite{OpenCL2009}. Local memory on Fermi GPUs is 48 KB per multiprocessor \cite{CudaGuide2011}. The \textit{global device memory} allows sharing between work-groups and is the slowest but most abundant memory. 
In the GPU computing literature, the terms \textit{thread} and \textit{shared memory} are synonymous to \textit{work-item} and \textit{local memory} respectively, and are preferred below. 
 
Our initial implementation of RBF-FD on multiple GPUs (\cite{BolligFlyerErlebacher2012}) tested two approaches to computation of derivatives. 
In both cases, the stencil weights are stored in CSR format \cite{Bell2009}, 
a packed one-dimensional array in global memory with all the weights 
of a single stencil in consecutive memory addresses. Each operator is stored as an independent CSR matrix. The consecutive ordering on the weights implies that the solution vector %structured according to the ordering of set $\mathcal{G}$ 
is treated as random access. 

All the computation on the GPU is performed in 8-byte double precision. 




\subsection{Naive Approach: One thread per stencil}

In this first implementation, each thread computes 
the derivative at one stencil center  (Figure~\ref{fig:oneThreadPerStencil}). 
The advantage of this approach is trivial concurrency.  Since each stencil has the same number of neighbors, each derivative has an identical number of computations. As long as the number of stencils is a multiple of the warp size, there are no idle threads. Should the total number of stencils be less than a multiple of the warp size, the final warp would contain idle threads, but the impact on efficiency would be minimal assuming the stencil size is sufficiently large. 

Perfect concurrency from a logical point of view does not 
imply perfect efficiency in practice. 
Unfortunately, the naive approach 
is memory bound. When threads access weights in global memory, 
a full warp accesses a 128-byte segment in a single memory operation \cite{CudaGuide2011}.
Since each thread handles a single stencil, the various threads in a warp access data in very disparate areas of global memory, rather than the same segment. This leads to very large slowdowns as extra memory operations are added for each 128-byte segment that the threads of a warp must access.
However, with stencils sharing many common nodes, and the Fermi hardware providing caching, some weights in the unused portions of the segments might remain in cache long enough to hide the cost of so many additional memory loads. 



\begin{figure}[htbp]
      \centering
       \includegraphics[width=3in]{../figures/paper1/figures/omnigraffle/oneThreadPerStencil.pdf}
      \caption{Naive approach to sparse matrix-vector multiply. Each thread is responsible for the sparse vector dot product of weights and solution values for derivatives at a single stencil.  }
      \label{fig:oneThreadPerStencil}
\end{figure}


\subsection{Alternate Approach: One warp per stencil} 

\begin{figure}[htbp]
      \centering
       \includegraphics[width=3in]{../figures/paper1/figures/omnigraffle/oneWarpPerStencil.pdf}
      \caption{Alternative approach. A full warp (32 threads) collaborate to apply weights  and compute the derivative at a stencil center. }
      \label{fig:oneWarpPerStencil}
\end{figure}


An alternate approach, illustrated in Figure~\ref{fig:oneWarpPerStencil}, dedicates a full warp of threads to a single stencil. Here, 32 threads load the weights of a stencil and the corresponding elements of the solution vector. As the 32 threads each perform a subset of the dot product, their intermediate sums are accumulated in 32 elements of shared memory (one per thread).
Should  a stencil be larger than the warp size, the warp iterates over the stencil in increments of the warp size until the full dot product is complete. Finally, the first thread of the warp performs a sum reduction across the 32 (warp size)  intermediate sums stored in shared memory and writes the derivative value to global memory. 

By operating on a warp by warp basis, weights for a single stencil are loaded with a reduced number of memory accesses. Memory loads for the solution vector remain random access but see some benefit when solution values for a stencil are in a small neighborhood in the memory space. Proximity in memory can be controlled by node indexing (see Chapter~\ref{chap:stencils}). 

For stencil sizes smaller than 32, some threads in the warp always remain idle. Idle threads do not slow down the computation within a warp, but under-utilization of the GPU is not desirable. For small stencil sizes, caching on the Fermi can hide some of the cost of memory loads for the naive approach, with no idle threads, making it more efficient. The real strength of one warp per stencil is seen for large stencil sizes. 
%As part of future work on optimization, we will consider a parallel reduction in shared memory, as well as assigning multiple stencils to a single warp for small  $n$. 

\subsection{Performance}
For the performance of our custom RBF-FD implementation we refer readers to \cite{BolligFlyerErlebacher2012} and Appendix~\ref{app:keeneland_alltoallv_benchmarks}. 


\section{ViennaCL Implementation} 


In the time since the publication of \cite{BolligFlyerErlebacher2012} custom OpenCL kernels have been dropped in favor of a library called ViennaCL. 

Sparse formats supported by ViennaCL are shown in Figure~\ref{fig:sparse_format}.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{gpu_content/omnigraffle/SparseStorage.pdf}
\caption{Sparse format example demonstrating a sparse matrix and corresponding storage data structures. ELL format assumes two non-zeros per row. }
\label{fig:sparse_format}
\end{figure}

Other formats exist such as the Hybrid ELL plus CSR (HYB) format \cite{Bell2009}, Diagonal (DIAG) and Jagged Diagonal (JAD) variants, and a number of variants on the ELL format that operate in blocks (BELL), slices (SELL), or both (SBELL) \cite{SuKeutzer2012}. The HYB format stores the bulk of a matrix in an ELL container and the excess non-zeros per row in a CSR format. HYB benefits from reduced storage and memory loads in the ELL, and maintains the generality of a CSR matrix. 

%\begin{figure}
%\centering
%\includegraphics[width=0.5\textwidth]{../figures/spmv/spmv_vcl_gflops-eps-converted-to.pdf}
%\caption{Single GPU ViennaCL SpMV throughput (GFLOP/sec) compared to Boost::uBLAS CSR. Stencil size $n=50$ for all cases.}
%\label{fig:spmv_vcl_gflops}
%\end{figure}


%TODO: low OI is source of frustration. 

We consider the following formats: Coordinate Matrix format (COO), Compressed-Row Storage format (CSR), ELLPACK Storage format (ELL), and a Hybrid format (HYB) that combines the ELL and CSR formats. On top of 

ViennaCL provides seamless interoperability with the Boost::UBLAS, EIGEN and MTL libraries via C++ templates. We test the performance of our algorithm on one or more CPUs with the Boost::UBLAS library. 


%TODO: CUDA MPI not a feature provided by OpenCL. 

%
%\begin{verbatim}
%"__kernel void vec_mul(\n"
%"          __global const unsigned int * row_indices,\n"
%"          __global const unsigned int * column_indices, \n"
%"          __global const float * elements,\n"
%"          __global const float * vector,  \n"
%"          __global float * result,\n"
%"          unsigned int size) \n"
%"{ \n"
%"  for (unsigned int row = get_global_id(0); row < size; row += get_global_size(0))\n"
%"  {\n"
%"    float dot_prod = 0.0f;\n"
%"    unsigned int row_end = row_indices[row+1];\n"
%"    for (unsigned int i = row_indices[row]; i < row_end; ++i)\n"
%"      dot_prod += elements[i] * vector[column_indices[i]];\n"
%"    result[row] = dot_prod;\n"
%"  }\n"
%"}\n"
%\end{verbatim}
%
%\begin{verbatim}
%"__kernel void vec_mul(\n"
%"    const __global int* coords,\n"
%"    const __global float* elements,\n"
%"    const __global const float * vector,\n"
%"    __global float * result,\n"
%"    const unsigned int row_num,\n"
%"    const unsigned int col_num,\n"
%"    const unsigned int internal_row_num,\n"
%"    const unsigned int items_per_row,\n"
%"    const unsigned int aligned_items_per_row\n"
%"    )\n"
%"{\n"
%"    uint glb_id = get_global_id(0);\n"
%"    uint glb_sz = get_global_size(0);\n"
%"    for(uint row_id = glb_id; row_id < row_num; row_id += glb_sz)\n"
%"    {\n"
%"        float sum = 0;\n"
%"        \n"
%"        uint offset = row_id;\n"
%"        for(uint item_id = 0; item_id < items_per_row; item_id++, offset += internal_row_num)\n"
%"        {\n"
%"            float val = elements[offset];\n"
%"            if(val != 0.0f)\n"
%"            {\n"
%"                int col = coords[offset];    \n"
%"                sum += (vector[col] * val);\n"
%"            }\n"
%"            \n"
%"        }\n"
%"        result[row_id] = sum;\n"
%"    }\n"
%"}\n"
%\end{verbatim}




%TODO: ALL SPMV related work.
%\cite{Bell2009} 
%TODO: \cite{Kreuzer2012} in distributed GPU. 
%\cite{Vuduc2005} etc. 



%\section{Performance}
%In order to quantify the performance of our implementation, we can measure two
%factors. First, we can check the speedup achieved on the GPU relative to the
%CPU to get an idea of how much return of investment is to be expected by all
%the effort in porting the application to the GPU. Speedup is measured as the
%time to execute on the CPU divided by the time to execute on the GPU. 
%
%The second quantification is to check the throughput of the process. By
%quantifying the GFLOP throughput we have a measure that tells us two things:
%first, a concrete number quantifying the amount of work performed per second by
%either hardware, and second because we can calculate the peak throughput possible on
%each hardware, we also have a measure of how occupied our CPU/GPU units are.
%With the GFLOP/sec we can also determine the cost per watt for computation and
%conclude on what problem sizes the GPU is cost effective to target and use. 
%
%As we parallelize across multiple GPUs, these same numbers can come into
%play. However we are also interested in the efficiency. Efficiency is the
%speedup divided by the number of processors. With efficiency we have a measure
%of how well-utilized processors are as we scale either the problem size (weak)
%or the number of processors (strong). As the efficiency diminishes we can
%conclude on how many stencils/nodes per processor will keep our processors
%occupied balanced with the shortest compute time possible (i.e., we are
%maximizing return of investment). 

%Many GPU applications claim a 50x or higher speedup. This will never be the case for RBF-FD for the simple reason that the method reduces to an SpMV. The SpMV is a low computational complexity operation with only two operations for every one memory load. 




%\subsection{OpenCL vs CUDA}
%The market is volatile. Companies survive by investing margins in their next great product. If a product fails or the company faces a recall, their survival may come into question. Thus far, NVidia's CUDA has been wildly popular, but for the longest time (until May 2012) it was closed source. The closed source limited the language to NVidia hardware. As such, the OpenCL language gained popularity due to its support for AMD, Intel, mobile devices, web browsers, etc. NVidia's push to provide an open source compiler may be an attempt to regain the market share, but OpenCL appears to be on good footing. One other point: with an open source NVidia compiler, OpenCL can be optimized by the more mature NVidia compiler for their proprietary hardware. OpenCL compilers are also becoming more sophisticated at auto-optimization. 


%TODO: paralution
Paralution is another library that provides sparse matrix containers, iterative solvers, preconditioners, etc. Paralution provides interfaces to plug into Deal.ii, OpenFOAM, and other packages, and runs on multi-core CPUs and GPUs. 


To further optimize RBF-FD on the GPU, we formulate the problem in terms of a Sparse Matrix-Vector Mulitply (SpMV). When we consider the problem in this light we generate a single Differentiation Matrix that can see two optimizations not possible with our stencil-based view: 
\begin{itemize} 
\item First, the sparse containers used in SpMV allow for their own unique optimizations to compress storage and leverage hardware cache.
\item Evaluation of multiple derivatives can be accumulated by association into one matrix operation. This reduces the total number of floating point operations required per iteration. 
\end{itemize}


%TODO: tell the story of ViennaCL. 
%TODO: what motivated the switch
The library includes a variety of sparse matrix formats, solvers, preconditioners, etc. Dense and sparse containers simplify targeting the GPU. 

At the onset of work with ViennaCL two things were apparent: 1) the library was incredibly powerful but limited to square matrices; and 2) the library had no support for distributed computing. Our implementation of RBF-FD (\cite{BolligRBFFDCode}) required rectangular matrices resulting from domain decomposition. The enhancements to the API have since been added to the main branch of ViennaCL. 


%TODO: CSR Bytes:Flop ratio: \url{http://arxiv.org/pdf/1101.0091v1.pdf}



%Most recently, the PETSc library gained GPU support with the addition of both CUSP \cite{Cusp2012} and ViennCL \cite{Rupp2010} back-ends \cite{Minden2010}. PETSc---which dates back to the early 1990s---is widely-used for solving PDEs with proven scalability on the world's top supercomputers. PETSc provides distributed sparse and dense matrix algebra, Krylov solvers, and preconditioners. The support for GPU back-ends allows a vast number of applications that have been developed over more than two decades to run on multiple GPUs with minimal modification. Since PETSc already has overlapping MPI communication and computation for distributed CPU environments, the GPU offloaded applications also benefit from this feature. Yokota et al. \cite{YokotaGPU2010} demonstrate that PETSc multi-GPU applications can scale excellently (in the strong sense) when configured for tree-based communication, which arises naturally from Fast Multipole Methods. The authors scale their boundary element method across 256 GPUs (2 per compute node) with 80\% parallel efficiency, and 512 GPUs (4 per compute node) with 50\% efficiency. In contrast to this our distributed implementation currently achieves no more than 57\% parallel efficiency when spanning 8 GPUs. While we do not take advantage of tree-based communications, this is planned as part of the future work to improve scalability. 



%GPUs currently support single and double precision. RBF-FD certainly requires double precision at the moment. Solving for the weights in single precision might work, but the lower precision would not be able to handle the ill-conditioned system. Projecting double precision weights into single precision results in weights that sum to $\approx 10^{-4}$, a poor approximation to zero. 


%the addition of a new software layer that finally made GPGPU accessible to the general public. The CUDA API includes routines for memory control, interoperability with graphics contexts (i.e., 
%OpenGL programs), and provides GPU implementation subsets of BLAS and FFTW libraries \cite{CudaGuide2013}. After the undeniable success of CUDA for C, new projects emerged to encourage GPU programming in languages like FORTRAN (see e.g., HMPP \cite{HMPP2009} and Portland Group Inc.'s CUDA-FORTRAN \cite{CudaFortran2009}). 



%\section{Test Environments}
%
%\begin{itemize}	
%\item Early testing was completed on the Keeneland GPU cluster. At the time Keeneland was in the Keeneland Initial Deployment System (KIDS) had 320 NVidia M2070 GPUs (Fermi). 240 CPUs. 6GB of memory. 
%%\item The Spear cluster at FSU has 16x NVIDIA Tesla m2050 GPUs. 3GB of memory. 
%\item The Cascade cluster at the Minnesota Supercomputing Institute (UMN) has 32x NVidia M2070 GPUs. 8 nodes, 12 cores each with four GPUs attached to each. 6GB of memory each. In addition, Cascade has 8 NVidia Kepler K20 GPUs. 
%\end{itemize}


%TODO: list example loops for sparse kernels


Due to the assumption that all stencils have equal size, the ELL format is preferred as the default. 
 

\section{SpMV Performance on Cascade}

The actual observed GFLOP/sec achieved on Cascade's M2070 GPUs with the ViennaCL package is shown in Figure~\ref{fig:gflops_cascade_m2070}. Figure~\ref{fig:gflops_cascade_k20} provides achieved GFLOP/sec for Cascade's K20 GPUs.  

\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n17.png}
\caption{Stencil size $n=17$}
%\label{fig:gflops_cascade_m2070_n17}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n31.png}
\caption{Stencil size $n=31$}
%\label{fig:gflops_cascade_m2070_n31}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n50.png}
\caption{Stencil size $n=50$}
%\label{fig:gflops_cascade_m2070_n50}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n101.png}
\caption{Stencil size $n=101$}
%\label{fig:gflops_cascade_m2070_n101}
\end{subfigure}
\caption{Performance comparison of CSR, COO and ELL matrix formats on Cascade's NVidia M2070 GPU and Intel Westmere CPU (Intel Xeon X5675).}
\label{fig:gflops_cascade_m2070}
\end{figure}


\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n17.png}
\caption{Stencil size $n=17$}
%\label{fig:gflops_cascade_k20_n17}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n31.png}
\caption{Stencil size $n=31$}
%\label{fig:gflops_cascade_k20_n31}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n50.png}
\caption{Stencil size $n=50$}
%\label{fig:gflops_cascade_k20_n50}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n101.png}
\caption{Stencil size $n=101$}
%\label{fig:gflops_cascade_k20_n101}
\end{subfigure}
\caption{Performance comparison of ViennaCL CSR, COO and ELL matrix formats on Cascade's NVidia K20 GPU versus the Boost::uBLAS CSR format on and Intel Sandy-Bridge CPU (Intel Xeon E5-2670).}
\label{fig:gflops_cascade_k20}
\end{figure}


Figures~\ref{fig:ell_gflops_cascade_m2070} and \ref{fig:ell_gflops_cascade_k20} summarize the best achieved GFLOP/sec (in all cases the ELL format) for each stencil size on the M2070's and K20's respectively. Figures~\ref{fig:ell_speedup_cascade_m2070} and \ref{fig:ell_speedup_cascade_k20} show the achieved speedup each GPU gained over their respective Westmere and Sandy-Bridge CPUs. In the case of the K20, Figure~\ref{fig:ell_speedup_cascade_k20} shows that the Sandy-Bridge architecture is closing the divide between CPUs and GPUs, although an order of magnitude difference in the GFLOP/sec still exists.

\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_cascade_m2070.png}
\caption{GFLOP/sec, NVidia M2070 GPU (Cascade)}
\label{fig:ell_gflops_cascade_m2070}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_cascade_k20.png}
\caption{GFLOP/sec, NVidia K20 GPU (Cascade)}
\label{fig:ell_gflops_cascade_k20}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_speedup_cascade_m2070.png}
\caption{Speedup, NVidia M2070 GPU (Cascade) vs. CSR format on Intel Westmere (Intel Xeon X5675)}
\label{fig:ell_speedup_cascade_m2070}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_speedup_cascade_k20.png}
\caption{Speedup, NVidia K20 GPU (Cascade) vs. CSR format on Sandy-Bridge (E5-2670)}
\label{fig:ell_speedup_cascade_k20}
\end{subfigure}
\caption{Comparison by stencil size for the ELL sparse matrix format.}
\end{figure}

\section{Conclusions and Future Work}

This chapter presents the first implementation of RBF-FD to run on GPUs. Our OpenCL implementations apply RBF-FD differentiation matrices via Sparse Matrix-Vector multiplication (SpMV) to compute derivatives everywhere in the domain. The solution values are then updated via a fourth-order Runge Kutta (RK4) time-step. 

Two custom GPU SpMV kernels were tested: one that performs the SpMV by dedicating one thread to compute each row of the differentiation matrix, and one that applies 32 threads (a warp) to compute each row. 

The Roofline model (\cite{Williams2009}) was introduced to manage expectations for the peak possible performance (in terms of GFLOP/sec) that can be achieved for RBF-FD. Although GPUs advertise over 1 TFLOP/sec peak possible performance in double precision, the actual achieved FLOP/sec for the memory-bound RBF-FD application is significantly lower. The SpMV operational intensity of $\frac{1}{8}$ FLOPs:bytes, limits the expected performance to be less than 20 GFLOP/sec on NVidia M2070 hardware. Our custom OpenCL kernels achieve at most 3 GFLOP/sec on NVidia M2070 GPUs.



Sparse containers allow increased efficiency compared to our custom kernels. 
In an effort to boost performance, we integrated the ViennaCL library into our RBF-FD implementation and compared performance of three sparse containers provided by the library to determine the most appropriate sparse representation for RBF-FD. We compare the performance of the ViennaCL sparse containers (ELL, CSR, COO), and a CPU-only implementation of SpMV to the BOOST::uBLAS compressed-row storage. 

%ViennaCL allows control of the number of work-items for each kernel. 
%ViennaCL includes a set of auto-tuning options. %TODO: provide tuned parameters

%TODO: what is profile for each GPU type
%TODO: What is the significance of tuning on our problem $n=17, 31, 50, 101,$ etc. 
%TODO: experiment: SpMV on N=10^6, n = variable (5->105)
%TODO: idealized experiment: SpMV on N=10^6 regular grid with n=variable.



\subsection{Future Work} 


In hindsight, the market today clearly favors NVidia for GPU computing with the majority of GPU clusters world-wide contain NVidia accelerators (typically Tesla units). The CUDA C/C++ compiler was open-sourced in the Spring of 2012 as an LLVM compiler, which allows other languages to directly interface with the GPU without the need for wrappers (see e.g., \cite{NumbaPro}). 

%The latest version of OpenMP (v4.0) introduces pragmas for offloading computation to accelerators. These pragmas will function similarly to pragmas from PGI, OpenACC, and the Intel MIC. 




\subsection{Preliminary Results for Intel Xeon Phi} 

As part of future work into accelerating RBF methods, investigations are underway into the new Intel Xeon Phi architecture \cite{IntelXeonPhi2013}. The variety of hardware available on Cascade will help us establish a clear argument in the choice of accelerator type and resolve the dilemma between choosing Phi vs GPU for our method. Since RBFs generalize other methods, our results should have broad reaching impact to answer similar questions for related methods.

%TODO: preliminary benchmarks on performance of ViennaCL SpMV. 

With the generalization of RBF-FD derivative computation formulated as a sparse matrix multiplication, we can 
% TODO: mention CUSP as alternative but concentrate on VCL
consider the various sparse formats provided by CUSP and ViennaCL. 

%TODO later: \item All stencils with non-uniform size
%TODO: What is the optimal choice of sparse container? How do the sparse containers compare in performance to each other, and to our custom kernels? What can we conclude? 


Figure~\ref{fig:gflops_cascade_intel_phi} is provided for comparison with Figures~\ref{fig:gflops_cascade_m2070} and \ref{fig:gflops_cascade_k20}. This stresses the point that although OpenCL offers functional portability, performance portability is still limited. 

\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n17.png}
\caption{Stencil size $n=17$}
%\label{fig:gflops_cascade_intel_phi_n17}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n31.png}
\caption{Stencil size $n=31$}
%\label{fig:gflops_cascade_intel_phi_n31}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n50.png}
\caption{Stencil size $n=50$}
%\label{fig:gflops_cascade_intel_phi_n50}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n101.png}
\caption{Stencil size $n=101$}
%\label{fig:gflops_cascade_intel_phi_n101}
\end{subfigure}
\caption{Performance comparison of ViennaCL CSR, COO and ELL matrix formats on Cascade's Intel Phi Accelerator, and the Boost::uBLAS CSR format on an Intel Sandy-Bridge CPU (Intel Xeon E5-2670). ViennaCL's kernels are optimized for NVidia GPUs, but function (albeit poorly) on the Intel Phi with the current release of Intel's OpenCL SDK (May, 2013). }
\label{fig:gflops_cascade_intel_phi}
\end{figure}




%
%
%Hardware architecture%	Memory layout%	Processing cores%	Trends in hardware since 2006 (additions and benfits)%Optimization%	SpMV memory layout%	Scheduling threads%	Reductions%OpenCL%	Why? %o	Cross platform support%o	Asynchronous Queuing with Dependencies%	Implementations details%o	Kernel%o	Work-Item%o	Work-Group%o	NDRange%o	Queue%o	Etc.%	How does it compare to CUDA? Phi%	Latest trends%o	Phi: bind against MKL for optimized CPU and MIC%o	CUDA-MPI%o	CUDA Sub-Kernel calls%o	CUDA uptake %	E.g., Matlab (MEX compiled kernel wrappers)%Conclusions on GPGPU%	Benefits are good%o	Cheap to purchase < $1K%o	superior performance 1.2 TFLOPs possible in one card%o	was a trending technology (major uptake in supercomputing and national labs)%	Downsides were varied%	Overall Impression is that%o	Uptake was wide-spread for research projects%o	Focus was on determining limits of the hardware%	Many studies focused on optimization of primitives which allow general use in applications such as RBF-FD without recreating the wheel when it comes to optimal algorithms. Allows researchers to concentrate on other investigations into application, preconditioning, data analysis, etc.%Newcomers to the field are interested USING gpgpu applications, rather than writing them  


%TODO: put somewhere else
% When matrix is sparse, a direct LU decomposition causes fill-in on factorization. In some cases the fill-in can be minimal, but in general one must assume that fill in can turn the sparse matrix into a dense matrix. To invert and solve Equation~\ref{eq:implicit_eq}, use an iterative solver like GMRES. The GMRES algorithm (described further in Chapter~\ref{chap:applications} applies successive SpMVs along with other vector operations to converge on a solution. Due to the dominance of SpMV in GMRES, the performance of RBF-FD reduces once again to SpMV.



\ifstandalone
\bibliographystyle{plain}
\bibliography{merged_references}
\end{document}
\else
\expandafter\endinput
\fi


