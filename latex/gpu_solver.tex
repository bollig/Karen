\makeatletter
\@ifundefined{standalonetrue}{\newif\ifstandalone}{}
\@ifundefined{section}{\standalonetrue}{\standalonefalse}
\makeatother
\ifstandalone
\documentclass{report}

\input{all_usepackages} 
\usepackage[margin=1.25in]{geometry}

\begin{document}
\fi

\chapter{GPU SpMV}
\label{chap:gpu_rbffd}

The most essential operation for RBF-FD derivative calculations is the sparse matrix-vector multiply (SpMV). 
Unfortunately, the SpMV is an inherently memory-bound problem (\cite{Bell2009, SuKeutzer2012, Kreutzer2012}). The general SpMV multiplies nonzero elements of each matrix row against corresponding elements of a dense vector. Each row is only applied once as a sparse dot product before it is discarded from memory. The sparse dot product performs two floating point operations (i.e., a multiply and add) for each element of the matrix. %A major concern for SpMV is that either a random set of weights per row are multiplied against a dense, structured vector, or the set of weights for each row are condensed for structure, but then multiplied against a random-access vector. In either case, 
Due to the low number of operations, it is the structure of the sparse matrix (i.e., how regular or random the weights are distributed in the rows) that governs how well the operation performs on the GPU. %GPU utilizes cache and what performance can be achieved overall. 

When GPU computing first took off in 2006 with the release of CUDA, it lacked support for sparse linear algebra. Bell and Garland \cite{Bell2009} launched the first study of the general SpMV problem on the GPU in 2009, and introduced a set of five sparse matrix data structures, or \emph{sparse formats} for the GPU, which compress the matrix in memory based on the presence of certain types of structure in the nonzeros. 
A number of alternative formats have also been proposed in \cite{SuKeutzer2012,Kreutzer2012,LiSaad2010} with the same goal of capitalizing on available structure. By embracing the structure of the matrix, sparse formats reduce the total number of bytes required to store the matrix and thus the number of bytes that must be loaded by the memory-bound SpMV kernels. 

Ultimately, the sparse formats in \cite{Bell2009} were included in the CUDA Toolkit (\cite{CudaToolkitDoc}) to provide basic sparse matrix operations (e.g., multiplication, addition, etc.) The formats are also included in the third party sparse linear algebra libraries CUSP \cite{Cusp2012} and ViennaCL \cite{Rupp2010,Rupp2010a}, which expand beyond the basic matrix operations to include features like dense matrix algebra, iterative Krylov solvers, preconditioners, etc. 

Unfortunately, the original absence of SpMV led our earliest efforts to port RBF-FD onto the GPU with a custom set of codes to perform the same SpMV operation, albeit with lower performance. Later efforts to improve performance eventually turned to leveraging the tuned SpMV implementations from the ViennaCL library. Details of both efforts are presented here. 

The content of this chapter is as follows. First, GPU computing is introduced with an outline of GPU features and benefits for solving scientific problems. Second, the OpenCL language is introduced along with a few hardware features that tie into the performance of the SpMV. Third, the Roofline Model (\cite{Williams2009}) is presented in an effort to develop reasonable  expectations for the level of performance RBF-FD can achieve on the GPU. This is followed by a presentation of our custom GPU kernels, along with details of how ViennaCL is utilized in the most recent iteration of RBF-FD on the GPU. Finally, the chapter concludes with a look at the actual performance of RBF-FD on a single GPU. 

\section{Introduction to GPU Computing} 

General Purpose GPU (GPGPU) computing is one of today's hottest trends in scientific computing. As problem sizes grow larger, it behooves us to work on parallel architectures, be it across multiple CPUs or one or more GPUs. With over 1 TFLOP/s peak possible throughput in double precision on a single GPU \cite{KeplerFactSheet}, a compelling argument is made for the latter. 

In the last decade, CPU designers have been forced to transition to multi- and many-core chips in order to keep up with demand for continued growth in computational performance. Power constraints make further frequency scaling on CPUs almost impossible, which leaves increased core counts as the only viable solution for increased performance to continue to follow the predicitons implied by Moore's Law \cite{Owens2007}. 


GPUs originated as dedicated hardware for video games and computer graphics. 
Due to the inherent task parallelism in computer graphics  (e.g., for rasterization), early GPU designers adopted a many-core strategy for hardware, which persists to this day. Computer graphics traditionally involved simple operations, so the majority of transistors on a GPU were dedicated to computation in vector pipelines with limited control for logic. The earliest GPUs had static rendering pipelines with fixed functions. In contrast to this, CPUs have always focused on fast serial and scalar operations. Thus, they allocate less transistors to computation in order to free space for the needs of advanced logic tasks like branching, caching, etc. \cite{Owens2007,CudaGuide2013}. %Figure~\ref{fig:gpu-devotes-more-transistors-to-data-processing} (Courtesy of NVidia, \cite{CudaGuide2013}) illustrates the difference between CPUs and GPUs at a high level. The high density of Arithmetic Logic Units (ALUs) with simple control and caching units is signature design feature of GPUs. 

Thanks to the highly profitable and always demanding game industry, the static rendering pipeline of yore was molded into a fully programmable and dynamic execution platform with a SIMD-like flavor. Prior to 2006, the GPU had already evolved away from fixed-function pipelines and into limited programmable functionality with control over a subset of phases in the rendering process. At the time researchers were able---with substantial effort---to trick the GPU into solving scientific problems by encoding solutions within the graphics rendering process (see e.g., \cite{Trendall2000,Jansen2007,Harris2005,Owens2007}).
GPUs, miles ahead in the many-core arena, only required a bit of flexibility in control and logic to compete against the CPU for general purpose computing.  

The release of NVidia's CUDA architecture and software stack \cite{CudaGuide2013} at the end of 2006, reduced substantially the need for tricks as developers could suddenly target the GPU with the ease of writing a generic C-like program. The monumental new hardware bypassed the graphics rendering pipeline and allowed programs to execute as though the GPU were just another compute device. The CUDA software stack included NVidia's compiler, \emph{nvcc}, for developers to compile custom GPU codes. Since day one, developers who wanted to quickly transition existing code-bases to the GPU had access to the CUBLAS API, an implementation of the Basic Linear Algebra Subprograms (BLAS) suite on the GPU. CUFFT, a CUDA implementation of the Fast Fourier Transform (FFT), was also provided with the same justification. In many ways NVidia ensured that GPGPU computing was easily accessible to the general public, and in the years since CUDA's release a vast number of libraries and language extensions have surfaced that leverage the GPU architecture for scientific computing in nearly every field of research (see e.g. \cite{NVidiaExamples}). 


The success of GPGPU is largely due to the incredible gap between compute capabilities of the GPU versus a CPU. Individual cores on a GPU operate at a lower clock frequency than CPU cores, but the sheer number of cores leads to an astounding level of throughput in terms of floating point operations per second (FLOPs). Figure~\ref{fig:floating-point-operations-per-second} (based on data from \cite{Behr2009, OpenCL2009}) illustrates the historical growth in terms of billions of FLOP/s (GFLOP/sec) achieved by NVidia GPUs versus Intel CPUs. Over the last decade the peak GFLOP/sec for a single GPU has sky-rocketed and consistently tops CPUs by a full order of magnitude. Today, a single NVidia K20 (Kepler-class) GPU is advertised as capable of 1.17 TFLOP/sec in double precision \cite{KeplerFactSheet} and the previous generation NVidia M2070 (Fermi-class) GPU is rated at 515 GFLOP/sec \cite{Fermi2009}. Meanwhile, benchmarks (e.g., \cite{Vladimirov2012}) have found that Intel hardware peaks at an order of magnitude lower with at most 97 GFLOP/sec for double precision operations using all cores of a Sandy-Bridge processor and at most 48 GFLOP/sec on all cores of a Westmere EP processor. To supply much needed data to parallel cores, GPUs have also evolved a memory hierarchy with bandwidth up to five times higher than Intel CPUs \cite{CudaGuide2013}. Intel has, in the past year, released the Xeon Phi Processor with 60 cores, rated at 2 TFLOP/sec in double precision~\cite{IntelXeonPhi2013}.

%TODO: update with latest GFLOPs (up to TFLOPs now)
\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{../figures/prospectus/GPU_Evolution_opencl.pdf}
\caption{Performance comparison for Graphics Processing Units (GPUs). GFLOP/sec data based on \cite{Behr2009, OpenCL2009}. Although data in this figure was last added in 2010, the divide between NVidia and Intel hardware continues to persist today. Currently, the NVidia M2070 (Fermi-class) GPU is capable of 515 GFLOP/sec in double precision, while the NVidia K20 (Kepler-class) GPU is capable of 1.17 TFLOP/sec in double precision \cite{CudaGuide2013}.}
%Meanwhile, the Intel Westmere EP and Sandy-Bridge chips tout 71.8 GFLOP/sec and 157.7 GFLOP/sec respectively, leaving the CPUs an order of magnitude behind NVidia \cite{IntelGFLOPs}}
\label{fig:floating-point-operations-per-second}
\end{figure}

\section{OpenCL}

Due to the success of CUDA (architecture, language, compiler, etc.), and partly because it was vendor specific and closed source, the OpenCL standard for GPU computing was proposed in 2009 \cite{OpenCL2009}. When CUDA was released in 2006, NVidia was not the only contender in the ring of GPU computing: AMD/ATI also had programmable GPUs that could be targeted by a high level language called Brook+ \cite{BrookGPU2004}. Distantly related, Intel was already working toward multi-/many-core architectures that could be targeted by Intel Threaded Building Blocks \cite{IntelTBB}. The OpenCL standard proposed a parallel computing language with the ability to run on any multi-/many-core architecture. The standard is maintained by the Khronos Group, an advisory committee with representatives from the biggest names in hardware and computing (e.g., Apple, AMD, Intel, NVidia, Samsung, Sony, etc.). 

At the onset of this work it was decided that utilizing OpenCL was preferred over CUDA. The GPU computing market was relatively young, and no single company had a dominant share of the HPC market. GPU hardware from both NVidia and ATI was evolving rapidly and OpenCL promised functional portability (i.e., it would run everywhere), so our bets were hedged on the most likely technology to be supported across vendors going into the future. Note that functional portability does not guarantee performance portability (i.e., optimal kernels for NVidia are not optimal for ATI). Some features like 2-D and 3-D textures, which are part of the CUDA standard, are vendor provided extensions in OpenCL and not supported everywhere. Although OpenCL does not leverage all of the NVidia specific hardware features, knowledge of the hardware generally helps structure kernels for the best performance. 


%\subsection{OpenCL Memory and Execution Models}

Designing OpenCL kernels for the GPU follows a similar set of rules as in CUDA. Problems are broken into hundreds or thousands of parallel tasks, with each task involving as many operations as possible per element of memory in order to justify the high cost of memory access.

Figure~\ref{fig:opencl_execute_model} shows the execution model for OpenCL code. OpenCL assumes that all target architectures support some level of SIMD (Single Instruction Multiple Data) execution, and that compute intensive sections of an application are offloaded to the GPU as SIMD \emph{kernels} (callable routines). At the hardware level, a large number of \textit{work-items} (software threads) execute a kernel. For Fermi and Kepler level GPUs, groups of 32 hardware threads are referred to as \textit{warps} and represent the unit of hardware threads that execute concurrently on a single \textit{multi-processor} \cite{CudaGuide2013}. 
In OpenCL (i.e., software), bundles of work-items are called \textit{work-groups}, and execute as a collection of warps constrained to the same multiprocessor. In turn, multiple work-groups of matching dimension are grouped into an \textit{NDRange}, which represents the total number of work-items used to execute a kernel \cite{OpenCL2009}. In the GPU computing literature, the terms \textit{thread} and \textit{warp} have become the prominent vocabulary used to describe the execution of kernels, and are preferred below in reference to execution at the hardware level. 

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{../figures/prospectus/opencl_execute_model.pdf}
\caption{The OpenCL execution model executes a kernel in parallel across many work-items. A collection of work-items execute in SIMD as a work-group. The work-groups are further bundled into an NDRange representing the full set of tasks executed within a kernel. } 
\label{fig:opencl_execute_model}
\end{figure}

% For compute intensive problems, a large number of strategies exist to restructure computation and ensure SIMD processing of work-items (see e.g., \cite{CudaGuide2013}). Unfortunately, RBF-FD is a memory-bound problem. In such cases optimizations must focus solely on efficient use of memory and cache. 

The memory model for OpenCL, depicted in Figure~\ref{fig:opencl_memory_model}, follows a similar hierarchy to the execution model. On a multiprocessor, each multiprocessor core executes a work-item (thread) with a limited set of registers. The number of registers varies with the generation of hardware, but always come in small quantities (e.g., 32K per multiprocessor on the Fermi \cite{CudaGuide2013}). Accessing registers is essentially free, but they are shared across all work-items executing on the multiprocessor. Therefore, keeping data in registers requires one to balance the kernel complexity and the number of work-items per work-group. 


\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{../figures/prospectus/opencl_memory_model.pdf}
\caption{The OpenCL memory model provides a hierarchy of increasingly faster memories, each with smaller size and more limited scope. A single GPU contains multiple multiprocessors and a large global memory shared by all multiprocessors. Within each multiprocessor, the many cores/threads execute work-items and have access to local and cache memory shared with other work-items of the same multiprocessor. Individual work-items also private memory independent from other work-items. The OpenCL memory model generically abstracts the memory architectures for both CPUs and GPUs.} 
\label{fig:opencl_memory_model}
\end{figure}

Work-items of a single work-group share data with one another through a small \textit{local memory} that also doubles as an on-processor L1 cache. Fermi- and Kepler-class GPUs have 64 KB of configurable memory shared between the local and cache memory on each multiprocessor. The option to configure the size of local versus cache memory is CUDA specific and not offered as an extension in OpenCL, so the default split is assumed with 16 KB in cache and 48 KB of local memory \cite{CudaGuide2013}.
Sharing information across multiprocessors is possible in \textit{global device memory}---the largest and slowest memory space on the GPU. To improve seek times into global memory, Fermi and Kepler level architectures include a shared L2 cache for all multiprocessors in addition to the L1 on each multiprocessor. 

When reading and writing global memory on the GPU, the hardware operates on L1 cache lines that are 128 bytes wide and shared by a warp of work-items. Load or store operations on global memory result in an entire cache line being read or written in a single transaction. If warps operate on data spanning multiple cache lines, then the hardware attempts to coalesce operations into as few transactions as possible \cite{CudaGuide2013}. For example, if a warp loads 32 double precision variables (8 bytes each) that are consecutive in memory, then two transactions are needed. If the variables are not consecutive in global memory, but randomly dispersed in global memory, then up to 32 independent memory transactions could be required. The cost of a single transaction in global memory is estimated to take about 400 clock cycles on Fermi and Kepler class GPUs, whereas a single arithmetic instruction issued for for all 32 threads of a warp can take between 2 and 4 clock cycles to complete \cite{CudaGuide2013}. Thus, properly packing operations to utilize full cache lines and hide the latency in global memory access is key to performance in kernels. 

Techniques such as node reordering, presented in Chapter~\ref{chap:stencils}, is one effective way to increase cache line utilization. Although this work does not go into depth on the subject of optimal cache line usage, it is definitely part of future optimizations (see e.g., the work in \cite{ErlebacherSauleFlyerBollig2013}). 


%Our approach to solving PDEs with RBF-FD computes derivatives with a sparse matrix-vector multiply (SpMV). %Once a line has entered the L1 cache, the warp of work-items can access the cache line concurrently, although some rules exist on how quickly

\section{Managing Performance Expectations on the GPU}

Before launching into the discussion of how GPU kernels are actually implemented for RBF-FD, we first attempt to manage expectations of the type of performance the method can achieve on the GPU. To this end, we consider the \emph{roofline model} \cite{Williams2009}, which is useful for estimating the maximum possible performance (in terms of GFLOP/sec) for any kernel on the GPU. 
 
On the GPU, global memory bandwidth is the constraining resource in kernel performance. The roofline model relates processor performance to the global memory traffic based on the \emph{operational intensity} of a kernel. Operational intensity is defined as the ratio of floating point operations (FLOPs) to the total number of bytes loaded by a kernel. The operational intensity depends of course on the precision assumed for calculations. For example, the operational intensity of an SpMV in double precision is two FLOPs (a multiply and add) for every two 8-byte words, which gives a ratio of $\ ^{1}/_{8}$ FLOPs:Bytes. 

The roofline model states that the maximum possible performance of a GPU kernel is given as
\begin{align}
\text{Max Attainable GFLOP/sec} = \min \begin{cases} \text{Peak Performance (Double Precision)} \\ \text{Max Memory Bandwidth} \times \text{Operational Intensity}  \end{cases}
\label{eq:roofline}
\end{align}
where values for the peak performance and memory bandwidth are given by the vendor-provided hardware specifications (e.g., \cite{M2070FactSheet, KeplerFactSheet}). 

This work tests two generations of NVidia GPUs shown in Table~\ref{tbl:gpu_comparison}. The first GPU is the previous generation NVidia ``Fermi"-class M2070 with 6 GB global memory and a maximum memory bandwidth of 148 GB/sec \cite{M2070FactSheet}. The second GPU is the latest generation NVidia ``Kepler"-class K20, which has only 5 GB of global memory but 208 GB/sec bandwidth \cite{KeplerFactSheet}. Both GPUs are mounted on the Cascade cluster at the University of Minnesota Supercomputing Institute. 

Cascade is a heterogenous compute cluster built with a total of 14 compute nodes. The cluster is configured in such a way that the first eight nodes each have two 6-core Intel Xeon X5675 ``Westmere-EP" class CPUs (3.06 GHz), 96 GB of memory, and four M2070 GPUs (i.e., 32 Fermi-class GPUs in total). The next four compute nodes each contain dual 8-core Intel Xeon E5-2670 ``Sandy-Bridge" class CPUs (2.60 GHz), 124 GB of memory, and two K20 GPUs (i.e., 8 total). The remaining nodes in the cluster are styled the same as the K20 compute nodes, except the GPUs are replaced by a single Intel Xeon Phi 5110P accelerator \cite{IntelXeonPhi2013}. The cluster is connected via a QDR-link InfiniBand network in order to promote its use as a distributed multi-GPU computing environment. In Chapter~\ref{chap:multigpu_rbffd} we present details of RBF-FD spanning multiple GPUs on Cascade, but at present are only concerned with the performance on a single GPU. 

\begin{table}[t]
\centering
\caption{Comparison of NVidia GPU features \cite{M2070FactSheet,KeplerFactSheet}.}
\label{tbl:gpu_comparison}
\begin{tabular}{c|c|c}
 & M2070  & K20 \\ \hline
Class & Fermi & Kepler \\ \hline
Global Memory & 6 GB & 5 GB \\ 
Max Memory Bandwidth & 148 GB/sec & 208 GB/sec \\ 
Peak Performance (Double Precision) & 515 GFLOP/sec & 1170 GFLOP/sec \\ 
Multiprocessors & 14 & 13 \\ 
Total Cores & 448 & 2496 \\ 
Cores per Multiprocessor & 32 & 192 \\ \hline
\end{tabular}
\end{table}

Figures~\ref{fig:roofline_m2070} and \ref{fig:roofline_k20} show the double precision roofline models for the M2070 and K20 GPUs, and are produced by drawing Equation~\ref{eq:roofline} as a function of the operational intensity. The bandwidth and peak GFLOP/sec data for both figures comes from Table~\ref{tbl:gpu_comparison}. Note that the M2070 and K20 roofline models estimate the maximum possible performance of a double precision SpMV to be 18.5 GFLOP/sec and 26 GFLOP/sec respectively. These estimates are vastly lower than the advertised peaks of 515 GFLOP/sec and 1170 GFLOP/sec, but in order to achieve the advertised peaks one would need to increase the operational intensity to 4 FLOPs:Byte for the M2070 or 6 FLOPs:Byte on the K20. One possible approach to achieve more performance is to add additional vectors to the multiplication so that the sparse matrix is multiplied against a dense matrix (see e.g., the work in \cite{ErlebacherSauleFlyerBollig2013}). 

\begin{figure} 
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../figures/spmv/roofline_m2050_m2070-eps-converted-to.pdf}
\caption{Roofline Model for NVidia Fermi class GPUs, M2050 and M2070. SpMV Peak: 18.5 GFLOP/sec (double precision).}
\label{fig:roofline_m2070}
\end{subfigure}
\quad
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{../figures/spmv/roofline_k20-eps-converted-to.pdf}
\caption{Roofline Model for NVidia Kepler K20. SpMV Peak: 26 GFLOP/sec (double precision).}
\label{fig:roofline_k20}
\end{subfigure}
\caption{Roofline models manage expectations for maximum possible GFLOP/sec based on the Operational Intensity of kernels. The SpMV needed by RBF-FD has an operational intensity of $\ ^{1}/_{8}$ FLOPS:Bytes. }
\end{figure}

The roofline model is an idealistic estimate for the maximum possible performance one can achieve. Actual measured performance is always lower than the estimates. 






%DONE: problem with SpMV is operational intensity 
%DONE: OI dictates GFLOP/sec (define GFLOP/sec)
%DONE: roofline model predicts that the stock operation can only achieve 18.5 GFLOP/sec
%TODO: the best way to optimize is through memory load reduction which leads to alternative formats
%TODO: alternative formats include ELL, CSR, COO 

%TODO: initially custom kernels attempted to implement efficiently
%TODO: custom kernels by thread and warp tested 
%TODO: data was structured as a CSR
%TODO: ended up with this convoluted set of operations that perform RK4 in a few kernels
%TODO: performance in Appendix C shows maximum of 3 GFLOP/sec, even when operating on two independent SpMVs

%TODO: in an effort to improve performance we adopted ViennaCL 
%TODO: ViennaCL support for sparse algebra was competitive.  
%TODO: design now includes backends for CUDA, OpenMP, although some operations require OpenCL
%TODO: included in PETSc
%TODO: easy to implement kernels by stepping out to matrix primitives (SpMV, AXPY, etc). Tuned kernels for each
%TODO: contributions to ViennaCL for rectangular matrices and distributed computation

%TODO: performance achieved in ViennaCL formats shown here
%TODO: test performance on cascade where two types of GPU and intel phi are available
%TODO: compared to CPU CSR which was fastest BOOST::uBLAS 
%TODO: surprising that COO performs better than CSR for larger n (explained as: ???; memory may already be loaded?)
%TODO: regardless of COO vs CSR, ELL is clear winner with 43\% of estimated peak
%TODO: expected M2070 to be slower than K20
%TODO: suspect that K20 is same performance because the hardware has one less multiprocessor 
%TODO: and OpenCL executes using NVidia driver from Fermi class, NVidia no longer actively maintaining
%TODO: private comm with other ViennaCL developers suspects NVidia throttles OpenCL so K20 data is less true
%TODO: for now accept 8 GFLOP/sec in ELL and verify the performance growth by stencil size
%TODO: growth by stencil size compares all stencil sizes on M2070 and K20. 
%TODO: comparison includes speedup against CPU CSR shows our expectations of 20x speedup not yet realized
%TODO: speedup actually decreasing because of the increasing performance of CPUs as they become many-core

%TODO: preliminary work on Intel Phi questioned whether it is a GPU killer
%TODO: phi features ring, cache, SIMD cores
%TODO: opencl portability allowed us to repeat matrix format tests with ViennaCL
%TODO: GFLOP/sec shown here. dismal results
%TODO: low low data, CSR better than ELL 
%TODO: two reasons: opencl driver and kernels do not express parallelism necessary 

%TODO: future GPU will investigate ELL alternatives BELL/SBELL/etc. 
%TODO: future work will find more appropriate OpenCL or leverage ViennaCL OpenMP backend on Phi
%TODO: give up on single SpMV and work toward SpMM and multiple SpMMs. 
%TODO: already working with MIC vector instructions for up to xxx GFLOP/sec for multiple SpMMs


\section{Custom Kernels}
\label{sec:custom_gpu_kernels}
 
%
%Our codebase began as a prototype demonstrating the feasibility of RBF-FD
%operating on the GPU. The initial code, published in \cite{BolligFlyerErlebacher2012}, was developed as $N$ independent dot
%products of weights and solution values to approximate derivatives. Weights were
%stored linearly in memory, with the solution values read randomly. On the GPU,
%stencils were evaluated independently by threads, or shared by a warp of
%threads. Operating on stencils in this way implied that GPU kernels were to be
%hand written, tuned and optimized. 
%
%Much later, our perspective evolved to see derivative approximation and
%time-stepping as sparse matrix operations. This opened new possibilities for
%optimization and allowed us to forego hand optimization and fine-tuning of GPU
%kernels. With all of the effort put into optimizing sparse operations within
%libraries like CUSP \cite{Bell2009}, ViennaCL \cite{Rupp2010} and even the
%NVidia provided CUSPARSE \cite{CudaToolkitDoc}, formulating the problem in terms of
%sparse matrix operations allows us to quickly prototype on the GPU and leverage
%all of the optimizations available within the third party libraries. 

%Stencil weights are calculated once by the CPU and copied to the GPU at the beginning of the simulation. The static weights are then reused in every iteration on the GPU. When weights are initially computed they are written to disk in the MatrixMarket format, a standard format for archiving GPU Subsequent runs load weights from disk. Ignoring code initialization, the cost of the algorithm is simply the explicit time advancement of the solution. 


Our initial implementation of RBF-FD on the GPU (published in \cite{BolligFlyerErlebacher2012}) accelerates the standard fourth order Runge-Kutta (RK4) scheme with OpenCL kernels. 
The RK4 steps are: 
\begin{eqnarray} 
\mathbf{k}_1 &=& \Delta t f(t_n, \mathbf{u}_n) \nonumber \\
\mathbf{k}_2 &=& \Delta t f(t_n+\frac{1}{2}\Delta t, \mathbf{u}_n + \frac{1}{2}\mathbf{k}_1) \nonumber \\
\mathbf{k}_3 &=& \Delta t f(t_n+\frac{1}{2}\Delta t, \mathbf{u}_n + \frac{1}{2}\mathbf{k}_2)  \label{eqn:rk4}\\
\mathbf{k}_4 &=& \Delta t f(t_n+\Delta t, \mathbf{u}_n + \mathbf{k}_3) \nonumber \\
\mathbf{u}_{n+1} &=& \mathbf{u}_{n} + \frac{1}{6}(\mathbf{k}_1 + 2\mathbf{k}_2 + 2\mathbf{k}_3 +\mathbf{k}_4), \nonumber
\end{eqnarray}
where each equation has a corresponding kernel launch. 
To handle a variety of Runge-Kutta methods as well as PDEs, steps $\mathbf{k}_{1\rightarrow4}$ in Equation~\ref{eqn:rk4} correspond to calls to the same \emph{evaluation kernel} with different arguments. Within each call to the evaluation kernel, the RK4 computes
\begin{align*}
f(t_n, u_n) = - D_{\diffop{}} u_n + H u_n, 
\end{align*}
where $D_{\diffop{}}$ is the sparse differentiation matrix for the derivative operator and $H$ is the sparse hyperviscosity filter. On the GPU, the kernel parallelizes across rows of the differentiation matrices, so all derivatives and filters for stencils are computed in a single kernel call.

%To evaluate $\mathbf{k}_{1\rightarrow4}$, the discretized operators from Equation~(\ref{eq:evaluation_with_hyperviscosity}) are applied using sparse matrix-vector multiplication. 
%If the operator $D_{\diffop{}}$ is composed of multiple independent derivatives (e.g., $D_{\diffop{}} = D_x + D_y$), then each differentiation matrix is applied as a separate sparse dot product within an SpMV, including an additional multiplication for the discretized $H$ operator.

 The evaluation kernel returns two output vectors: 
\begin{enumerate} 
\item $\mathbf{k}_i = \Delta t f(t_n + \alpha_{i} \Delta t, \mathbf{u}_n + \alpha_{i} \mathbf{k}_{i-1})$, for steps $i=1,2,3,4$, and
\item  $\mathbf{u}_n + \alpha_{i+1} \mathbf{k}_i$
\end{enumerate} 
We choose $\alpha_{i}=0, \frac{1}{2}, \frac{1}{2}, 1, 0$ and $\mathbf{k}_{0} = \mathbf{u}_n$. The second output for each $\mathbf{k}_{i=1,2,3}$ serves as input to the next evaluation, $\mathbf{k}_{i+1}$. In an effort to avoid an extra kernel launch---and corresponding memory loads---the SAXPY that produces the second output uses the same evaluation kernel. Both outputs are stored in global device memory. When the computation spans multiple GPUs, steps $\mathbf{k}_{1\rightarrow3}$ are each followed by a communication barrier to synchronize the subsets $\mathcal{O}$ and $\mathcal{R}$ of the second output (this includes copying the subsets between GPU and CPU). An additional synchronization occurs on the updated solution, $\mathbf{u}_{n+1}$, to ensure that all GPUs share a consistent view of the solution going into the next time-step.

Figure~\ref{fig:multi_GPU_flow} summarizes the time advancement steps for one or more GPUs. Starting with code initialization, RBF-FD stencils and weights are computed on the CPU and assembled into differentiation matrices. The initial conditions, including the matrices, are copied to the GPU, and the solver enters a loop to perform the time advancement using the method of lines. Each iteration the GPU launches the evaluation kernel four times before launching a fifth kernel to advance the solution by computing $u_{n+1}$ from Equation~\ref{eqn:rk4}. At each evaluation substep, a GPU kernel customized for the PDE and labelled as the \texttt{PDE Solver}, is executed to apply derivative operators at the specified time-step. In order to do this, the \texttt{PDE Solver} loads RBF-FD weights and solution values from memory and then calls a generic GPU function \texttt{Apply Weights} to perform the SpMV for derivatives. The output vectors from calls to \texttt{Apply Weights} are scaled and combined by \texttt{PDE Solver}. %When running on multiple GPUs between kernel executions the CPU will trigger data transfers from GPU to CPU whenever communication is necessary, send and receive data via MPI, and then transfer data from CPU to GPU. 

\begin{figure}[ht]
      \centering
       \includegraphics[width=5in]{../figures/paper1/figures/omnigraffle/RK4_multi_GPU_flow.pdf}
      \caption{Workflow for RK4 on one or more GPUs. }
      \label{fig:multi_GPU_flow}
\end{figure}
 
Our initial implementation of RBF-FD on multiple GPUs (\cite{BolligFlyerErlebacher2012}) tested two approaches to compute derivatives within \texttt{Apply Weights}. 
In both cases, the stencil weights are stored in  
a packed one-dimensional array in global memory with $N*n$ nonzeros and all the weights 
for a single stencil (i.e., an entire row) stored in consecutive memory locations. The one-dimensional array of weights is indexed by two arrays. The first array contains $N$ integer offsets used to jump to the first nonzero of each row. The second array contains $N*n$ integers representing the column index for each nonzero. This sparse compression is equivalent to the \emph{Compressed Row Storage} (CSR) format in \cite{Bell2009, Rupp2010, Cusp2012}.
% AND THE SAME AS ELLPACK?

Each derivative operator is stored as an independent CSR matrix with independent indexing. The consecutive ordering on the weights implies that the solution vector %structured according to the ordering of set $\mathcal{G}$ 
is treated as random access.  % WHAT DOES THIS MEAN: treated as random access?
% Where is CSR indexing discussed first? 

%All the computation on the GPU is performed in 8-byte double precision. 



\subsection{Naive Approach: One thread per stencil}

In this first implementation, each thread computes 
the derivative at one stencil center  (Figure~\ref{fig:oneThreadPerStencil}). 
The advantage of this approach is trivial concurrency.  Since each stencil has the same number of neighbors, each derivative has an identical number of computations. As long as the number of stencils is a multiple of the warp size, there are no idle threads. Should the total number of stencils be less than a multiple of the warp size, the final warp would contain idle threads, but the impact on efficiency would be minimal assuming the stencil size is sufficiently large. In this approach all threads of a warp operate in lock-step on one stencil node after another for perfect concurrency. % CONCURRENCY DOES NOT IMPLY PERFORMANCE

\begin{figure}[ht]
      \centering
       \includegraphics[width=3in]{../figures/paper1/figures/omnigraffle/oneThreadPerStencil.pdf}
      \caption{Naive approach to sparse matrix-vector multiply. Each thread is responsible for the sparse vector dot product of weights and solution values for derivatives at a single stencil.  }
      \label{fig:oneThreadPerStencil}
\end{figure}

Perfect concurrency from a logical point of view does not 
imply perfect efficiency in practice. The compression of the sparse matrix means all stencil weights, offsets, and column indices are read by a warp as contiguous blocks from global memory, but in this case each thread of the warp must load a different memory block. Unfortunately, that can mean up to 32 memory transactions to load weights for the warp. Additionally, the random access on the solution vector implies that sine a warp processes multiple stencils, threads might thrash the cache if they need to load new 128-byte-wide cache lines too often. The new cache lines often end up evicting lines that would be of use to other threads in the warp for later stencil nodes. The larger the stencil size, the more difficult it becomes for solution value cache lines to avoid eviction. Only small stencil sizes stand to benefit from this approach. 
%However, with stencils sharing many common nodes, and the Fermi hardware providing caching, some weights in the unused portions of the segments might remain in cache long enough to hide the cost of so many additional memory loads.




\subsection{Alternate Approach: One warp per stencil} 

An alternate approach, illustrated in Figure~\ref{fig:oneWarpPerStencil}, dedicates a full warp of threads to a single stencil. Here, 32 threads load the weights of a stencil and the corresponding elements of the solution vector. As the 32 threads each perform a subset of the dot product, their intermediate sums are accumulated in 32 elements of shared memory (one per thread).
Should  a stencil be larger than the warp size, the warp iterates over the stencil in increments of the warp size until the full dot product is complete. Finally, the first thread of the warp performs a sum reduction across the 32 intermediate sums stored in shared memory and writes the derivative value to global memory. 

\begin{figure}[ht]
      \centering
       \includegraphics[width=3in]{../figures/paper1/figures/omnigraffle/oneWarpPerStencil.pdf}
      \caption{Alternative approach. A full warp (32 threads) collaborate to apply weights  and compute the derivative at a stencil center. }
      \label{fig:oneWarpPerStencil}
\end{figure}

By operating on a warp by warp basis, weights for a single stencil are loaded with a reduced number of memory accesses. Memory loads for the solution vector remain random access but with only one stencil to worry about the warp is able to maximize the cache utilization and avoid evictions. %YOU SURE ABOUT THIS? 
The best performance of course is achieved when solution values for a stencil are in a small neighborhood in the memory space. Proximity in memory can be improved by node indexing (see Chapter~\ref{chap:stencils}). 

For stencil sizes smaller than 32, some threads in the warp always remain idle. Idle threads do not slow down the computation within a warp, but under-utilization of the GPU is not desirable. For small stencil sizes, caching on the Fermi can hide some of the cost of memory loads for the naive approach, with no idle threads, making it more efficient. The real strength of one warp per stencil is seen for large stencil sizes. 
%As part of future work on optimization, we will consider a parallel reduction in shared memory, as well as assigning multiple stencils to a single warp for small  $n$. 

\subsection{Performance}

For the performance of the custom RBF-FD implementation we refer readers to \cite{BolligFlyerErlebacher2012} and Appendix~\ref{app:keeneland_alltoallv_benchmarks}, where the kernels are benchmarked on an NVidia M2070 GPU. The benchmarks show that dedicating a full warp of threads to each row of the SpMV performs the best for stencil sizes $n \geq 32$, and one thread per stencil otherwise. However, our observed GFLOP/sec for both kernels is fairly low with less than 3 GFLOP/sec in the best case, which is less than 15\% of the 18.5 GFLOP/sec peak estimated by the roofline model. 

% OF COURSE you do not know how efficient your implementation is for n=32. 

\section{ViennaCL Implementation} 

In the interest of boosting performance of RBF-FD beyond 3 GFLOP/sec, work on the custom OpenCL kernels was suspended in favor of leveraging the SpMV built into the ViennaCL library \cite{Rupp2010, Rupp2010a}.  
The ViennaCL library advertises itself as an OpenCL library that provides standard data types for linear algebra operations on GPUs and multi-core CPUs \cite{Rupp2010}. The library, written in C++ templates, abstracts the GPU behind an API for both dense and sparse linear algebra, iterative krylov solvers, matrix preconditioners, etc. Auto-tuned OpenCL kernels simplify targeting the GPU and allow developers to focus on prototyping applications rather than optimizing their own low-level kernels \cite{Rupp2010a}. 
The API is structured following existing programming and interface conventions from the Boost::uBLAS library, a CPU-only linear algebra library (\cite{BoostSite}). In fact, the ViennaCL and Boost::uBLAS APIs are directly compatible, and many ViennaCL algorithms (e.g., iterative solvers, preconditioners, etc.) can be applied to Boost::uBLAS data structures. Similar interoperability exists between ViennaCL and the EIGEN and MTL libraries. 

In an interesting twist, the most recent version of ViennaCL (v1.4.2) also supports multiple parallel back-ends that allow one to target different parallel architectures with the same API. Back-end options include OpenCL, CUDA, and OpenMP. Our work focuses solely on the OpenCL back-end.

We have integrated the ViennaCL library into the RBF-FD GPU project (\cite{BolligRBFFDCode}), and attempt to use the API whenever possible. Two things were apparent when we started working with ViennaCL: a) the library is incredibly powerful but it was limited to square matrices; and b) there was no support multiple GPUs. Since the library is open source, we joined the project as developers and made a number of contributions to the main codebase that add support for rectangular matrices, a matrix preconditioner, a number of bugfixes, and soon: our multi-GPU algorithms with overlapping communication and computation presented in Chapter~\ref{chap:multigpu_rbffd}.

As a testament to the functionality of the ViennaCL library and its utility within the RBF-FD GPU project, we note that the PETSc project (\cite{petsc-web-page}) recently integrated the library into its codebase. PETSc originally leveraged the CUDA-only library, CUSP (\cite{Cusp2012}), to internally offload matrix computations to NVidia GPUs \cite{Minden2010}. However, with the functional portability of OpenCL and the new ViennaCL back-ends, PETSc is now able to transparently target parallel architectures across all vendors. In turn, investigations like \cite{Yokota2010,Yokota2012}, which depend on PETSc, likewise gain that ability. 

ViennaCL supports a number of sparse formats similar to \cite{Bell2009,Cusp2012}. Due to the low operational intensity in the SpMV operation, the best way to boost performance is to increase compression on the sparse matrix. To this end we consider the performance of three ViennaCL formats shown in Figure~\ref{fig:sparse_format}: 
\begin{itemize} 
\item \emph{Coordinate Matrix (COO)} -- The sparse matrix is compressed to three arrays of equal length. The first array stores all nonzero weights from the matrix contiguous by row. The second and third arrays store the row and column indices (resp.) for each nonzero element. For a mesh of $N$ nodes and RBF-FD stencil size $n$ this format requires $(3*N*n)$ storage. 
\item \emph{Compressed Row Storage (CSR)} -- Similar to COO, the matrix is stored in three arrays. All $N*n$ nonzero weights are stored in the first array. In the second array, $N$ integers indicate the offset (into the first array) to get to the first nonzero of each row. Finally, the $N*n$ column indices for each nonzero are stored in the third array. This format requires a total of $(2*(n+1)*N)$ storage.
\item \emph{ELLPACK (ELL)} -- The matrix is assumed to have exactly $m$ nonzeros per row. Then the matrix is compressed to two arrays, each of size $N*m$, that contain only the matrix nonzeros and the column indices. If a row has less than $m$ nonzeros then zeros are used to pad up to the required number of nonzeros. However, based on the assumption that all RBF-FD stencils will have equal size, we have $m=n$ and the format is a good fit for the problem. ELL requires a total of $(2*N*n)$ storage. 
\end{itemize}
%COO and CSR matrices are convenient for problems that have variable number of nonzeros per row. However, the extra storage means that 
%, the ELL format is clearly preferred. 


\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{gpu_content/omnigraffle/SparseStorage.pdf}
\caption{Sparse format example demonstrating a sparse matrix and corresponding storage data structures. ELL format assumes two nonzeros per row. }
\label{fig:sparse_format}
\end{figure}


\subsection{Performance}
To compare the performance of ViennaCL to our custom GPU kernels, we benchmarked the SpMV in a simulated RK4 iteration. In each case, a single RBF-FD differentiation matrix is assembled and then multiplied against four different vectors as evaluation steps. The resulting vectors are then scaled and added together to advance the time-step. We benchmark the time spent in SpMV for the ViennaCL sparse containers (ELL, CSR, and COO), as well as a CPU-only CSR container from the BOOST::uBLAS library. 

Performance of the SpMV is measured as
\begin{align}
GFLOP/sec = (\ ^{N * n * 2} /_{t_{s}} ) * 10^{-9},
\end{align}
where $t_{s}$ is the SpMV execution time given in seconds, and the scale of 2 counts the number of FLOPs (a multiply and add) required per nonzero of the matrix. Variables $N$ and $n$ describe the number of rows and nonzeros per row respectively (equivalently the mesh resolution and stencil size).

Benchmarks are provided for both the NVidia M2070 and NVidia K20 GPUs on the Cascade cluster. In each case we consider a range of mesh resolutions up to one million nodes on the unit sphere, and stencil sizes $n=17, 31, 50, 101$. 

We start in Figure~\ref{fig:gflops_cascade_m2070} with the observed GFLOP/sec achieved on the M2070. Unsurprisingly we find that the performance of the CSR format on the GPU is nearly the same GFLOP/sec achieved by our custom GPU kernels in Appendix~\ref{app:keeneland_alltoallv_benchmarks}. However, the performance of the COO format is interesting in that it exceeds the performance of the lower memory CSR format in most cases. For $n=101$ the COO format is approximately 50\% faster than the CSR format. Although this development is interesting, our motivation in the benchmarks is to achieve the best possible performance. For each stencil size the ELL format clearly demonstrates itself as the best with a peak of up to 8 GFLOP/sec in the case of $n=101$ nodes per stencil.

\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n17.png}
\caption{Stencil size $n=17$}
%\label{fig:gflops_cascade_m2070_n17}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n31.png}
\caption{Stencil size $n=31$}
%\label{fig:gflops_cascade_m2070_n31}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n50.png}
\caption{Stencil size $n=50$}
%\label{fig:gflops_cascade_m2070_n50}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_m2070_n101.png}
\caption{Stencil size $n=101$}
%\label{fig:gflops_cascade_m2070_n101}
\end{subfigure}
\caption{Performance comparison of CSR, COO and ELL matrix formats on Cascade's NVidia M2070 GPU and Intel Westmere CPU (Intel Xeon X5675).}
\label{fig:gflops_cascade_m2070}
\end{figure}

Figure~\ref{fig:gflops_cascade_k20} repeats the benchmarks for Cascade's K20 GPU. In this case, the K20 is expected to perform up to 50\% better than the M2070 due to increased bandwidth. Although an increase is visible for CSR and COO data, the ELL results are disappointingly similar to the M2070 results. The K20 also achieves only up to 8 GFLOP/sec peak for stencil size $n=101$. 

\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n17.png}
\caption{Stencil size $n=17$}
%\label{fig:gflops_cascade_k20_n17}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n31.png}
\caption{Stencil size $n=31$}
%\label{fig:gflops_cascade_k20_n31}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n50.png}
\caption{Stencil size $n=50$}
%\label{fig:gflops_cascade_k20_n50}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_k20_n101.png}
\caption{Stencil size $n=101$}
%\label{fig:gflops_cascade_k20_n101}
\end{subfigure}
\caption{Performance comparison of ViennaCL CSR, COO and ELL matrix formats on Cascade's NVidia K20 GPU versus the Boost::uBLAS CSR format on and Intel Sandy-Bridge CPU (Intel Xeon E5-2670).}
\label{fig:gflops_cascade_k20}
\end{figure}


Figures~\ref{fig:ell_gflops_cascade_m2070} and \ref{fig:ell_gflops_cascade_k20} summarize the best achieved GFLOP/sec (in all cases the ELL format) for each stencil size on the M2070 and K20 respectively. The figures show that stencil sizes $n=17$ and $n=31$ on the K20 perform roughly 20\% worse than the M2070. 

In Figures~\ref{fig:ell_speedup_cascade_m2070} and \ref{fig:ell_speedup_cascade_k20} we consider the speedup achieved by each GPU over their respective CPUs  (i.e., Westmere-EP and Sandy-Bridge), which run the Boost::uBLAS CSR format. In \cite{BolligFlyerErlebacher2012} and Appendix~\ref{app:keeneland_alltoallv_benchmarks} our custom kernels achieved at most 4x speedup over the CPU in a setup nearly identical to the M2070 case here. In this case, leveraging the ELL format achieves between 10x and 14x speedup on one million nodes.

In the case of the K20 shown in Figure~\ref{fig:ell_speedup_cascade_k20}, the faster Sandy-Bridge architecture closes some of the divide between CPUs and GPUs, but we still maintain 8x to 10x speedup on one million nodes. 

\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_cascade_m2070.png}
\caption{GFLOP/sec, NVidia M2070 GPU (Cascade)}
\label{fig:ell_gflops_cascade_m2070}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_cascade_k20.png}
\caption{GFLOP/sec, NVidia K20 GPU (Cascade)}
\label{fig:ell_gflops_cascade_k20}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_speedup_cascade_m2070.png}
\caption{Speedup, NVidia M2070 GPU (Cascade) vs. CSR format on Intel Westmere-EP (Intel Xeon X5675)}
\label{fig:ell_speedup_cascade_m2070}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/ell_comparison_speedup_cascade_k20.png}
\caption{Speedup, NVidia K20 GPU (Cascade) vs. CSR format on Sandy-Bridge (E5-2670)}
\label{fig:ell_speedup_cascade_k20}
\end{subfigure}
\caption{Comparison by stencil size for the ELL sparse matrix format.}
\end{figure}

\subsection{Preliminary Results for Intel Xeon Phi} 

As part of our continued effort to accelerate RBF methods for HPC environments, investigations are underway to target the Intel's new Xeon Phi architecture \cite{IntelXeonPhi2013}; Intel's attempt to gain footing in the GPU/accelerator market. 

The Xeon Phi hardware on Cascade has 60 cores, each the equivalent of a Pentium class processor extended to support 64-bit words, and four hardware threads per core (i.e., a total of 240 threads). Each of the 60 cores also has an associated vector unit, which operates on 512-bit wide SIMD registers. The SIMD registers function as a vector of eight double precision values. A bi-directional ring interconnect supplies each of the cores data from an 8 GB global memory. 

The card has a theoretical maximum bandwidth of 320 GB/s and double precision peak performance of 1011 GFLOP/sec \cite{IntelXeonPhi2013}. The significantly higher bandwidth on the Xeon Phi leads to the expectation that the new architecture should easily outperform NVidia GPUs on SpMV. 

In early 2013, Intel released a beta OpenCL driver for the Xeon Phi. In light of this we were able to repeat our ViennaCL benchmarks on the new hardware. The resulting data, presented in Figure~\ref{fig:gflops_cascade_intel_phi} \emph{should only be considered preliminary}.  The figure is provided for comparison with Figures~\ref{fig:gflops_cascade_m2070} and \ref{fig:gflops_cascade_k20}, and the most obvious issue within Figure~\ref{fig:gflops_cascade_intel_phi} is that every single test case on the Xeon Phi demonstrates substantially lower performance than on NVidia GPUs. We suspect that the low performance is due in part to the beta status of the OpenCL driver. Another concern is that the ViennaCL SpMV kernels were written for NVidia hardware and it is known that OpenCL does not guarantee performance portability. 

As our investigation into the Xeon Phi continues, the benchmarks will be reassessed on upcoming releases of the OpenCL drivers, and we may consider implementing a set of Xeon Phi specific kernels as another ViennaCL back-end. 

\begin{figure} 
\centering
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n17.png}
\caption{Stencil size $n=17$}
%\label{fig:gflops_cascade_intel_phi_n17}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n31.png}
\caption{Stencil size $n=31$}
%\label{fig:gflops_cascade_intel_phi_n31}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n50.png}
\caption{Stencil size $n=50$}
%\label{fig:gflops_cascade_intel_phi_n50}
\end{subfigure}
\quad
\begin{subfigure}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{gpu_content/cascade_spmv/gflops_cascade_intel_phi_n101.png}
\caption{Stencil size $n=101$}
%\label{fig:gflops_cascade_intel_phi_n101}
\end{subfigure}
\caption{Performance comparison of ViennaCL CSR, COO and ELL matrix formats on Cascade's Intel Phi Accelerator, and the Boost::uBLAS CSR format on an Intel Sandy-Bridge CPU (Intel Xeon E5-2670). ViennaCL's kernels function (albeit poorly) on the Intel Phi with the current beta release of Intel's OpenCL SDK (May, 2013). }
\label{fig:gflops_cascade_intel_phi}
\end{figure}


\section{Conclusions and Future Work}

This chapter presented the first implementation of RBF-FD to run on GPUs. Our OpenCL implementations apply RBF-FD differentiation matrices via Sparse Matrix-Vector multiplication (SpMV) to compute derivatives everywhere in the domain. The solution values are then updated via a fourth-order Runge Kutta (RK4) time-step. 


The roofline model (\cite{Williams2009}) was introduced to manage expectations for the peak possible performance that can be achieved for RBF-FD. Although GPUs like the NVidia M2070 advertise 515 GFLOP/sec peak possible performance in double precision, the SpMV operational intensity of $\ ^{1}/_{8}$ FLOPs:Bytes limits our expectation of actual performance to be less than 20 GFLOP/sec. 

Two custom GPU SpMV kernels were described: one that performs the SpMV by dedicating one thread to compute each row of the differentiation matrix, and one that applies 32 threads (a warp) to compute each row. The performance of the custom kernels is rather low at 3 GFLOP/sec at best, but our recent effort to leverage the reduced memory footprint of the ELLPACK sparse matrix format from the ViennaCL library ultimately achieves up to 8 GFLOP/sec, or 43\% of the peak possible performance for SpMV on NVidia M2070 GPUs. 

Note there is still room to improve the SpMV performance, in addition to a few other directions for future work:
\begin{itemize} 
\item 
It has come to our attention that the HPC market today appears to favor NVidia for GPU computing. %For example, the majority of clusters world-wide contain NVidia accelerators. The CUDA C/C++ compiler was open-sourced in the Spring of 2012 as an LLVM compiler, which allows other languages to directly interface with the GPU without the need for wrappers (see e.g., \cite{NumbaPro}). 
That, combined with the lack of support for numerous hardware features in OpenCL, leads to the belief that we will focus on CUDA kernels as we continue into the future. 
%As we proceed into the future effort will likely focus on CUDA kernels. 

\item We are aware that a number of additional sparse formats exist such as the Hybrid ELL plus CSR (HYB) format \cite{Bell2009}, Diagonal (DIAG) and Jagged Diagonal (JAD) variants \cite{LiSaad2010}, and a number of variants on the ELL format that improve cache use by operating in blocks (BELL), slices (SELL), or both (SBELL) \cite{SuKeutzer2012}. It remains to be seen how these formats can impact performance for RBF-FD. 

\item Rather than focus only on the memory bound SpMV operation, we are already investigating scenarios that increase operational intensity and better utilize the GPU hardware. For example, a single sparse matrix multiplied against a dense matrix (i.e., the SpMM operation), as well as multiple sparse matrices times the same dense matrix (see e.g., \cite{ErlebacherSauleFlyerBollig2013}). 
\end{itemize}



\ifstandalone
\bibliographystyle{plain}
\bibliography{merged_references}
\end{document}
\else
\expandafter\endinput
\fi


