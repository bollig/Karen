



%\section{Two level parallelism}
%
%Our current implementation assumes that we are computing on a cluster of CPUs,
%with one GPU attached to each CPU. The CPU maintains control of execution and
%launches kernels on the GPU that execute in parallel. Under the OpenCL standard
%\cite{OpenCL2009}, a tiered memory hierarchy is available on the GPU with
%\textit{global device memory} as the primary and most abundant memory space.
%The memory space for GPU kernels is separate from the memory available to a
%CPU, so data must be explicitly copied to/from global device memory on the GPU. 
%
%\begin{itemize} 
%\item How to copy to/from GPU
%\item Where are the synchronization points (lockstep and overlapping)
%\end{itemize}
%
%\authnote{The basic primitives necessary to parallelize our problems: SpMV, AXPY and }


%\subsection{Overlapping Computation and Communication} 
%Need to describe our approach to launch 
%
%In simplest form (\& indicates an overlapped execution; overlapping is possible by non-blocking launches to the GPU before comm is started): 
%\begin{itemize} 
%\item $ U_{Q \backslash B}' = A_{Q \backslash B} U_{Q \backslash B}$ \& MPI\_alltoallv
%\item $ U_{B}' = A_{B} U_{B}$ 
%\end{itemize} 
%This form benefits because stencils are operated on in their entirety. For the GPU this is important because we maintain SIMD nature. 
%
%A more complex form which might overlap more comm and comp depending on the size of B (i.e., might be beneficial for large stencil sizes): 
%\begin{itemize} 
%\item $ U_{Q \backslash B}' = A_{Q \backslash B} U_{Q \backslash B}$ \& MPI\_alltoallv
%\item \& $ U_{B}' = A_{B \backslash R} U_{B \backslash R}$  (partial sparse vector product)
%\item $ U_{B}' += A_{R} U_{R}$ 
%\end{itemize} 
%
%Note that there is no non-blocking version of MPI\_alltoallv within the MPI standard, so the above two algorithms will not be overlapped when computing on the CPU. To overlap on the CPU, \cite{Kandalla2011} refers to the non-blocking collective as MPI\_ialltoallv. With a collective like this, we could enforce a pattern like: 
%\begin{itemize} 
%\item MPI\_alltoallv
%\item \& $ U_{Q \backslash B}' = A_{Q \backslash B} U_{Q \backslash B}$ 
%\item \& $ U_{B}' = A_{B \backslash R} U_{B \backslash R}$  (partial sparse vector product)
%\item MPI\_wait
%\item $ U_{B}' += A_{R} U_{R}$ 
%\end{itemize} 

%K. Kandalla, H. Subramoni, K. Tomko, D. Pekurovsky, S. Sur and D.K. Panda, "High-performance and scalable non-blocking all-to-all with collective offload on InfiniBand clusters: a study with parallel 3D FFT", Computer Science - Research and Development, vol. 26(3-4):237-246, 2011.
%T. Hoefler, A. Lumsdaine and W. Rehm, "Implementation and Performance Analysis of Non-Blocking Collective Operations for MPI", International Conference for High Performance Computing, Networking, Storage and Analysis (SC07), Reno, USA, 2007.




%\subsubsection{Boundary Conditions} 
%Boundary conditions are handled on the GPU in the following way: 

%TODO: dirichlet: kernel copies dirichlet values into place \authnote{Need to dust off code}



%The term \textit{General Purpose Computing on GPUs} (GPGPU) refers to the application of a Graphics Processing Unit (GPU) 
%to problems other than typical rendering tasks (i.e., vertex transformation, rasterization, etc). Driven by the gaming industry's 
%insatiable desire for more realistic graphics, GPU performance in the last decade years has grown tremendously compared to CPUs.

%Based on \cite{Jansen:2007, CudaGuide:2008, GTX280:2008, Core2Extreme:2008, FermiPrice:2009}, we see in Figure~
%\ref{fig:gpuevolution} the widening gap between compute capabilities on 
%the GPU and CPU. The latest GPUs are capable of approximately ten times the number of 
%floating point operations per second compared to the latest generation CPU. Naturally, this peak 
%performance comes with limitations: historically GPUs have always lagged behind the CPU in terms 
%of flexibility and programmability.


%Implementations of RBF methods are known to be in PETSC \cite{Yokota2010}. The method is simple, and straight-forward to code, but the majority of implementations exist in MATLAB. MATLAB is justified as a platform for computing due to its plethora of solvers, preconditioners, toolkits and support for all things ``matrices" (as it should with a name like ``MATrix LABoratory" (MATLAB). MATLAB is a wonderful platform to develop new features for the RBF methods, but the efficiency of the platform limits its ability to scale to very large problems. For such cases the developers recommend implementations in C or FORTRAN. To aid development in C/FORTRAN, one can take full advantage of the decades worth of research in



 


