



\section{Two level parallelism}

Our current implementation assumes that we are computing on a cluster of CPUs,
with one GPU attached to each CPU. The CPU maintains control of execution and
launches kernels on the GPU that execute in parallel. Under the OpenCL standard
\cite{OpenCL2009}, a tiered memory hierarchy is available on the GPU with
\textit{global device memory} as the primary and most abundant memory space.
The memory space for GPU kernels is separate from the memory available to a
CPU, so data must be explicitly copied to/from global device memory on the GPU. 

\begin{itemize} 
\item How to copy to/from GPU
\item Where are the synchronization points (lockstep and overlapping)
\end{itemize}

\authnote{The basic primitives necessary to parallelize our problems: SpMV, AXPY and }


%\subsection{Overlapping Computation and Communication} 
%Need to describe our approach to launch 
%
%In simplest form (\& indicates an overlapped execution; overlapping is possible by non-blocking launches to the GPU before comm is started): 
%\begin{itemize} 
%\item $ U_{Q \backslash B}' = A_{Q \backslash B} U_{Q \backslash B}$ \& MPI\_alltoallv
%\item $ U_{B}' = A_{B} U_{B}$ 
%\end{itemize} 
%This form benefits because stencils are operated on in their entirety. For the GPU this is important because we maintain SIMD nature. 
%
%A more complex form which might overlap more comm and comp depending on the size of B (i.e., might be beneficial for large stencil sizes): 
%\begin{itemize} 
%\item $ U_{Q \backslash B}' = A_{Q \backslash B} U_{Q \backslash B}$ \& MPI\_alltoallv
%\item \& $ U_{B}' = A_{B \backslash R} U_{B \backslash R}$  (partial sparse vector product)
%\item $ U_{B}' += A_{R} U_{R}$ 
%\end{itemize} 
%
%Note that there is no non-blocking version of MPI\_alltoallv within the MPI standard, so the above two algorithms will not be overlapped when computing on the CPU. To overlap on the CPU, \cite{Kandalla2011} refers to the non-blocking collective as MPI\_ialltoallv. With a collective like this, we could enforce a pattern like: 
%\begin{itemize} 
%\item MPI\_alltoallv
%\item \& $ U_{Q \backslash B}' = A_{Q \backslash B} U_{Q \backslash B}$ 
%\item \& $ U_{B}' = A_{B \backslash R} U_{B \backslash R}$  (partial sparse vector product)
%\item MPI\_wait
%\item $ U_{B}' += A_{R} U_{R}$ 
%\end{itemize} 

%K. Kandalla, H. Subramoni, K. Tomko, D. Pekurovsky, S. Sur and D.K. Panda, "High-performance and scalable non-blocking all-to-all with collective offload on InfiniBand clusters: a study with parallel 3D FFT", Computer Science - Research and Development, vol. 26(3-4):237-246, 2011.
%T. Hoefler, A. Lumsdaine and W. Rehm, "Implementation and Performance Analysis of Non-Blocking Collective Operations for MPI", International Conference for High Performance Computing, Networking, Storage and Analysis (SC07), Reno, USA, 2007.




%\subsubsection{Boundary Conditions} 
%Boundary conditions are handled on the GPU in the following way: 

dirichlet: kernel copies dirichlet values into place \authnote{Need to dust off code}



%The term \textit{General Purpose Computing on GPUs} (GPGPU) refers to the application of a Graphics Processing Unit (GPU) 
%to problems other than typical rendering tasks (i.e., vertex transformation, rasterization, etc). Driven by the gaming industry's 
%insatiable desire for more realistic graphics, GPU performance in the last decade years has grown tremendously compared to CPUs.

%Based on \cite{Jansen:2007, CudaGuide:2008, GTX280:2008, Core2Extreme:2008, FermiPrice:2009}, we see in Figure~
%\ref{fig:gpuevolution} the widening gap between compute capabilities on 
%the GPU and CPU. The latest GPUs are capable of approximately ten times the number of 
%floating point operations per second compared to the latest generation CPU. Naturally, this peak 
%performance comes with limitations: historically GPUs have always lagged behind the CPU in terms 
%of flexibility and programmability.

 

\subsection{Implicit Solvers}

Perhaps this section should move up. I think it might be best to discuss GMRES and iterative methods before Distributed Solvers (that way we can say the general solution form is ``matrix form''. but in parallel we need to use distributed algorithms.

The GMRES algorithm follows Algorithm~\ref{alg:gmres_general}. 

The Arnoldi process can be completed using in a variety of ways. Some libraries like CUSP and \cite{Saad2003} prefer the straightforward Givens rotations because they are easily parallelizable. The givens algorithm is provided in Algorithm~\ref{alg:gmres_givens}

Others like [...] prefer to use an alternate algorithm. This algorithm is demonstrated in Algorithm~\ref{alg:gmres_h}.

Parallelizing the algorithm is possible with communication points listed in red (*). 

Itasca

An Alltoall collective shares .... as in Figure~\ref{fig:mpi_alltoall_visual}. A basic Alltoall collective would work in most cases. However, RBF-FD stencils are not required/guaranteed to be symmetric. As a result, we often have non-uniform sharing across partitions, so an Alltoall would require over-padding all send/receive buffers to allow the maximum receive size times the number of processors. 

In order to avoid excessive padding, we choose the Alltoallv collective, which allows processors to share non-uniform number of bytes with one another. Figure~\ref{fig:mpi_alltoallv_visual} illustrates the benefit of Alltoallv in comparison to Alltoall (Figure~\ref{fig:mpi_alltoall_visual}. By allowing variable number of bytes, the total message size is reduced for all processors. The downside to an alltoallv




The data in the Itasca scaling figures considers the case where an MPI\_Alltoallv or MPI\_Isend/MPI\_Irecv is used. By issuing the receives before encoding the send buffers we ensure that no processes wait unnecessarily long for their collectives to start.

When overlapping GPU computation and communication we further reduce the cost of collectives 
Note that the overlapping GPU optimization to reduce the collective times also applies here. We can cut the cost of communication by avoiding the transcribe after the receives have completed. 


\section{On the use of libraries for parallel solvers}

\authnote{$\rightarrow$} 
Implementations of RBF methods are known to be in PETSC \cite{Yokota2010}. The method is simple, and straight-forward to code, but the majority of implementations exist in MATLAB. MATLAB is justified as a platform for computing due to its plethora of solvers, preconditioners, toolkits and support for all things ``matrices" (as it should with a name like ``MATrix LABoratory" (MATLAB). MATLAB is a wonderful platform to develop new features for the RBF methods, but the efficiency of the platform limits its ability to scale to very large problems. For such cases the developers recommend implementations in C or FORTRAN. To aid development in C/FORTRAN, one can take full advantage of the decades worth of research in

Since our focus within this work is to lay the foundation for parallel computing
with RBF-FD, we have made several simplifying assumptions in our code design.
Libraries like PETsc, Hypre, Trilinos and Deal.ii distribute sparse matrix
operations in similar fashion to our approach. When work initially began on this
dissertation, none of these competing libraries contained support for the GPU.
PETsc is currently developing support for the GPU, but we have not had the
chance to consider it yet. 

Our codebase began as a prototype demonstrating the feasibility of RBF-FD
operating on the GPU. Starting from the perspective of RBF-FD as a Lagrangian
method with stationary nodes, the code was developed as $N$ independent dot
products of weights and solution values to approximate derivatives. Weights were
stored linearly in memory, with the solution values read randomly. On the GPU,
stencils were evaluated independently by threads, or shared by a warp of
threads. Operating on stencils in this way implied that GPU kernels were to be
hand written, tuned and optimized. 

Much later, our perspective evolved to see derivative approximation and
time-stepping as sparse matrix operations. This opened new possibilities for
optimization and allowed us to forego hand optimization and fine-tuning of GPU
kernels. With all of the effort put into optimizing sparse operations within
libraries like CUSP \cite{Bell2009}, ViennaCL \cite{Rupp2010} and even the
NVidia provided CUSPARSE \cite{CUSPARSE}, formulating the problem in terms of
sparse matrix operations allows us to quickly prototype on the GPU and leverage
all of the optimizations available within the third party libraries. 

 


