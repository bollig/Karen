%!TEX root = karen.tex

\chapter{Keeneland Results}

The benchmarks seen in \cite{BolligFlyerErlebacher2012} show the strong scaling of our implementation with a for loop and send/recv. These results were run on Keeneland. The changes to MPI communication are the result of changing from blocking sends and receives (MPI\_send/MPI\_recv) to an MPI\_alltoallv. The test case is vortex roll-up. 

\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{0.425\textwidth}
\includegraphics[width=1.0\textwidth]{../figures/keeneland_results/alltoallv/speedup_1proc_oneWarpPerStencil.pdf}
\caption{One warp per stencil kernel on one GPU in Keeneland}
\label{fig:alltoall_1proc_warp}
\end{subfigure} 
\begin{subfigure}[b]{0.425\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{../figures/keeneland_results/alltoallv/speedup_1CPU_vs_NCPU.pdf}
\caption{Multi-CPU strong scaling on Keeneland for one warp per stencil \authnote{Check if this is SpMV only or with comm...}}
\label{fig:alltoall_multicpu_scaling}
\end{subfigure} 
\begin{subfigure}[b]{0.425\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{../figures/keeneland_results/alltoallv/speedup_1CPU_vs_NGPU_WarpPerStencil.pdf}
\caption{Multi-GPU strong scaling vs one CPU on Keeneland for one warp per stencil}
\label{fig:alltoall_multigpu_vs_cpu_scaling}
\end{subfigure} 
\begin{subfigure}[b]{0.425\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{../figures/keeneland_results/alltoallv/speedup_1GPU_vs_NGPU_WarpPerStencil.pdf}
\caption{Multi-GPU strong scaling vs one GPU on Keeneland for one warp per stencil}
\label{fig:alltoall_multigpu_vs_gpu_scaling}
\end{subfigure} 
\begin{subfigure}[b]{0.425\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{../figures/keeneland_results/alltoallv/multiCPU_costs.pdf}
\caption{Multi-CPU benchmarks by component on Keeneland}
\label{fig:alltoall_multicpu_costs}
\end{subfigure} 
\begin{subfigure}[b]{0.425\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{../figures/keeneland_results/alltoallv/multiGPU_warp_costs.pdf}
\caption{Multi-GPU benchmarks by component on Keeneland}
\label{fig:alltoall_multigpu_costs}
\end{subfigure} 
\end{figure} 

Figure~\ref{fig:alltoall_1proc_warp} shows that our GPU kernel is not much different than in the paper. I have a list of optimizations I'm going through, but this test case focuses on improving the communication. 

Figure~\ref{fig:alltoall_multicpu_scaling}  shows the strong scalability of our method on Multiple CPUs. In distributed computing, ideal scaling is linear. This figure demonstrates that our method does scale linearly (almost super-linearly) as the number of CPUs increases, so our prospect for spanning all CPUs on Keeneland is within reach for problem sizes large enough. The super-linear speedup seen for 10 processors results from improved caching on processors as their individual problem sizes decrease and the processors are able to keep a larger percentage of the problem within fast cache memory.

Figure~\ref{fig:alltoall_multigpu_vs_cpu_scaling}  shows the scaling of multiple GPUs vs 1 CPU. Ideally, this figure would be the product of the previous two figures since the GPUs are attached to CPUs in a one to one correspondence. However, we see from the sub-linear scaling that while the GPU accelerators are decreasing the time to compute solutions, there is less and less return of investment as the number of processors increase. Between this Figure and the previous, the only thing that differs is the hardware on which stencils are evaluated. The cost of communication stays the same as in the previous figure. But that means the communication consumes a increasing percentage of the iteration time, until is dominates. 
Additionally, computing on the GPU requires transfer (additional communication) of data between CPU and GPU. 

Figure~\ref{fig:alltoall_multigpu_vs_gpu_scaling} shows the scalability of multiple GPUs vs 1 GPU. Here we see a sub-linear behavior for all cases. This is attributed to both the cost of transfer between CPUs and GPUs and the decreasing problem size as number of processors increases, which underutilizes the GPUs. 


Figure~\ref{fig:alltoall_multicpu_costs} and Figure~\ref{fig:alltoall_multigpu_costs} show the smaller percentage of time per iteration dedicated to communication compared to the figures in the paper. In the Figure~\ref{fig:alltoall_multigpu_costs}, the way the times bottom out indicates we are/have converged on the minimum time required to launch a GPU kernel, transfer to/from the GPU, and communicate the problem via MPI. To scale to more processors, a larger problem size is absolutely necessary.


%I am generating another set of figures that demonstrate the scaling when we overlap communication and computation. MPI collectives do not allow overlap, but the asynchronous GPU kernel launches do. Therefore, I expect:
%    - the scaling on multiple CPUs vs 1 CPU to be the same as it is now
%    - the scaling on multiple GPUs vs 1 GPU will improve to linear/super-linear for problem sizes that occupy the hardware longer than the minimum kernel launch time. For N=27556 we might only see linear speedup up to 6 or 8 processors. 
%    - larger problem sizes will still be necessary (I have benchmarks for 100K, 500K and 1M on the sphere).
%    

%TODO: Work-Group Size and Number of Stencils
%What if a work-group is larger than a warp? What if the group was occupied by multiple stencils. What improvements to speedup do we see?
%
%How many stencils can each group handle (assuming values stay in shared memory? 
%Shared memory bank conflicts? How do we sort the values? 
%What is the occupancy of the GPU?
%


\chapter{Spear Results}

These results were run on Spear. MPI communication is done with an MPI\_alltoallv collective. The test case is the vortex roll-up. 

\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{0.425\textwidth}
\includegraphics[width=1.0\textwidth]{../figures/spear_results/alltoallv/speedup_1proc_oneWarpPerStencil.eps}
\caption{One warp per stencil kernel on one GPU in Spear}
\label{fig:alltoall_1proc_warp}
\end{subfigure} 
\begin{subfigure}[b]{0.425\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{../figures/spear_results/alltoallv/speedup_1CPU_vs_NCPU.eps}
\caption{Multi-CPU strong scaling on spear for one warp per stencil}
\label{fig:alltoall_multicpu_scaling}
\end{subfigure} 
\begin{subfigure}[b]{0.425\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{../figures/spear_results/alltoallv/speedup_1CPU_vs_NGPU_WarpPerStencil.eps}
\caption{Multi-GPU strong scaling vs one CPU on Spear for one warp per stencil}
\label{fig:alltoall_multigpu_vs_cpu_scaling}
\end{subfigure} 
\begin{subfigure}[b]{0.425\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{../figures/spear_results/alltoallv/speedup_1GPU_vs_NGPU_WarpPerStencil.eps}
\caption{Multi-GPU strong scaling vs one GPU on Spear for one warp per stencil}
\label{fig:alltoall_multigpu_vs_gpu_scaling}
\end{subfigure} 
\end{figure} 

\chapter{Stokes Revisited}

\begin{align}
\div{[\eta(\grad{\vu} + (\grad{\vu})^T)]} + Ra T \hat{r} & = \grad{p} \label{eq:stokes_momentum} \\
\div{\eta(\grad{\vu}} + \div{(\grad{\vu})^T)} + Ra T \hat{r} & = \grad{p} \\
\grad{\eta} \cdot \grad{\vu} + \eta \div{\grad{\vu}} + \div{(\grad{\vu})^T)} + Ra T \hat{r} & = \grad{p} \\
\grad{\eta} \cdot \begin{pmatrix} \pd{u}{x} & \pd{u}{y} & \pd{u}{z} \\ \pd{v}{x} & \pd{v}{y} & \pd{v}{z} \\ \pd{w}{x} & \pd{w}{y} & \pd{w}{z} \end{pmatrix} + \eta \div{\begin{pmatrix} \pd{u}{x} & \pd{u}{y} & \pd{u}{z} \\ \pd{v}{x} & \pd{v}{y} & \pd{v}{z} \\ \pd{w}{x} & \pd{w}{y} & \pd{w}{z} \end{pmatrix}} + \div{\begin{pmatrix} \pd{u}{x} & \pd{v}{y} & \pd{w}{z} \\ \pd{u}{x} & \pd{v}{y} & \pd{w}{z} \\ \pd{u}{x} & \pd{v}{y} & \pd{w}{z} \end{pmatrix}} + Ra T \hat{r} & = \grad{p} \\
\begin{pmatrix} \pd{\eta}{x} \pd{u}{x} & \pd{\eta}{x} \pd{u}{y} & \pd{\eta}{x} \pd{u}{z} \\ \pd{\eta}{y} \pd{v}{x} & \pd{\eta}{y} \pd{v}{y} & \pd{\eta}{y} \pd{v}{z} \\ \pd{\eta}{z} \pd{w}{x} & \pd{\eta}{z} \pd{w}{y} & \pd{\eta}{z} \pd{w}{z} \end{pmatrix} + \eta \begin{pmatrix} \pdd{u}{x} & \pd{u}{y \partial x} & \pd{u}{z \partial x} \\ \pd{v}{x \partial y} & \pdd{v}{y} & \pd{v}{z \partial y} \\ \pd{w}{x \partial z} & \pd{w}{y \partial z} & \pdd{w}{z} \end{pmatrix} + \div{\begin{pmatrix} \div{\vu} \\ \div{\vu} \\ \div{\vu} \end{pmatrix}} + Ra T \hat{r} & = \grad{p} \\
\begin{pmatrix} \pd{\eta}{x} \pd{u}{x} & \pd{\eta}{x} \pd{u}{y} & \pd{\eta}{x} \pd{u}{z} \\ \pd{\eta}{y} \pd{v}{x} & \pd{\eta}{y} \pd{v}{y} & \pd{\eta}{y} \pd{v}{z} \\ \pd{\eta}{z} \pd{w}{x} & \pd{\eta}{z} \pd{w}{y} & \pd{\eta}{z} \pd{w}{z} \end{pmatrix} + \eta \begin{pmatrix} \pdd{u}{x} & \pd{u}{y \partial x} & \pd{u}{z \partial x} \\ \pd{v}{x \partial y} & \pdd{v}{y} & \pd{v}{z \partial y} \\ \pd{w}{x \partial z} & \pd{w}{y \partial z} & \pdd{w}{z} \end{pmatrix} + \div{\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}} + Ra T \hat{r} & = \grad{p} \\
\begin{pmatrix} \pd{\eta}{x} \pd{u}{x} & \pd{\eta}{x} \pd{u}{y} & \pd{\eta}{x} \pd{u}{z} \\ \pd{\eta}{y} \pd{v}{x} & \pd{\eta}{y} \pd{v}{y} & \pd{\eta}{y} \pd{v}{z} \\ \pd{\eta}{z} \pd{w}{x} & \pd{\eta}{z} \pd{w}{y} & \pd{\eta}{z} \pd{w}{z} \end{pmatrix} + \eta \begin{pmatrix} \pdd{u}{x} & \pd{u}{y \partial x} & \pd{u}{z \partial x} \\ \pd{v}{x \partial y} & \pdd{v}{y} & \pd{v}{z \partial y} \\ \pd{w}{x \partial z} & \pd{w}{y \partial z} & \pdd{w}{z} \end{pmatrix} + Ra T \hat{r} & = \grad{p} \\
\begin{pmatrix} 0 \pd{u}{x} & 0 \pd{u}{y} & 0 \pd{u}{z} \\ 0 \pd{v}{x} & 0 \pd{v}{y} & 0 \pd{v}{z} \\ 0 \pd{w}{x} & 0 \pd{w}{y} & 0 \pd{w}{z} \end{pmatrix} + \eta \begin{pmatrix} \pdd{u}{x} & \pd{u}{y \partial x} & \pd{u}{z \partial x} \\ \pd{v}{x \partial y} & \pdd{v}{y} & \pd{v}{z \partial y} \\ \pd{w}{x \partial z} & \pd{w}{y \partial z} & \pdd{w}{z} \end{pmatrix} + Ra T \hat{r} & = \grad{p} \\
\eta \begin{pmatrix} \pdd{u}{x} & \pd{u}{y \partial x} & \pd{u}{z \partial x} \\ \pd{v}{x \partial y} & \pdd{v}{y} & \pd{v}{z \partial y} \\ \pd{w}{x \partial z} & \pd{w}{y \partial z} & \pdd{w}{z} \end{pmatrix} + Ra T \hat{r} & = \grad{p}
\end{align}
The off diagonal blocks are simple to include in code because it is the product $P\cdot \pd{}{x} * P \cdot \pd{}{y}$ (remember they are symmetric so $xy = yx$). Also in the case of the interleaved values we have extra non-zeros bundled together to benefit from cache effects. When $\eta$ is a non-constant function then we have extra work in the assembly to accumulate blocks. Recall that the use of $Q$ arises due to the fact that $div(curl(F)) = 0$, so $Q$ essentially gets the curl of an input vector field and we use the resulting $\curl{F}$ as the manufactured divergence free field. 