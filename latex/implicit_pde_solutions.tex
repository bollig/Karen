%!TEX root = karen.tex

\subsection{Multi-GPU Preconditioned GMRES Applied to Stokes Flow}

%This whole chapter is incomplete work. We started working toward stoke solutions on the unit sphere but it has yet to be completed. 
%
%\section{Introduction}

%We consider herein the 

Another major effort currently underway is the implicit solution of coupled PDEs with RBF-FD on multiple GPUs. Implicit methods assemble a sparse linear system that must be solved by either inverting the differentiation matrix directly (e.g., via a sparse LU decomposition), or solving the system via a sparse iterative solver (e.g., GMRES or BiCGStab).
Our effort adopts the iterative solver method, and we have implemented a multi-GPU preconditioned GMRES method in OpenCL similar to the CUDA implementation in \cite{Bahi2011}. The core operation within sparse iterative solvers is the SpMV. Therefore, all optimizations achieved in Chapters~\ref{chap:gpu_rbffd}, \ref{chap:distributed_rbffd}, and \ref{chap:multigpu_rbffd} continue to apply in this case. 

Many practical applications in sciences such as geophysics, climate modeling, and computational fluid dynamics must solve variations of the Navier-Stokes equations, and depend on an implicit solve component. This paper develops multi-GPU algorithms for implicit RBF-FD systems toward the goal of integration within transient flow problems.

To verify the functionality of the implicit solver, we attempt to solve a simplified model of steady-state viscous Stokes flow on the surface of a sphere, governed by: 
  \begin{align}
\div{[\eta(\grad{\vu} + (\grad{\vu})^T)]} + Ra T \hat{r} & = \grad{p} \label{eq:stokes_momentum} \\
%\pd{T}{t} + (\vu \cdot \grad) T & = \Laplacian T \ \ \ \ \ \ \ \ \textsl{(Energy)}\label{eq:stokes_energy}
\div{\vu} & = 0 \label{eq:stokes_continuity} 
\end{align}
where the unknowns $\vu = \begin{pmatrix} U \\ V \\ W\end{pmatrix}$ and $p$ represent the vector velocity- and scalar pressure-field respectively, $\eta$ is the viscosity tensor, $Ra$ is the non-dimensional Rayleigh number, and $T$ is an initial temperature profile. 

This effort leads to a number of different avenues for future work, which we now survey. 

%This article introduces the first (to our knowledge) parallel approach to solve the steady-state equations on the surface of the unit sphere with the Radial Basis Function-generated Finite Differences (RBF-FD) method. Building on our work in \cite{BolligFlyerErlebacher2012}, which parallelized explicit RBF-FD advection, our goal is to integrate both explicit and implicit components within a larger transient flow model. 


%For decades, the demand for fast and accurate numerical solutions in fluid flow has lead to a plethora of computational methods for various geometries, discretizations and dimensions.  On the sphere in $\R^3$ popular discretizations include the standard latitude-longitude grid, cubed-sphere \cite{NairTransport05}, Ying-Yang overlapping grid \cite{Kameyama2008a}, icosahedral grid \cite{Randall2002} and centroidal voronoi tessellations \cite{Du2006}. Associate with each discretization is a mesh---specific to the choice of numerical method---that indicates connectivity of nodes for differentiation.


 %The explicit component of transient flow is a natural extension of our work in \cite{BolligFlyerErlebacher2012}. 

%While RBF-FD differentiation matrices are applied in the same fashion as standard FD methods, they are unique in that they are asymmetric, non-positive definite and potentially have high condition numbers. To solve an implicit system therefore, we requie an iterative krylov solver like GMRES or BiCGStab which are applicable to matrices of this type. Additionally, preconditioned variants of these methods are required to reduce the complexity of the solution process. 
%%
%
%Parallel GMRES
%\begin{itemize} 
%	\item CPU only: PETSc \cite{Yokota2010}, Hypre \cite{Wildemann2009} 
%	\item Parallel GMRES on single GPU available in ViennaCL \cite{Rupp2010} and CUSP \cite{Cusp2012}
%	\item Parallel GMRES on Multiple GPUs \cite{Bahi2011}
%	\item Reduced Communication with increased computation \cite{Dekker2000}
%\end{itemize} 

%\section{Bad Problem} 
%
%Our initial derivation of the Stokes problem was incorrect. What we thought was
%the proper identity reduced the problem to simple scalar velocity laplacians and
%pressure gradients. The correct formulation of Stokes flow in 3-D would have a
%curl component connecting each dimension. 
%
%Rather than start over and reformulate our problem, we opted to continue
%development of our GPU-based solver with the recognition that the problem we
%posed is actually a coupled Poisson problem and can be solved using the same
%iterative solver. By implementing and testing the solver and preconditioners for
%one problem, we prepare for the other.  
%
%\subsection{Details}
%Items to test for our solver: GFLOP/sec throughput (CPU,GPU), Convergence of Solver (in
%iteration residual) vs Preconditioners.
%
%Test problems: Sphere coupled poisson. Annulus. Stokes on annulus? 
%
%

\subsubsection{Multi-GPU Preconditioned GMRES}

The Generalized Minimum Residual (GMRES) iterative solver was introduced in 1986 by Saad and Schultz \cite{Saad1986}. In contrast to methods like the popular Conjugate Gradient method, which requires symmetric positive definiteness to converge to a solution, the GMRES method allows for general matrix structures. The differentiation matrices produced by RBF-FD are typically unsymmetric, non-positive definite, and non-diagonally dominant. 

At the core of the GMRES algorithm is an Arnoldi (orthogonalization) process to compute an implicit QR factorization. Two common variants of GMRES exist, which differ in their approach to orthogonalization (see \cite{Saad2003}): the first, based on Given's rotation, and the second dependent on Householder reflections. The original GMRES iteration, the Given's variant, is highly parallelizable. The Householder variant was later introduced for reduced memory and operation counts at the cost of reduced parallelism. 

Our implementation of multi-GPU GMRES leverages ViennaCL to directly benefit from the performance of SpMV, vector dot products and other linear algebra primitives. 
Unfortunately, the implementation of GMRES included in the ViennaCL library depends on Householder reflections. As part of our effort, the Given's variant is implemented similar to \cite{Bahi2011}. 

Algorithm~\ref{alg:gmres} presents the basic algorithm of our implementation for a left-preconditioned GMRES algorithm with restarts. The comments in blue denote portions of the algorithm that depend on MPI collectives and the MPI routine that is required. 
Spanning the GMRES across multiple GPUs depends on the same collectives as those developed in Chapter~\ref{chap:multigpu_rbffd}. 


%At the core of the GMRES algorithm is an Arnoldi (orthogonalization) process. 
%
%Saad \cite{Saad1986} introduced a practical implementation of the GMRES method based on Given's rotations to compute an implicit QR factorization. The Given's based algorithm is part of libraries like CUSP \cite{Cusp2012} and CULA Sparse \cite{CULA}; ViennaCL implements the Householder reflection algorithm.

%At the core of the GMRES algorithm is an Arnoldi (orthogonalization) process. %TODO: \authnote{significance of orthogonalization}. 

%TODO: variants in orthogonalization steps and variants in preconditioners, actual iterations, restarts etc. 
%



%The authors of \cite{Bahi2011} do not describe their orthogonalization process, but based on the description of their parallelization strategy it is safe to assume a Given's rotation is used.

%TODO: The Arnoldi process can be completed using in a variety of ways. Some libraries like CUSP and \cite{Saad2003} prefer the straightforward Givens rotations because they are easily parallelizable. The givens algorithm is provided in Algorithm~\ref{alg:gmres_givens}

%TODO: Others like [...] prefer to use an alternate algorithm. This algorithm is demonstrated in Algorithm~\ref{alg:gmres_h}.


\begin{algorithm}                      % enter the algorithm environment
\caption{Left-preconditioned GMRES(k) with Given's Rotations}          % give the algorithm a caption
\label{alg:gmres}                           % and a label for \ref{} commands later in the document
\begin{algorithmic}[1]                    % enter the algorithmic environment
    \State $\varepsilon$ (tolerance for the residual norm $r$), $x_0$ (initial guess), and set $convergence = false$
   	\State {\color{blue} MPI\_Alltoallv($x_0$)}
    \While{ $convergence == false$}  
    \State $r_0 = M^{-1} (b-Ax_0)$ \label{alg:gmres_precond}
   	\State {\color{blue} MPI\_Alltoallv($r_0$)}
    \State $\beta = ||r_0||_2$			\Comment{{\color{blue} MPI\_Allreduce($<r_0,r_0>$)}}
    \State $v_1 = r_0 / \beta$ 

	\For {$j=1$ to $k$} %\label{alg:gmres_rotation_loop_start} 
			\State $w_j = M^{-1} A v_j$  \label{alg:gmres_precond2} 
			\Comment{{\color{blue} MPI\_Alltoallv($w_j$)}}
			\For {$i = 1$ to $j$} %\label{alg:gmres_inner_loop_start}
				\State $h_{i,j} = <w_j, v_i>$ \Comment{{\color{blue} MPI\_Allreduce }}
				\State $w_j = w_j - h_{i,j} v_i$
			\EndFor %\label{alg:gmres_inner_loop_stop}
			\State $h_{j+1, j}  = ||w_j||_2$		\Comment{{\color{blue} MPI\_Allreduce}}
			\State $v_{j+1} = w_j / h_{j+1,j}$		
	\EndFor %\label{alg:gmres_rotation_loop_stop} 

			\State Set $V_k = [v_1, \cdots, v_k]$ and $\bar{H}_k = (h_{i,j})$ an upper Hesssenberg matrix of order $(m+1)\times m$
			\State Solve a least-square problem of size $m$: $\min_{y \in \R^k} ||\beta e_1 - \bar{H}_k y||_2$ %\label{alg:gmres_least_squares} 	
			\State $x_k = x_0 + V_k y_k$ %\label{alg:gmes_residual_norm}

	\If { $||M^{-1}(b-Ax_k)||_2  < \varepsilon$ }
		\State $convergence = true$
	\EndIf
	%\State $x_0 = x_k$ \Comment{{\color{blue} MPI\_Alltoallv(x_0)}}
    \EndWhile
\end{algorithmic}
\end{algorithm}

Lines \ref{alg:gmres_precond} and \ref{alg:gmres_precond2} of Algorithm~\ref{alg:gmres} depend on the application of a preconditioner, $M^{-1}$, to accelerate convergence of the solver. Our initial tests of GMRES applied to RBF-FD have found that the method is slow to converge without preconditioning. Our current effort includes testing a number of preconditioners including Incomplete LU decomposition, and both geometric and algebraic multigrid methods \cite{Saad2003}.
%
%demonstrates slow  that GMRES applied without preconditioning is slow to converge and often requires more than 250 iterations to reach a reasonable level of convergence. 
%
%Our tests show that an incomplete LU factorization with zero fill-in \cite{Saad2003} functions well. 
%
%We also find that a large Krylov subspace must be saved. GMRES converges best when approximately 250 dimensions are saved between restarts. 


%Note that the application of a preconditioner such as ILU0 introduces an additional call to MPI\_Alltoallv before everywhere $M^{-1}$ is present in Algorithm~\ref{alg:gmres}.


\subsubsection{Stokes Flow on the Unit Sphere}

As part of the verification process for our multi-GPU GMRES implementation, we attempt to solve for manufactured solutions of an overly simplified steady state example of Stokes flow on the surface of the unit sphere. 

%TODO: Boundary conditions detract from the accuracy of RBF-FD and introduce other issues such as Runge phenomena \cite{RBFRungePaper}, so we first verify the solution without boundaries. Also, this section 

%TODO: \authnote{ Need to reference work that solves problem in two steps and justify our approach to solve in one step. Golub paper might be good for this. Or the Stoke preconditioners paper. SIMPLE method?  }

Assuming $\eta$ is a constant (i.e., $\grad\eta = 0$), Equations~\ref{eq:stokes_momentum} and \ref{eq:stokes_continuity} result in a linear system of the following form:
\begin{align}
\begin{pmatrix}
-\eta \Laplacian & 0 & 0 & \pd{}{x_1} \\ 
0 & -\eta \Laplacian & 0 & \pd{}{x_2} \\ 
0 & 0 & -\eta \Laplacian & \pd{}{x_3} \\ 
\pd{}{x_1} & \pd{}{x_2} & \pd{}{x_3} & 0 \\
\end{pmatrix} \begin{pmatrix}
U \\ V \\ W \\ p 
\end{pmatrix} = \frac{RaT}{\sqrt{x_1^2 + x_2^2 + x_3^2}}\begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ 0\end{pmatrix}.
\label{eq:stokes_constant_viscosity}
\end{align} 
Recall from Chapter~\ref{chap:rbffd_method} that on the surface of the unit sphere the radial term vanishes, so we are left with:
\begin{align*}
\Laplacian    \equiv \LaplaceBeltrami . 
\end{align*}
The following RBF operator from \cite{WrightFlyerYuen10}---Equation (20)---is applied to the RHS of Equation~\ref{eq:rbffd_weight_system} to generate Laplace-Beltrami RBF-FD weights: 
\begin{align*} 
\LaplaceBeltrami{} \phi = \frac{1}{4} \left[ \left(4-r^2\right) \pdd{}{r} + \frac{4-3r^2}{r} \pd{}{r} \right] \phi,
\end{align*} 
where $r$ is the Euclidean distance between nodes of an RBF-FD stencil and is independent of our choice of coordinate system. 
Additionally, the off-diagonal blocks in Equation~\ref{eq:stokes_constant_viscosity} must be constrained to the sphere via the projection matrix $P = I - \mathbf{x} \mathbf{x}^T$ (also from Chapter~\ref{chap:rbffd_method}) and can be obtained from Equation~\ref{eq:rbffd_weight_system} with the following operator:  
\begin{align*} 
P\pd{}{x_i} \phi = ( x_i \vx^T \vx_k - x_{i,k}) \frac{1}{r(\vx_k - \vx)}  \pd{}{r} \phi(r(\vx_k - \vx)) |_{\vx=\vx_j} 
\end{align*}
% following \cite{FlyerWright09, FlyerLehto11},
% following \cite{FlyerWright09, FlyerLehto11},
%Recall from Chapter~\ref{chap:rbffd_method} that the on the unit Sphere, $\Laplacian \equiv \LaplaceBeltrami$. 
% operator in spherical polar coordinates for $\mathbb{R}^3$ is: 
%\begin{align*} 
%\Laplacian \phi = \left(\underbrace{\frac{1}{\hat{r}} \pd{}{\hat{r}} \left( \hat{r}^{2} \pd{}{\hat{r}}  \right)}_{\mathsf{radial}} + \underbrace{\frac{1}{\hat{r}^2} \Delta_{S}}_{\mathsf{angular}} \right) \phi. %\label{eq:laplacian_as_radial_angular}
%\end{align*}
%Here $\LaplaceBeltrami$ is the Laplace-Beltrami operator---i.e., the Laplacian constrained to the surface of the sphere with radius $\hat{r}$. This form nicely illustrates the components of the $\Laplacian$ corresonding to the radial and angular terms.
%\frac{1}{||\mathbf{x}||}
%\begin{align}
%P = I - \mathbf{x} \mathbf{x}^T =  \begin{pmatrix} 
%(1-x_1^2) & -x_1 x_2 & -x_1 x_3 \\
%-x_1 x_2 & (1-x_2^2) & -x_2 x_3 \\ 
%-x_1 x_3 & -x_2 x_3 & (1-x_3^2) 
%\end{pmatrix} = \begin{pmatrix} P_{x_1} \\ P_{x_2} \\ P_{x_3} \end{pmatrix}
%\label{eq:gradient_projection}
%\end{align}
%where $\mathbf{x}$ is the unit normal at the stencil center, and 
%
%Using the chain rule, and assumption that $r(\vx_k-\vx)=\vectornorm{\vx_k-\vx} = \sqrt{(x_{1,k}-x_1)^2 + (x_{2,k}-x_2)^2 + (x_{3,k}-x_3)^2}$, we obtain the unprojected gradient of $\phi$ as
%$$\nabla \phi(r(\vx_k - \vx)) = \pd{r}{\vx} \pd{\phi(r(\vx_k - \vx))}{r} = - (\vx_k - \vx)\frac{1}{r(\vx_k - \vx)} \pd{\phi(r(\vx_k - \vx))}{r}$$. 
%
%Applying the projection matrix gives 
%\begin{align}
%\mathbf{P} \nabla \phi(r(\vx_k - \vx)) & = - (\mathbf{P} \cdot \vx_k - \mathbf{P}\cdot\vx)\frac{1}{r(\vx_k - \vx)} \pd{\phi(r(\vx_k - \vx))}{r} =  - (\mathbf{P}\cdot\vx_k - 0)\frac{1}{r(\vx_k - \vx)} \pd{\phi(r(\vx_k - \vx))}{r} \\
%& = - (I-\vx\vx^T)(\vx_k
%)\frac{1}{r(\vx_k - \vx)} \pd{\phi(r(\vx_k - \vx))}{r} \\
%& = \begin{pmatrix} x \vx^T \vx_k - x_k \\ y \vx^T \vx_k -  y_k \\ z \vx^T \vx_k -z_k \end{pmatrix} \frac{1}{r(\vx_k - \vx)} \pd{\phi(r(\vx_k - \vx))}{r} 
%% \end{align}
%\cite{FlyerWright09, FlyerLehto11} show that with a little manipulation, the weights can be directly computed with these operators: 
%\begin{align} 
%P\pd{}{x_1} = ( x_1 \vx^T \vx_k - x_{1,k}) \frac{1}{d(\vx_k - \vx)} \pd{\phi(d(\vx_k - \vx))}{d} |_{\vx=\vx_j} \\
%P\pd{}{x_2} = ( x_2 \vx^T \vx_k - x_{2,k}) \frac{1}{d(\vx_k - \vx)} \pd{\phi(d(\vx_k - \vx))}{d} |_{\vx=\vx_j} \\
%P\pd{}{x_3} = ( x_3 \vx^T \vx_k - x_{3,k}) \frac{1}{d(\vx_k - \vx)} \pd{\phi(d(\vx_k - \vx))}{d} |_{\vx=\vx_j}
%\end{align}

\subsubsection{Manufactured Solution}
Our current investigation attempts to solve for $\vu$ and $p$ simultaneously. 
To verify our implementation, we manufacture a solution that satisfies the continuity equation, Equation~\ref{eq:stokes_continuity}. Applying the identity
\begin{align*} 
\div (\curl g(\vx)) = 0,
\end{align*} 
for any function $g(\vx)$, we can easily manufacture a solution by choosing some vector function $g(\vx)$, projecting it onto the sphere via $P_x g(x)$ and applying the curl projection, $Q_x$: 
\begin{align*} 
Q_x = \begin{bmatrix} 0 & -x_3 & x_2 \\ x_3 & 0 & -x_1 \\ -x_2 & x_1 & 0 \end{bmatrix}.
\end{align*} 
Then, a manufactured solution that satisfies both momentum and continuity conditions on the surface of the sphere is given by: 
\begin{align*} 
\vu = Q_x (g(\vx))
\end{align*} 

Typically, on the surface of the sphere, the projection operator from Equation~\ref{eq:gradient_projection} must be applied to an arbitrary $g(\vx)$. 
Our approach is to choose the components of $g(\vx)$ to be various spherical harmonics in Cartesian coordinates. Spherical harmonics nicely satisfy $P_x Y_l^m = Y_l^m$. Consequently, the entire application of $P_x$ is neglected. 


We select $g(x)$ to be: 
\begin{align}
g(x) = 8 Y_{3}^{2} - 3Y_{10}^{5} + Y_{20}^{20} 
\end{align}
and the pressure function:
\begin{align}
P = Y_6^4 
\end{align} 

An example manufactured solution is shown in Figure~\ref{fig:manufactured_solution}, projected onto the plane via a Mollweide projection: 
\begin{align*}
x &= \frac{2\sqrt{2}}{\pi} \lambda \cos{\theta} \\
y &= \sqrt{2}\sin{\theta}.
\end{align*}

\begin{figure} 
\centering
\begin{subfigure}[b]{\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{../figures/paper2/figures/U_exact.png}
\caption{Solution $Q_x( g(x) )$ with $g(x) = 8 Y_{3}^{2} - 3Y_{10}^{5} + Y_{20}^{20}$}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
\centering
\includegraphics[width=0.95\textwidth]{../figures/paper2/figures/RHS.png}
\caption{Right Hand Side (Constant Viscosity)}
\end{subfigure}
\caption{A divergence free field is manufactured for the sphere. }
\label{fig:manufactured_solution}
\end{figure} 


\subsubsection{Constraints} 
Due to the lack of boundary conditions on the sphere, the family of solutions that satisfy the PDE in Equation~\ref{eq:stokes_constant_viscosity} includes four free constants (one for each $U$, $V$, $W$ and $p$). 
%The null space is found using the matlab sparse SVD command: \begin{mcode}{1}
%[Usvd, Ssvd, Vsvd, flagSVD] = svds(LHS,10,0);
%sing_value_indices = find(max(Ssvd) < 1e-6)
%\end{mcode} 

%TODO: null-space *of the solution*? or null-space of the equation?
One way to close the null-space of the equation is to augment Equation~\ref{eq:stokes_constant_viscosity} with the following constraints: 
\begin{multline}
\left(\begin{array}{cccc:cccc}  
-\eta \Laplacian & 0 & 0 & \pd{}{x_1} & 1_{N \times 1} & 0 & 0 & 0 \\ 
0 & -\eta \Laplacian & 0 & \pd{}{x_2} & 0 & 1_{N \times 1} & 0 & 0\\ 
0 & 0 & -\eta \Laplacian & \pd{}{x_3} & 0 & 0 & 1_{N \times 1} & 0\\ 
\pd{}{x_1} & \pd{}{x_2} & \pd{}{x_3} & 0 & 0 & 0 & 0 & 1_{N \times 1}\\
\hdashline
1_{1 \times N} & 0 & 0 & 0 & \multicolumn{4}{c}{\multirow{4}{*}{$0_{4 \times 4}$}} \\
0 & 1_{1 \times N} & 0 & 0 & \\
0 & 0 & 1_{1 \times N} & 0 & \\ 
0 & 0 & 0 & 1_{1 \times N} & \\
\end{array} \right) \left(\begin{array}{c} 
U \\ V \\ W \\ p \\ \hdashline c_1 \\ c_2 \\ c_3 \\ c_4
\end{array} \right) \\
 = \frac{RaT}{\sqrt{x_1^2 + x_2^2 + x_3^2}}\begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ 0 \\ \hdashline \int_\Omega U \partial\Omega \\ \int_\Omega V \partial\Omega \\ \int_\Omega W \partial\Omega \\ \int_\Omega p \partial\Omega \end{pmatrix}.
\label{eq:stokes_system_with_constraints}
\end{multline}
where the subscript on $1_{N \times 1}$ indicates a $N \times 1$ vector of ones. These constraints add the unknowns $(c_1, c_2, c_3, c_4)$, which should solve to be zero. The four rows on the bottom require that the solution satisfy the integral over the domain for each solution component. In combination with the four added columns, the constraints indicate that the solution components must satisfy integrals using the same constant value. This is only possible if the constants are zero. The added constraints are not physically significant, but are chosen to satisfy the system algebraicly. Of course when solving Equation~\ref{eq:stokes_system_with_constraints} with GMRES, the constraints help increase the rate of convergence. 

Although the constraints in Equation~\ref{eq:stokes_system_with_constraints} satisfy the system algebraically, they have no physical significance. As another facet for future work, we are investigating methods that avoid the constraints by splitting Equation~\ref{eq:stokes_system_with_constraints} into a two-step solution process (e.g., solving for $\vu$ then $p$). This would be similar to the SIMPLE method proposed in \cite{Patankar1972} and widely used in computational fluid dynamics. 

%We also investigate the use of GMRES without constraints. This increases the number of iterations required to converge, but allows increased parallelism (decreased data sharing). %TODO: why is dropping the constraints valid? GMRES assumes zero for each of those? 
%\authnote{Perhaps we can iterate without constraints until convergence slows then ``restart" the problem on a single GPU with constraints included? Limits scalability but would allow more parallelism for part of iterations while also reasonable convergence.}

\subsubsection{Interleaved Solution}
%
%Equation~\ref{eq:stokes_system_with_constraints} has all solution values for the vectors $U, V, W$ and $p$ grouped together, the values of $u_1$, $v_1$, $w_1$ and $p_1$ correspond to node $(x_1, y_1, z_1)$. 

In a distributed environment some values for $\vu$ and $p$ are obtained from ghost nodes. Rather than execute four collectives (one per component), we interleave the solution vectors to group all components by node (e.g., $( u_1, v_1, w_1, p_1,\cdots, u_N, v_N, w_N, p_N)^T$). Figure~\ref{fig:interleaved_solution} demonstrates the concept of interleaving the solution. This allows us to operate on four-element wide vectors containing the $\{u_i, v_i, w_i, p_i\}$ for each node $x_i$, and maintains contiguous memory copies between host and device in multi-GPU mode. 

\begin{figure}[ht] 
\centering
        \begin{subfigure}[b]{0.4\textwidth}
        		\centering
		\includegraphics[width=1.0\textwidth]{../figures/paper2/figures/N10201_KDTree_Stokes.pdf} 
		\caption{Grouped by Solution Component}
		\label{fig:original_stokes_dm}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{../figures/paper2/figures/N10201_KDTree_ReorderedStokes.pdf} 
		\caption{Interleaved Solution Components}
		\label{fig:interleaved_stokes_dm}
	\end{subfigure} \\
	\begin{subfigure}[b]{0.4\textwidth}
        		\centering
		\includegraphics[width=1.0\textwidth]{../figures/paper2/figures/N10201_KDTree_Stokes_10to50.pdf}
		\caption{Grouped Submatrix $(10:50) \times (10:50)$}
		\label{fig:original_stokes_zoom_dm}
	\end{subfigure}
	\quad
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
	\includegraphics[width=1.0\textwidth]{../figures/paper2/figures/N10201_KDTree_ReorderedStokes_10to50.pdf} 
		\caption{Interleaved Submatrix $(10:50) \times (10:50)$}
		\label{fig:interleaved_stokes_zoom_dm}
	\end{subfigure} \\
\caption{Sparsity pattern of linear system in Equation~\ref{eq:stokes_constant_viscosity}. Solution values are either ordered by component (e.g., $( u_1, \cdots, u_N, v_1, \cdots, v_N, w_1, \cdots, w_N, p_1, \cdots, p_N)^T$) or interleaved (e.g., $( u_1, v_1, w_1, p_1,\cdots, u_N, v_N, w_N, p_N)^T$). }
\label{fig:interleaved_solution}
\end{figure} 

Figure~\ref{fig:interleaved_solution} demonstrates the effect of interleaving our solution. The sparsity pattern of the original DM with solutions grouped by component is shown in Figure~\ref{fig:original_stokes_dm}. Well defined blocks of non-zeros are filled with RBF-FD weights from Equation~\ref{eq:stokes_constant_viscosity}. Figure~\ref{fig:interleaved_stokes_dm} presents the sparsity pattern for interleaved solution components. The pattern is similar to a single block of Figure~\ref{fig:original_stokes_dm}, but the sub-matrix $(10:50)\times(10:50)$ of each solution ordering, shown in Figures~\ref{fig:original_stokes_zoom_dm} and \ref{fig:interleaved_stokes_zoom_dm}, illustrate that non-zeros in Figure~\ref{fig:interleaved_stokes_dm} are small $4\times4$ blocks with the structure of Equation~\ref{eq:stokes_constant_viscosity}.

Our current application of interleaving was started only to simplify MPI communication and data transfers between CPUs and GPUs. However, future work will continue to investigate interleaving for the purpose of increased operational intensity. For example, each of the 9 nonzero blocks of Equation~\ref{eq:stokes_system_with_constraints} can be multiplied independently against the four vectors $U, V, W$ and $p$. Our work in \cite{ErlebacherSauleFlyerBollig2013} showed that multiplying multiple sparse matrices against multiple vectors , we investigated the increase

%Future work in this area will include integrating the SpMM from \cite{ErlebacherSauleFlyerBollig2013} into an a fluid simulation.


%\subsubsection{Manufactured Solution}


%
%\subsubsection{Convergence}
%
%%TODO: convergence for Stokes
%As the problem size $N$ increases, we expect the approximation to the solution to converge on the order of $\sqrt{n}$ where $n$ is the choice of stencil size. %Figure~\ref{fig:convergence} demonstrates the convergence of our solution with respect to $\sqrt{N}$ for stencil sizes $n=31$, $101$ 
%%
%\authnote{Include convergence plot from handout as initial evidence; state that its ongoing investigation to improve}. 




%\cite{Divo2005} studied ILU for global collocation 


%TODO: Plot comparing residual of GMRES without precond and with ILU0 

%TODO:  Need to comment on the conditioning of the system and how stencils can influence convergence


%\authnote{So my matlab code also demonstrates the behavior I was seeing in C++. The $n=31$ case converges VERY quickly with ILU0. However $n=40$ is slow. Another case that works well is $n=110$. I will have to go back and figure out how Im generating the stencils differently. I wonder if the $log(\kappa) = 14$ for $n=40$ is pushing conditioning too high to use? }

%TODO: demonstrate that ILU improve convergence rate (table showing 3 resolutions and convergence with and without ILU)

%TODO: add ILU algorithm
%\begin{algorithm}                     
%\caption{Incomplete LU Factorization with Zero Fill-in (ILU0)}         
%\label{alg:ilu0}                        
%\begin{algorithmic}[1]                  
%    \For i = 0
%    \State $a_{i,i} = a_{i,i} / a_{i,i}$
%    \EndFor
%\end{algorithmic}
%\end{algorithm}

%We want to express benchmarks in terms of the number of GMRES iterations per second, and the number of iterations required to converge. Readers wont 
%care what percentage of peak we are getting, just how fast we get to the solution. 


%
%TODO: State that the conditions under which a problem is solved determines how quickly it will converge. For example, nodes too close together on delaunay meshes cause higher condition numbers and require more iterations. Proper choice of preconditioners can dramatically reduce the number of iterations required to converge. Although preconditioners incur an additional cost in preprocessing and at each iteration of the solver, the potential number of iterations they save 


%John Dennis dissertation has a list of nice datasets (compare the list to Hoth) and their preconditioners. We might list similar paramters used for results here. 
