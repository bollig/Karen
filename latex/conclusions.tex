\chapter{Final Discussion, Conclusions and Future Work}
\label{chap:conclusions}

This work is the first known investigation of the RBF-FD method applied to geophysical PDEs with a focus on high performance computing. The RBF-FD numerical method was chosen based on its low complexity ($O(N)$) and high order accuracy, as well as the ability to function on arbitrary grids, independent of the number of dimensions. In recent years the generalized FD method has grown in popularity in the atmospheric and geophysical communities, and has reached a point where some believe it could compete against more traditional methods like classical Finite Differences, Finite Volume and Finite Elements that are often found in large-scale simulations that execute on supercomputers. Unfortunately, existing implementations of RBF-FD are predominantly serial MATLAB codes that run on laptops and consider grid sizes up to a couple hundred thousand nodes.


The goal of this work was to scale RBF-FD solutions on high resolution meshes (i.e., millions of nodes) across high performance clusters, and to lead the way for its adoption within HPC and supercomputing circles. Additionally, we sought to extend RBF-FD to clusters with one or more GPUs attached to each compute node with the knowledge that each GPU offers greater than 1 TFLOP/sec peak performance in double precision. In light of these goals we accomplished the following: 

\begin{itemize}
\item Chapters~\ref{chap:background} and \ref{chap:rbffd_method} provided a survey of RBF methods for PDEs and highlighted the evolution from the original application of RBF Interpolation in 1970 to the most recent extensions developed for RBF-FD as of this writing (2013). 

\item Chapter~\ref{chap:stencils} investigated RBF-FD stencil generation and introduced the Fixed-Grid algorithm to a) perform nearest neighbor queries for stencils and b) precondition grids as part of RBF-FD preprocessing. 
\begin{itemize} 
\item We compared the performance of our method to the RBF community favorite, $k$-D Tree, and find that the lower complexity Fixed-Grid method only achieves up to 2.5x speedup over $k$-D Tree on randomly distributed nodes. In most cases the methods perform equally well with $O(N \log_2 N)$ complexity to build and query.
\item The Fixed-Grid method effectively preconditions random node distributions by reordering nodes in memory to keep physically close neighbors nearby in memory. The study finds that using the preconditioned node distributions can accelerate RBF-FD time-step performance by up to 5x (for quasi-random node distributions) due to spatial locality of node information in memory and improved cache effects when operating on stencils. 
\item A discussion of integer dilation and bit interleaving demonstrates the process to construct a number of space-filling curve variants in 2-D and 3-D. The curves reorder cells of the Fixed-Grid algorithm on the prospect of additional cache hits during RBF-FD time-steps. A comparison of space-filling curve orderings to the well-known Reverse Cuthill-McKee (RCM) algorithm finds that RCM delivers an ordering that has both the lowest differentiation matrix bandwidth and the fastest derivative calculation. Unfortunately, the RCM ordering only accelerates derivative calculations by less than 10\% compared to the default space-filling curve (i.e., IJK/Raster-order) generated in the Fixed-Grid method. We find it most convenient to start all RBF-FD calculations by generating stencils with the basic IJK ordered Fixed-Grid method, and then---only if peak performance is absolutely necessary---reorder nodes with the RCM algorithm. 
\end{itemize} 




\item Chapter~\ref{chap:gpu_rbffd} presented the first implementation of RBF-FD to run on a single NVidia GPU using OpenCL. 
\begin{itemize} 

\item Two strategies were employed within custom GPU kernels to compute derivatives and perform a fourth order Runge Kutta time-step. The custom kernels were applied to solve hyperbolic PDEs and performed up to 3 GFLOP/sec, which translates to 2x-4x speedup over the CPU. 

\item  At the core of RBF-FD is the Sparse Matrix Vector multiply (SpMV). The SpMV requires two floating point operations for every two elements of memory loaded. In double precision this translates to an operational intensity of $\frac{1}{8}$ FLOP/Gbytes, which indicates that the SpMV is a memory bound operation. Due to the low operational intensity, the Roofline Model \cite{Williams2009} dictates that the SpMV can only hope to achieve at most $18.5$ GFLOP/sec on an NVidia M2070 GPU, a far cry from the GPU's advertised peak TFLOP/sec performance on the GPU. 

\item To improve the performance of the SpMV we adopt the ELLPACK (ELL) sparse matrix structure provided by the ViennaCL library \cite{Rupp2010}. The ELL structure reduces the number of bytes loaded from memory during the SpMV and increases performance to 8 GFLOP/sec, which is approximately 43\% of the peak performance dictated by the Roofline Model, and up to 18x faster than the performance of the SpMV on the CPU. 

%\item Leveraging the functional portability of OpenCL we provide preliminary benchmarks evaluating the performance of RBF-FD on the Intel Phi Architecture. The results

\end{itemize}

Chapter~\ref{chap:distributed_rbffd} introduced the design and tuning of the first distributed RBF-FD implementation for CPU and GPU clusters.
\begin{itemize} 
\item Our home-grown implementation of RBF-FD divides computation with a Restricted Additive Schwarz (RAS) domain decomposition. We depend on METIS \cite{Karypis1999} for grid partitioning and load balancing. Individual processes do not require a full mapping between RBF nodes and CPUs. Instead, processes assemble and operate on local linear systems for their subdomains. Communication between processes is limited to nearest-neighbor or all-to-subset collectives based on the intersection of subdomains.

\item A number of steps are taken to improve communication collectives in the CPU-only implementation. The resulting algorithm splits derivative calculation into two steps and overlaps communication with computation. We observe that up to 80\% of the cost in communication can be hidden in some cases. 

\item Scaling benchmarks up to 1024 processes (divided into 8 processes per node) and a grid resolution of $N=160^3$ nodes (i.e., 4.1 million) prove that the overlapping communication and computation scales reasonably well in both a strong and weak sense for a moderate number of processors (e.g., $p \leq 256$). We achieve 30\% to 40\% parallel efficiency (both strong and weak) on $p=128$ processors when given a problem size of $N=160^3$ nodes and stencil sizes between $n=17$ and $n=50$ on Itasca, the CPU-only HPC cluster at the University of Minnesota. %Scaling beyond $p > 256$ processors shows 

\end{itemize} 

\item Chapter~\ref{chap:multigpu_rbffd} extended the work on GPUs to distribute RBF-FD across a cluster of 32 GPUs (four per compute node). %To our knowledge, this is also the only known investigation in the broader RBF community to target multiple GPUs. 
\begin{itemize} 
\item The multi-GPU implementation is extended to a novel overlapping algorithm that naturally derives from our distributed CPU optimizations. Non-blocking MPI collectives plus two asynchronous OpenCL queues amortize the cost of data transfer between CPU and GPU, MPI communication, and in some cases a substantial amount of computation. Iterations of the resulting implementation achieve an average of 3x better strong scaling compared to a non-overlapping equivalent. 
\item In some cases, overlapping communication and computation for the GPU exceeds our expectations for scaling. For example, in some cases non-overlapping communication faces serialization on memory transfers between CPU and GPU due to contention on I/O hubs connecting four GPUs per compute node to CPU sockets. The contention not only delays memory transfers but also leads to nearly serialized execution times across the four local GPUs per node. Overlapping communication and computation and fully leveraging GPU queues hides the contention, and in some cases avoids it altogether, resulting in over 4x speedup compared to the non-overlapped cases. 
\end{itemize} 

\item In Chapter~\ref{chap:applications} our explicit solvers are applied to solve two well studied hyperbolic PDEs (Vortex Roll-up \cite{NairTransport05, NairJablonowski08} and Cosine Bell Advection \cite{JakobChien1995}).
\begin{itemize} 
\item Convergence studies confirm that hyperviscosity (\cite{Fornberg2011b}) stabilizes solutions.
\item Tuned parameters are provided for selecting the RBF support parameter and scaling hyperviscosity as a function of the problem size.
\item Each test case is run in all compute environments (e.g., single CPU, multi-CPU, single GPU, and multi-GPU) in double precision. The expectation in each case is that the final solutions and errors will match the single CPU results up to 12 digits. In fact, the IEEE double precision floating point support on GPUs allows perfect reproduction of all CPU results. 
\end{itemize}

\end{itemize} 



\section{Future Work}


This work suggests a number of additional investigations:
\begin{itemize}
\item There are a number of alternatives for SpMV that target various matrix features (e.g., BELL, SELL, SELL-T, etc.) \cite{Kreutzer2012,SuKeutzer2012}. Some may be more beneficial than others. 

\item ViennaCL has support for multiple backends, including the Intel Phi. Initial experiments result in performance $\frac{1}{10}$-th as fast as the NVidia M2070 and K20 GPUs. Those results are: a) limited by a beta driver for OpenCL on Intel Phi; and b) using OpenCL kernels intended for the GPU architecture. Unleashing the hardware for the best performance for RBF-FD may require porting kernels to the low-level, hardware-specific vector instructions for the Intel Phi (see e.g., related work in \cite{ErlebacherSauleFlyerBollig2013}).

\item  In an effort to improve strong and weak scaling of our distributed code, we may need to detect and adjust automatically to the cluster network topology. For example, the domain decomposition employed by \cite{Yokota2012} (i.e., a hierarchical decomposition for the fast multipole method) is known to fit well on fat-tree topologies such as the one found in the Itasca cluster. On top of topology concerns we also suspect that MPI communication can be improved with proper data packing and power-of-2 aligned messages.

\item Although METIS functions well for general domain decompositions, it fails to properly load balancing our partitions for $p > 256$ processors. Many alternative algorithms and libraries exist for decomposition and graph partitioning that may prove better at load balancing for a large number of processors. 

\item 
New features on NVidia Kepler level GPUs and CUDA v5.5 allow dynamic parallelism. Dynamic parallelism allows GPU kernels to spawn new kernels directly on the GPU without returning to the CPU. It provides stream priority features that allow kernels to preemptively execute out of order based on priority and dependencies. Such a feature is powerful for MPI support where the GPU can continue processing kernels while waiting on MPI communication to complete.
 
\item In Section~\ref{sec:epsilon_contours} we mention the need for stable methods to solve for weights. We have partially implemented the RBF-GA method from \cite{Fornberg2012}.  Initial exposure to the algorithm leaves the impression that the change of basis on the RHS is a nuisance. In an effort to bypass the difficulty in deriving new RHS expressions, we plan to assemble complex DMs using DMs of lower (e.g., first-) order operators. See Appendix~\ref{app:indirect_weights} for details on how such DMs would be composed.
\item Another major effort currently underway is the implicit solution of coupled PDEs with RBF-FD on multiple GPUs. Implicit methods assemble a sparse linear system that must be solved by either inverting the differentiation matrix directly (e.g., via a sparse LU decomposition), or by solving the system via a sparse iterative solver (e.g., GMRES or BiCGStab).
Our effort adopts the iterative solver method, and we have implemented a multi-GPU preconditioned GMRES method in OpenCL similar to the CUDA implementation in \cite{Bahi2011}. The core operation within sparse iterative solvers is the SpMV. Therefore, all optimizations achieved in Chapters~\ref{chap:gpu_rbffd}, \ref{chap:distributed_rbffd}, and \ref{chap:multigpu_rbffd} continue to apply in this case. Additional details related to this effort are provided in Appendix~\ref{app:implicit_solver}.

\end{itemize}

