\chapter{Final Discussion, Conclusions and Future Work}
\label{chap:conclusions}


\section{Future Work}

This work leads to a number of additional investigations:
\begin{itemize}
\item Transitioning away from RBF-FD specific OpenCL kernels and into the higher level matrix algebra library ViennaCL reveals new optimizations for SpMV, possible only when treating the sparse matrix from a higher level. There are a number of alternatives for SpMV that target various matrix features (e.g., jagged diagonal, blocked ELL, etc). New investigation will determine the which, if any, alternatives can benefit RBF-FD. 

\item ViennaCL has support for multiple backends, including the Intel Phi. Initial experiments result in performance $\frac{1}{10}$th as fast as the K20 GPU. Those results are: a) limited by a beta driver for OpenCL on Intel Phi; and b) using OpenCL kernels intended for the GPU architecture. Continued investigations are targeting features (language, vector instructions, etc.) of the Phi. 
\item To improve the accuracy of RBF-FD weights, an experimental RBF-GA implementation was started. The initial exposure to the algorithm leaves the impression that the change of basis on the RHS is a nuisance. In effort to bypass the difficulty in deriving new RHS expressions, we plan to assemble complex DMs using DMs of lower (i.e., first-) order operators.
\end{itemize}

%Unanswered Questions: 
%\begin{itemize} 
%\item If RBF-FD is higher order accurate than FV, why would anyone opt for FV?
%\begin{itemize} 
%\item the conservative form FV is preferred in physical science since it does not allow loss of (what? mass/energy/???)
%\item FV are typically low-order. what is the trade-off in complexity between methods. For a much higher-order RBF-FD if it is not conservative, how many iterations is it accurate consistent with the low-order FV? l
%\end{itemize}
%\end{itemize}


\section{On the Future of GPU Computing}

%TODO: Move this to the conclusions chapter
%  addresss this as: 
%   Future of GPU is with CUDA and vendor specific features. 
%   AMD, Intel are trying to compete, but NVidia has the market and new entrants should stick with CUDA. 


%The question, however, is frequently asked: is GPU computing a buzzword that will die out, or is it here to stay? 
%
%Fine tuning of kernels is the way of the past. Anyone fine tuning kernels will be operating at a lower level, attempting to optimize library routines underlying large scale codes. The fine tuning can ultimately be replaced by auto-tuned kernels (e.g., see work in ViennaCL to auto-tune similar to libATLAS). 
%
%Already this is seen in attempts to port applications to the GPU in so-called ``minimally invasive" fashion. 
%
%So the short answer is: yes. And it is from this assumption that we proceed with our efforts to target the GPU with RBF-FD. However, we have made a few predictions on the future of GPUs and these predictions guided our efforts to port RBFs onto GPUs. 
%
%First, we are proponents for OpenCL over CUDA. Although CUDA has a large following due to its early release, we believe in functional portability of code enabled by OpenCL over the performance provided by CUDA. 

New features on NVidia Kepler level GPUs and CUDA v5 allow dynamic parallelism. Dynamic parallelism provides the ability to generate new levels of parallelism on the GPU rather than return to the CPU.  Dynamic parallelism also leads to stream priority features which allow kernels to preemptively execute out of order based on priority and dependencies. Such a feature is powerful for MPI support where the GPU can continue processing kernels while waiting on MPI communication to complete. 
